{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6368ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import re\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Scrapping and crawling modules\n",
    "import undetected_chromedriver as uc\n",
    "from requests.utils import quote, unquote\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def getTime(str: str) -> datetime:\n",
    "    '''\n",
    "    Convert string of datetime in isoformat to datetime object and adjust the timezone\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    - str: str\n",
    "        String of datetime in isoformat\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    - date_time_obj: datetime\n",
    "        Datetime object adjusted to UTC+7\n",
    "    '''\n",
    "    date_time_obj = datetime.fromisoformat(str)\n",
    "    date_time_obj =date_time_obj + timedelta(hours=7)   # Fuckass X timezone needs to be adjusted to UTC+7. Might as well make it adjustable based on user timezone later\n",
    "\n",
    "    return date_time_obj\n",
    "\n",
    "def minOneDay(str: str) -> str:\n",
    "    '''\n",
    "    This function is used to subtract one day from the given date\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    - str: str\n",
    "        String of datetime in isoformat\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    - str: str\n",
    "        String of datetime in isoformat minus one day\n",
    "    '''\n",
    "    str = datetime.strptime(str, \"%Y-%m-%d\")\n",
    "    str = str - timedelta(days=1)\n",
    "    str = str.strftime(\"%Y-%m-%d\")\n",
    "    return str\n",
    "\n",
    "def wait(timeout: int = 10) -> None:\n",
    "    '''\n",
    "    just a glorified simple function to wait for a certain amount of time with a progress bar.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    - timeout: int\n",
    "        Time to wait in seconds\n",
    "    '''\n",
    "    for _ in tqdm(range(timeout), desc=\"Waiting\"):\n",
    "        time.sleep(1)\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "def safe_int_from_aria(aria_label: str) -> int:\n",
    "    '''\n",
    "    Safely extract an integer from an aria-label string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - aria_label : str\n",
    "        The aria-label string containing the number.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - int\n",
    "        The extracted integer from the aria-label string. Returns 0 if no integer is found.\n",
    "    '''\n",
    "    match = re.search(r\"\\d+\", aria_label or \"\")\n",
    "    return int(match.group(0)) if match else 0\n",
    "\n",
    "\n",
    "\n",
    "class twitterScrapper:\n",
    "    '''\n",
    "    This class is used to scrape posts from X (formerly known as Twitter) based on given filters and date range.\n",
    "    \n",
    "    Methods\n",
    "    ----------\n",
    "    - login()\n",
    "        - Logs into Twitter using the provided credentials\n",
    "    - start()\n",
    "        - Starts the scraping process based on the given filters and date range\n",
    "    '''\n",
    "\n",
    "    def __init__(self, credentials: str = \"Credentials/twitter.json\"):\n",
    "        '''\n",
    "        This function is used to initialize the class and will also login to twitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        credentials : str\n",
    "            - Path to the twitter credentials json file\n",
    "            - The json file should be in the following format:\n",
    "            ```\n",
    "            {   \"username\" : \"your_username\",\n",
    "                \"password\" : \"your_password\",\n",
    "                \"email\"    : \"your_email\"}\n",
    "            ```\n",
    "        '''\n",
    "        \n",
    "        # Bunch of checkers and loaders for credentials\n",
    "        if not os.path.exists(credentials):\n",
    "            raise FileNotFoundError(\"Credentials file not found!\")\n",
    "        with open(credentials, \"r\") as f:\n",
    "            credentials = json.load(f)\n",
    "            try:\n",
    "                username = credentials[\"username\"]\n",
    "                password = credentials[\"password\"]\n",
    "                email = credentials[\"email\"]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Credentials file is not in the correct format!\")\n",
    "        \n",
    "        # Check if any of those credentials is None\n",
    "        if username is None or password is None:\n",
    "            raise ValueError(\"Username or password can't be empty!\")\n",
    "        if email is None:\n",
    "            warnings.warn(\"Email is not provided, this shit might not work if suspicious login attempt is detected!\", UserWarning)\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "\n",
    "        # For storing all the data during scraping\n",
    "        self.theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "                         \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}\n",
    "        \n",
    "        self.login()\n",
    "\n",
    "    # Utilities for this class\n",
    "    def _build_search_url(self, date_limit: str) -> str:\n",
    "        '''\n",
    "        Build the search URL based on the Filters and given datelimit\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - date_limit : str\n",
    "            The date limit for the search in the format \"YYYY-MM-DD\".\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - str\n",
    "            The constructed search URL.\n",
    "        '''\n",
    "        return f\"{self.SEARCH_URL}{self.FILTERS_COMBINATION}{quote(f' until:{date_limit}')}&f=live&src=typed_query\"\n",
    "\n",
    "    def _scrape_detected(self) -> bool:\n",
    "        '''\n",
    "        Check if scraping is detected by looking for the \"Something went wrong. Try reloading.\" message. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        - bool\n",
    "            True if scraping is detected (based on the presence of the error message), False otherwise.\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            sumthingwrong = WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                 EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span'))\n",
    "            )\n",
    "            sumthingwrong.text == \"Something went wrong. Try reloading.\"\n",
    "            return True\n",
    "        \n",
    "        except TimeoutException:\n",
    "            return False\n",
    "\n",
    "    def _wait_for_posts(self, current_date: str, counter: int) -> tuple[str, int, bool]:\n",
    "        '''\n",
    "        Wait for posts to load on the page. If no posts are found within the timeout period, step back one day and increment the counter.\n",
    "\n",
    "        The counter tracks the number of consecutive days with no posts found. If the counter exceeds the maximum allowed empty pages, the function indicates that all posts have been reached.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - current_date : str\n",
    "            The current date being checked in the format \"YYYY-MM-DD\".\n",
    "        - counter : int\n",
    "            The number of consecutive days with no posts found.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - tuple[str, int, bool]\n",
    "            A tuple containing the updated current date, the updated counter, and a boolean indicating whether the maximum number of empty pages has been reached.\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            # Is there a container for post?\n",
    "            WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\"))\n",
    "            )\n",
    "            counter = 0\n",
    "            return current_date, counter, False # Yes\n",
    "        except TimeoutException:\n",
    "            current_date = minOneDay(current_date)\n",
    "            counter += 1\n",
    "            print(f\"No posts found, stepping back to {current_date}, attempt {counter + 1}/{self.MAX_EMPTY_PAGES}\")\n",
    "            return current_date, counter, counter > self.MAX_EMPTY_PAGES    # Nothingburger and minus by one day\n",
    "\n",
    "    def _parse_post(self, post_element) -> str:\n",
    "        '''\n",
    "        Parse the post element to extract the full text of the post.\n",
    "\n",
    "        NOTE: This only handles text, links, emojis, and such. Media (images, videos, gifs) are not handled ***yet***.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - post_element : WebElement\n",
    "            The WebElement representing the post.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - str\n",
    "            The full text of the post.\n",
    "        '''\n",
    "\n",
    "        parts = post_element.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "        text = \"\"\n",
    "        for p in parts:\n",
    "            if p.tag_name == \"img\":                 # Emojis\n",
    "                text += p.get_attribute(\"alt\")\n",
    "            elif p.tag_name == \"a\":                 # Links (Not shortened with t.co domain)\n",
    "                text += p.text + \" \"\n",
    "            else:                                   # Normal text                 \n",
    "                text += p.text\n",
    "        return text\n",
    "\n",
    "    def _extract_post_data(self, element) -> tuple[str, str, str, str]:\n",
    "        '''\n",
    "        Extract post data from the given post element.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - element : WebElement\n",
    "            The WebElement representing the post.\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        - tuple[str, str, str, str]\n",
    "            A tuple containing the post text, quoted post text, post user, and post date.\n",
    "        '''\n",
    "\n",
    "        # Get the entire post element\n",
    "        post_element = element.find_element(\n",
    "            By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]',\n",
    "        )\n",
    "        # Post\n",
    "        post_text = self._parse_post(post_element)\n",
    "\n",
    "        # Quoted Post\n",
    "        try:\n",
    "            quoted_element = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "            quoted_text = ''.join(\n",
    "                i.text for i in quoted_element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            quoted_text = \"\"\n",
    "\n",
    "        post_date = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        post_user = element.find_element(By.XPATH, './/a/div/span').text\n",
    "\n",
    "        return post_text, quoted_text, post_user, post_date\n",
    "    \n",
    "    def _write_json(self, filename: str) -> None:\n",
    "        '''\n",
    "        Write the scraped data to a JSON file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        - filename : str\n",
    "            The name of the file to write the JSON data to.\n",
    "        '''\n",
    "        # theDict conversion to a list of dictionaries\n",
    "        theDict_JSON = {i: {\n",
    "                j: self.theDict[j][i] for j in self.theDict.keys()\n",
    "            } for i in range(len(self.theDict[\"post_text\"]))\n",
    "            }\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(theDict_JSON, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _write_csv(self, filename: str) -> None:\n",
    "        '''\n",
    "        Write the scraped data to a CSV file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - filename : str\n",
    "            The name of the file to write the CSV data to.\n",
    "        '''\n",
    "        df = pd.DataFrame(self.theDict)\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    def _load_latest_savepoint(self) -> bool:\n",
    "        '''\n",
    "        Load the latest savepoint from the Savepoints directory.\n",
    "\n",
    "        Uses the eariest date as the `self.start_date` from the most recently modified file in the Savepoints directory and loads the data into `self.theDict`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        - bool\n",
    "            True if a savepoint was loaded, False otherwise.\n",
    "        '''\n",
    "\n",
    "        save_dir = f\"Process/{self.processDir}/Savepoints\"\n",
    "\n",
    "        # Checkers if there's any savepoint\n",
    "        if not os.path.isdir(save_dir):\n",
    "            return False\n",
    "        files = [f for f in os.listdir(save_dir) if f.endswith((\".csv\", \".json\"))]\n",
    "        if not files:\n",
    "            return False\n",
    "        \n",
    "        # Get the latest savepoint file\n",
    "        latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(save_dir, f)))\n",
    "        latest_path = os.path.join(save_dir, latest_file)\n",
    "\n",
    "        if latest_file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(latest_path)\n",
    "        else:\n",
    "            with open(latest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "        \n",
    "        self.theDict = {col: df[col].tolist() for col in df.columns}    # Convert DataFrame to dictionary\n",
    "        if \"Date\" in self.theDict and self.theDict[\"Date\"]:\n",
    "            try:\n",
    "                earliest_dt = min(\n",
    "                    datetime.strptime(d, \"%Y-%m-%d-%H:%M:%S\")\n",
    "                    for d in self.theDict[\"Date\"]\n",
    "                    if isinstance(d, str)\n",
    "                )\n",
    "                self.start_date = earliest_dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        print(f\"Resumed from savepoint: {latest_file}\")\n",
    "        return True\n",
    "\n",
    "    def save(self, type: Literal[\"final\", \"savepoint\"]) -> str:\n",
    "        '''\n",
    "        Save the current progress to a file.\n",
    "\n",
    "        file extension and format is based on `self.saveFormat`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - type : Literal[\"final\", \"savepoint\"]\n",
    "            The type of save to perform. \"final\" for final save, \"savepoint\" for intermediate savepoint.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - str\n",
    "            The path to the saved file.\n",
    "        '''\n",
    "        if type not in {\"final\", \"savepoint\"}:\n",
    "            raise ValueError(\"Save type must be 'final' or 'savepoint'.\")\n",
    "\n",
    "        os.makedirs(f\"Process/{self.processDir}/Savepoints\", exist_ok=True)\n",
    "        \n",
    "        if type == \"savepoint\":\n",
    "            save_path = f\"Process/{self.processDir}/Savepoints/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        else:\n",
    "            save_path = f\"Process/{self.processDir}/Final\"\n",
    "\n",
    "        if self.saveFormat == \"csv\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "        elif self.saveFormat == \"json\":\n",
    "            self._write_json(f\"{save_path}.json\")\n",
    "        elif self.saveFormat == \"both\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "            if type == \"final\":                         # There's no fucking logic to save both on savepoint, only do it on final save\n",
    "                self._write_json(f\"{save_path}.json\")\n",
    "        else:\n",
    "            raise ValueError(\"saveFormat must be 'csv', 'json', or 'both'.\")\n",
    "        \n",
    "        return save_path    # This ain't used, but yeah.\n",
    "    \n",
    "    def login(self) -> None:\n",
    "        '''\n",
    "        Tries to login to the account based from the given credentials\n",
    "\n",
    "        Will use `self.username` and `self.password` only. However, if X detects your login as suspicious, `self.email` will be used.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        - ValueError\n",
    "            If email is required due to suspicious login attempt, but email is not provided on credentials.\n",
    "        '''\n",
    "        # Initialize the driver and check bot detection\n",
    "        self.driver = uc.Chrome()\n",
    "        self.driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "        # Check bot detection, \n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "        time.sleep(4)\n",
    "        botResult = self.driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "        if botResult != \"Normal\":\n",
    "            warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "        # Get to X login page\n",
    "        self.driver.get(\"https://x.com/i/flow/login\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Login handling\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(self.username)\n",
    "        self.driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # if email is needed. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "        try:\n",
    "            if self.driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "                print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "                if self.email is None:\n",
    "                    raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "                self.driver.find_element(By.XPATH, \"//input\").send_keys(self.email)\n",
    "                self.driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Put password\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(self.password)\n",
    "        self.driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "        print(\"Login sucess!\")\n",
    "        wait(10)\n",
    "\n",
    "    def start(self, filters, startDate: str = \"\", endDate: str = \"\",\n",
    "              scraping_Params  =  {\"wait_short\": 10, \"wait_long\": 30,\n",
    "                                  \"detection_wait\": 900, \"max_empty_pages\": 2},\n",
    "                                  saveFormat: Literal[\"csv\", \"json\", \"both\"] = \"csv\", \n",
    "                                  autoSave: bool = False, autoSaveInterval: int = 15, continue_if_timeout: bool = True,\n",
    "                                  processDir: str = \"\", resume_from_savepoint: bool = True) -> None:\n",
    "        '''\n",
    "        This function is used to start the scrapping process based on the given filters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        - Filters : dict\n",
    "            - A dictionary containing the filters for scrapping.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"all_these_words\": \"\",\n",
    "                \"this_exact_phrase\": \"\",\n",
    "                \"any_of_these_words\": \"\",\n",
    "                \"none_of_these_words\": \"\",\n",
    "                \"these_hashtags\": \"\", \n",
    "                \"from_accounts\": \"\",            \n",
    "                \"to_accounts\": \"\",              \n",
    "                \"mentioning_accounts\": \"\",      \n",
    "                \"Minimum_replies\": \"\",\n",
    "                \"Minimum_likes\": \"\",\n",
    "                \"Minimum_retweets\": \"\",\n",
    "                \"links\": True,\n",
    "                \"replies\": True,                \n",
    "            }\n",
    "            ```\n",
    "        - startDate : str\n",
    "            - The latest date for scrapping in the format \"YYYY-MM-DD\".\n",
    "            - If empty, will default to current date.\n",
    "        \n",
    "        - endDate : str\n",
    "            - The earliest date for scrapping in the format \"YYYY-MM-DD\".\n",
    "            - If empty, will default to \"2006-01-01\" (Twitter launch date).\n",
    "        \n",
    "        - scraping_Params : dict\n",
    "            - A dictionary containing the scrapping parameters.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"wait_short\": 10,\n",
    "                \"wait_long\": 30,\n",
    "                \"detection_wait\": 900,\n",
    "                \"max_empty_pages\": 2\n",
    "            }\n",
    "            ```\n",
    "        - wait_short : int\n",
    "            - Short wait time in seconds for page loading. Used for normal page loads. E.g, scrollig down to load more posts\n",
    "\n",
    "        - wait_long : int\n",
    "            - Long wait time in seconds for page loading. Used for cases where the page takes longer to load. Like after getting into a new page\n",
    "            \n",
    "        - detection_wait : int\n",
    "            - Wait time in seconds when scraping detection is encountered.\n",
    "\n",
    "        - max_empty_pages : int\n",
    "            - Maximum number of consecutive empty pages before stopping scrapping.\n",
    "\n",
    "        - saveFormat : Literal[\"csv\", \"json\", \"both\"]\n",
    "            - The format to save the scrapped data. Can be \"csv\", \"json\", or \"both\".\n",
    "            - Default is \"csv\".\n",
    "\n",
    "        - autoSave : bool\n",
    "            - Whether to automatically save the scrapped data at regular intervals.\n",
    "            - Default is False.\n",
    "\n",
    "        - autoSaveInterval : int\n",
    "            - The interval to automatically save the scrapped data based on how many posts have been scraped.\n",
    "            - Default is 15.\n",
    "\n",
    "        - continue_if_timeout : bool\n",
    "            - Whether to continue scrapping if scraping detection is encountered.\n",
    "            - Default is True.\n",
    "\n",
    "        - processDir : str\n",
    "            - The directory to save the scrapped data.\n",
    "            - If empty, will default to current date in \"YYYY-MM-DD\" format.\n",
    "\n",
    "        - resume_from_savepoint : bool\n",
    "            - Whether to resume scrapping from the latest savepoint if available.\n",
    "            - Default is True.\n",
    "\n",
    "        '''\n",
    "        self.SEARCH_URL = \"https://x.com/search?q=\"\n",
    "        \n",
    "        # Adjust filters values\n",
    "        filters[\"this_exact_phrase\"] = f'\\\"{filters[\"this_exact_phrase\"]}\\\"' if filters[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "        _any_terms = []\n",
    "        _any_raw = filters[\"any_of_these_words\"].strip()\n",
    "        if _any_raw:\n",
    "            for g1, g2, g3 in re.findall(r'\"([^\"]+)\"|\\'([^\\']+)\\'|(\\S+)', _any_raw):\n",
    "                _any_terms.append(g1 or g2 or g3)\n",
    "        filters[\"any_of_these_words\"] = (\n",
    "            f'({\" OR \".join(_any_terms)})' if _any_terms else \"\"\n",
    "        )\n",
    "        filters[\"none_of_these_words\"] = f'{\" \".join(f\"-{i}\" for i in filters[\"none_of_these_words\"].split())}' if filters[\"none_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"these_hashtags\"] = f'({\" OR \".join(f\"{i}\" for i in filters[\"these_hashtags\"].split())})' if filters[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"from_accounts\"] = f'({\" OR \".join(f\"from:{i}\" for i in filters[\"from_accounts\"].split())})' if filters[\"from_accounts\"] != \"\" else \"\"\n",
    "        filters[\"to_accounts\"] = f'({\" OR \".join(f\"to:{i}\" for i in filters[\"to_accounts\"].split())})' if filters[\"to_accounts\"] != \"\" else \"\"\n",
    "        filters[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in filters[\"mentioning_accounts\"].split())})' if filters[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"replies\"] = \"\" if filters[\"replies\"] else \"-filter:replies\" \n",
    "        filters[\"links\"] = \"\" if filters[\"links\"] else \"-filter:links\"\n",
    "\n",
    "        filters[\"Minimum_replies\"] = f'min_replies:{filters[\"Minimum_replies\"]}' if filters[\"Minimum_replies\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_likes\"] = f'min_faves:{filters[\"Minimum_likes\"]}' if filters[\"Minimum_likes\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_retweets\"] = f'min_retweets:{filters[\"Minimum_retweets\"]}' if filters[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "        FILTERS_COMBINATION = \"\"\n",
    "        for _, value in filters.items():\n",
    "            if value != \"\":\n",
    "                FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "        self.FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "\n",
    "        # Dates handling\n",
    "        if startDate == \"\":\n",
    "            startDate = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if endDate == \"\":\n",
    "            endDate = \"2006-01-01\"  # Twitter launch date\n",
    "        self.start_date = startDate\n",
    "        self.end_date = endDate\n",
    "\n",
    "        # scraping params\n",
    "        self.WAIT_SHORT = scraping_Params[\"wait_short\"]\n",
    "        self.WAIT_LONG = scraping_Params[\"wait_long\"]\n",
    "        self.DETECTION_WAIT = scraping_Params[\"detection_wait\"]\n",
    "        self.MAX_EMPTY_PAGES = scraping_Params[\"max_empty_pages\"]\n",
    "\n",
    "        # Other params\n",
    "        self.saveFormat = saveFormat\n",
    "        self.autoSave = autoSave\n",
    "        self.autoSaveInterval = autoSaveInterval\n",
    "        self.continue_if_timeout = continue_if_timeout\n",
    "        self.processDir = processDir if processDir != \"\" else datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        if resume_from_savepoint:\n",
    "            self._load_latest_savepoint()\n",
    "        \n",
    "        self.scrape()   # Immidiately start scraping right here right fucking now\n",
    "        \n",
    "    def scrape(self) -> None:\n",
    "        '''\n",
    "        Starts the scraping process.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        - RuntimeError\n",
    "            If scraping is detected and `continue_if_timeout` is False.\n",
    "        - Exception\n",
    "            For any other exceptions that occur during scraping including keyboard interrupts.\n",
    "        '''\n",
    "        reached_all_posts = False\n",
    "        counter = 0\n",
    "        seen = set()    # Uniqueness so there won't be a fuckton of duplicates\n",
    "\n",
    "        # If there's already data on self.theDict, populate seen set. Used for resuming from savepoint\n",
    "        if all(k in self.theDict for k in (\"post_text\", \"Date\", \"User\")) and self.theDict[\"post_text\"]:\n",
    "            seen = {\n",
    "                (self.theDict[\"post_text\"][i], self.theDict[\"Date\"][i], self.theDict[\"User\"][i])\n",
    "                for i in range(len(self.theDict[\"post_text\"]))\n",
    "            }\n",
    "        start_date = self.start_date\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "\n",
    "                # Get the current date upper limit\n",
    "                current_date_limit = start_date\n",
    "\n",
    "                # If all shits been scraped, will save and break\n",
    "                if reached_all_posts:\n",
    "                    print(\"All posts have been scraped!\")\n",
    "                    # Delete all temps aka Savepoints\n",
    "                    shutil.rmtree(f'Process/{self.processDir}/Savepoints/')\n",
    "                    self.save(\"final\")\n",
    "                    break\n",
    "\n",
    "                self.driver.get(self._build_search_url(current_date_limit))\n",
    "                time.sleep(self.WAIT_SHORT)\n",
    "\n",
    "                # CHECKER\n",
    "                ##  1 CHECKER FOR SCRAPING DETECTION, IF `continue_if_timeout` IS TRUE, WILL WAIT AND CONTINUE, ELSE WILL JUST STOP.\n",
    "                if self.continue_if_timeout:\n",
    "                    if self._scrape_detected():\n",
    "                        print(\"Scraping detected! Auto-saving progress...\")\n",
    "                        self.save(\"savepoint\")\n",
    "                        print(f\"Waiting for {self.DETECTION_WAIT} seconds\")\n",
    "                        # Wait for abyssmal amount of time\n",
    "                        wait(self.DETECTION_WAIT)\n",
    "                        continue\n",
    "\n",
    "                else:\n",
    "                    if self._scrape_detected():\n",
    "                        self.save(\"savepoint\")\n",
    "                        raise RuntimeError(\"Scraping detected! All progress have been saved.\")\n",
    "\n",
    "                ##  2 CHECKER FOR NO POSTS FOUND, IF SHIT HAPPENS WILL ROLE BACK FOR LIKE A DAY. IF SHIT KEEPS HAPPENING TILL `MAX_EMPTY_PAGES``, WILL STOP.\n",
    "                start_date, counter, reached_all_posts = self._wait_for_posts(start_date, counter)\n",
    "                if reached_all_posts:\n",
    "                    print(\"No more posts found!\")\n",
    "                    continue\n",
    "\n",
    "                last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                while True:\n",
    "                    elements = self.driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "\n",
    "                    for element in elements[:-1]:\n",
    "                        try:\n",
    "                            post_text, quoted_text, post_user, post_date = self._extract_post_data(element)\n",
    "                        except (NoSuchElementException, StaleElementReferenceException):\n",
    "                            continue\n",
    "\n",
    "                        key = (post_text, post_date, post_user)\n",
    "                        if key in seen:\n",
    "                            continue\n",
    "\n",
    "                        # If end date is reached, functional if user specified end_date at self.start()\n",
    "                        if self.theDict[\"Date\"] and getTime(self.theDict[\"Date\"][-1]) < getTime(self.end_date):\n",
    "                            reached_all_posts = True\n",
    "                            break\n",
    "                        \n",
    "                        seen.add(key)\n",
    "                        self.theDict[\"post_text\"].append(post_text)\n",
    "                        self.theDict[\"quotedPost_text\"].append(quoted_text)\n",
    "                        self.theDict[\"User\"].append(post_user)\n",
    "                        self.theDict[\"Date\"].append(post_date)\n",
    "\n",
    "                        for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                            self.theDict[\"Reply_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"Repost_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"Like_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"View_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "\n",
    "                        if self.autoSave and len(seen) % self.autoSaveInterval == 0:\n",
    "                            self.save(\"savepoint\")\n",
    "                        \n",
    "                    if reached_all_posts:\n",
    "                        break\n",
    "                        \n",
    "                    self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(self.WAIT_SHORT)\n",
    "                    new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                    if new_height == last_height:\n",
    "                        if self.theDict[\"Date\"]:\n",
    "                            last_date_only = \"-\".join(self.theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                            if getTime(last_date_only) >= getTime(current_date_limit):\n",
    "                                start_date = minOneDay(last_date_only)\n",
    "                            else:\n",
    "                                start_date = last_date_only\n",
    "                        else:\n",
    "                            start_date = minOneDay(current_date_limit)\n",
    "\n",
    "                        self.start_date = start_date\n",
    "                        break\n",
    "\n",
    "                    last_height = new_height\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Auto-saving progress before exiting...\")\n",
    "            self.save(\"savepoint\")\n",
    "            self.driver.quit()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc792b",
   "metadata": {},
   "source": [
    "### Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = twitterScrapper(\"Credentials/twitter.json\")   # Fill up your own credentials before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df523548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 searching for posts \n",
    "FILTERS = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"'\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",            # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"jokowi\",    # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}\n",
    "\n",
    "session.start(\n",
    "    startDate=\"\",                       # Latest date for scrapping, empty means current date\n",
    "    endDate=\"\",                         # Earliest date for scrapping, empty means 2006-01-01\n",
    "    filters=FILTERS,\n",
    "    scraping_Params={\n",
    "        \"wait_short\": 10,               # How long to wait for post to load in seconds after scrollin, etc\n",
    "        \"wait_long\": 30,                # How long to wait for page to load in seconds for longer loads\n",
    "        \"detection_wait\": 900,          # Wait time in seconds when scraping detection is encountered\n",
    "        \"max_empty_pages\": 2            # Maximum number of consecutive empty pages before stopping scrapping\n",
    "    },\n",
    "    saveFormat=\"both\",                  # Save both CSV and JSON\n",
    "    autoSave=True,                      # Enable autosave so `autoSaveInterval` works\n",
    "    autoSaveInterval=100,               # Save every 100 posts scraped\n",
    "    continue_if_timeout=True,           # Continue scrapping if scraping detection is encountered\n",
    "    processDir=\"jokowi_twitterACC\",     # process directory on Process/\n",
    "    resume_from_savepoint=True          # Resume from latest savepoint if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 searching for posts that have the exact words of either \"Makan Bergizi Gratis\" or \"MBG\" in it without any grok response from 2025-12-31 to this day\n",
    "\n",
    "FILTERS = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",         \n",
    "    \"this_exact_phrase\": \"\",         \n",
    "    \"any_of_these_words\": \"\\'Makan Bergizi Gratis\\' \\'MBG\\'\",\n",
    "    \"none_of_these_words\": \"grok\",\n",
    "    \"these_hashtags\": \"\",\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"\",\n",
    "    \"to_accounts\": \"\",\n",
    "    \"mentioning_accounts\": \"\",\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",\n",
    "    \"Minimum_likes\": \"\",\n",
    "    \"Minimum_retweets\": \"\",\n",
    "    \"links\": True,\n",
    "    \"replies\": True,\n",
    "}\n",
    "\n",
    "session.start(\n",
    "    endDate=\"2025-12-31\",\n",
    "    filters=FILTERS,\n",
    "    saveFormat=\"both\",\n",
    "    autoSave=True,\n",
    "    autoSaveInterval=100,\n",
    "    continue_if_timeout=True,   \n",
    "    processDir=\"MBG\",\n",
    "    resume_from_savepoint=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
