{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import re\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import shutil\n",
    "\n",
    "# Scrapping and crawling modules\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "\n",
    "# New chromediver\n",
    "import undetected_chromedriver as uc\n",
    "from requests.utils import quote, unquote\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(str):\n",
    "    '''\n",
    "    This function is used to convert string of datetime in isoformat to datetime object\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        date_time_obj:\n",
    "            - Datetime object\n",
    "    '''\n",
    "    date_time_obj = datetime.fromisoformat(str)\n",
    "    date_time_obj =date_time_obj + timedelta(hours=7)\n",
    "    return date_time_obj\n",
    "\n",
    "def minOneDay(str):\n",
    "    '''\n",
    "    This function is used to subtract one day from the given date\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        str:\n",
    "            - String of datetime in isoformat after subtracting one day\n",
    "    '''\n",
    "    str = datetime.strptime(str, \"%Y-%m-%d\")\n",
    "    str = str - timedelta(days=1)\n",
    "    str = str.strftime(\"%Y-%m-%d\")\n",
    "    return str\n",
    "\n",
    "def wait(timeout: int = 10):\n",
    "    '''\n",
    "    just a glorified simple function to wait for a certain amount of time\n",
    "    '''\n",
    "    for i in tqdm(range(timeout), desc=\"Waiting\"):\n",
    "        time.sleep(1)\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "def safe_int_from_aria(aria_label: str) -> int:\n",
    "    match = re.search(r\"\\d+\", aria_label or \"\")\n",
    "    return int(match.group(0)) if match else 0\n",
    "\n",
    "class twitterScrapper:\n",
    "    def __init__(self, credentials: str = \"Credentials/twitter.json\",\n",
    "                 ):\n",
    "        '''\n",
    "        This function is used to initialize the class and will also login to twitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        credentials : str\n",
    "            - Path to the twitter credentials json file\n",
    "            - The json file should be in the following format:\n",
    "            ```\n",
    "            {   \"username\" : \"your_username\",\n",
    "                \"password\" : \"your_password\",\n",
    "                \"email\"    : \"your_email\"}\n",
    "            ```\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(credentials):\n",
    "            raise FileNotFoundError(\"Credentials file not found!\")\n",
    "\n",
    "        with open(credentials, \"r\") as f:\n",
    "            credentials = json.load(f)\n",
    "            try:\n",
    "                username = credentials[\"username\"]\n",
    "                password = credentials[\"password\"]\n",
    "                email = credentials[\"email\"]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Credentials file is not in the correct format!\")\n",
    "        \n",
    "        # Check if any of those credentials is None\n",
    "        if username is None or password is None:\n",
    "            raise ValueError(\"Username or password can't be empty!\")\n",
    "        if email is None:\n",
    "            warnings.warn(\"Email is not provided, this shit might not work if suspicious login attempt is detected!\", UserWarning)\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "\n",
    "        # For storing all the data\n",
    "        self.theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "                         \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}\n",
    "        \n",
    "        self.login()\n",
    "\n",
    "    # Utils\n",
    "    def build_search_url(self, date_limit: str) -> str:\n",
    "        '''\n",
    "        This function is used to build the search URL based on the given date limit.\n",
    "        '''\n",
    "        return f\"{self.SEARCH_URL}{self.FILTERS_COMBINATION}{quote(f' until:{date_limit}')}&f=live&src=typed_query\"\n",
    "\n",
    "    def scrape_detected(self) -> bool:\n",
    "        try:\n",
    "            sumthingwrong = WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                 EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span'))\n",
    "            )\n",
    "            sumthingwrong.text == \"Something went wrong. Try reloading.\"\n",
    "            return True\n",
    "        \n",
    "        except TimeoutException:\n",
    "            return False\n",
    "\n",
    "    def wait_for_posts_or_stepback(self, current_date: str, counter: int) -> tuple[str, int, bool]:\n",
    "        try:\n",
    "            # Is there a container for post?\n",
    "            WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\"))\n",
    "            )\n",
    "            counter = 0\n",
    "            return current_date, counter, False # Yes\n",
    "        except TimeoutException:\n",
    "            current_date = minOneDay(current_date)\n",
    "            counter += 1\n",
    "            print(f\"No posts found, stepping back to {current_date}, attempt {counter + 1}/{self.MAX_EMPTY_PAGES}\")\n",
    "            return current_date, counter, counter > self.MAX_EMPTY_PAGES    # Nothing ever happens and minus by one day\n",
    "\n",
    "    def parse_post(self, post_element) -> str:\n",
    "        parts = post_element.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "        text = \"\"\n",
    "        for p in parts:\n",
    "            if p.tag_name == \"img\":\n",
    "                text += p.get_attribute(\"alt\")\n",
    "            elif p.tag_name == \"a\":\n",
    "                text += p.text + \" \"\n",
    "            else:\n",
    "                text += p.text\n",
    "        return text\n",
    "\n",
    "    def extract_post_data(self, element) -> tuple[str, str, str, str]:\n",
    "        post_element = element.find_element(\n",
    "            By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]',\n",
    "        )\n",
    "        post_text = self.parse_post(post_element)\n",
    "        try:\n",
    "            quoted_element = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "            quoted_text = ''.join(\n",
    "                i.text for i in quoted_element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            quoted_text = \"\"\n",
    "\n",
    "        post_date = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        post_user = element.find_element(By.XPATH, './/a/div/span').text\n",
    "        return post_text, quoted_text, post_user, post_date\n",
    "    \n",
    "    def _write_json(self, filename: str) -> None:\n",
    "        theDict_JSON = {i: {\n",
    "                j: self.theDict[j][i] for j in self.theDict.keys()\n",
    "            } for i in range(len(self.theDict[\"post_text\"]))\n",
    "            }\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(theDict_JSON, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _write_csv(self, filename: str) -> None:\n",
    "        df = pd.DataFrame(self.theDict)\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    def _load_latest_savepoint(self) -> bool:\n",
    "        save_dir = f\"Process/{self.processDir}/Savepoints\"\n",
    "        if not os.path.isdir(save_dir):\n",
    "            return False\n",
    "        files = [f for f in os.listdir(save_dir) if f.endswith((\".csv\", \".json\"))]\n",
    "        if not files:\n",
    "            return False\n",
    "        latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(save_dir, f)))\n",
    "        latest_path = os.path.join(save_dir, latest_file)\n",
    "\n",
    "        if latest_file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(latest_path)\n",
    "        else:\n",
    "            with open(latest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "\n",
    "        self.theDict = {col: df[col].tolist() for col in df.columns}\n",
    "        if \"Date\" in self.theDict and self.theDict[\"Date\"]:\n",
    "            try:\n",
    "                earliest_dt = min(\n",
    "                    datetime.strptime(d, \"%Y-%m-%d-%H:%M:%S\")\n",
    "                    for d in self.theDict[\"Date\"]\n",
    "                    if isinstance(d, str)\n",
    "                )\n",
    "                self.start_date = earliest_dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"Resumed from savepoint: {latest_file}\")\n",
    "        return True\n",
    "\n",
    "    def save(self, type: Literal[\"final\", \"savepoint\"]) -> str:\n",
    "        if type not in {\"final\", \"savepoint\"}:\n",
    "            raise ValueError(\"Save type must be 'final' or 'savepoint'.\")\n",
    "\n",
    "        os.makedirs(f\"Process/{self.processDir}/Savepoints\", exist_ok=True)\n",
    "        \n",
    "        if type == \"savepoint\":\n",
    "            save_path = f\"Process/{self.processDir}/Savepoints/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        else:\n",
    "            save_path = f\"Process/{self.processDir}/Final\"\n",
    "\n",
    "        if self.saveFormat == \"csv\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "        elif self.saveFormat == \"json\":\n",
    "            self._write_json(f\"{save_path}.json\")\n",
    "        elif self.saveFormat == \"both\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "            self._write_json(f\"{save_path}.json\")\n",
    "        else:\n",
    "            raise ValueError(\"saveFormat must be 'csv', 'json', or 'both'.\")\n",
    "        \n",
    "        return save_path\n",
    "    \n",
    "    def login(self):\n",
    "        '''\n",
    "        Tries to login to the account based from the given credentials\n",
    "\n",
    "        Will use `self.username` and `self.password` only. However, if X detects your login as suspicious, `self.email` will be used.\n",
    "        '''\n",
    "        # Initialize the driver and check bot detection\n",
    "        self.driver = uc.Chrome()\n",
    "        self.driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "        # Check bot detection, \n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "        time.sleep(4)\n",
    "        botResult = self.driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "        if botResult != \"Normal\":\n",
    "            warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "        # Get to X login page\n",
    "        self.driver.get(\"https://x.com/i/flow/login\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Login handling\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(self.username)\n",
    "        self.driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # if email is needed. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "        try:\n",
    "            if self.driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "                print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "                if self.email is None:\n",
    "                    raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "                self.driver.find_element(By.XPATH, \"//input\").send_keys(self.email)\n",
    "                self.driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Put password\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(self.password)\n",
    "        self.driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "        print(\"Login sucess!\")\n",
    "        wait(10)\n",
    "\n",
    "    def start(self, filters, startDate: str = \"\", endDate: str = \"\",\n",
    "              scraping_Params  =  {\"wait_short\": 10, \"wait_long\": 30,\n",
    "                                  \"detection_wait\": 900, \"max_empty_pages\": 2},\n",
    "                                  saveFormat: Literal[\"csv\", \"json\", \"both\"] = \"csv\", \n",
    "                                  autoSave: bool = False, autoSaveInterval: int = 15, continue_if_timeout: bool = True,\n",
    "                                  processDir: str = \"\", resume_from_savepoint: bool = True):\n",
    "        \n",
    "        '''\n",
    "        This function is used to start the scrapping process based on the given filters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filters : dict\n",
    "            - A dictionary containing the filters for scrapping.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"keywords\": \"your keywords\",\n",
    "                \"from\": \"username\",\n",
    "                \"to\": \"username\",\n",
    "                \"mentions\": \"username\",\n",
    "                \"since\": \"YYYY-MM-DD\",\n",
    "                \"until\": \"YYYY-MM-DD\",\n",
    "                \"min_replies\": int,\n",
    "                \"min_likes\": int,\n",
    "                \"min_reposts\": int,\n",
    "                \"min_views\": int,\n",
    "                \"has_links\": bool,\n",
    "                \"has_media\": bool,\n",
    "                \"has_photos\": bool,\n",
    "                \"has_videos\": bool,\n",
    "                \"has_gifs\": bool\n",
    "            }\n",
    "            ```\n",
    "        '''\n",
    "        self.SEARCH_URL = \"https://x.com/search?q=\"\n",
    "        \n",
    "                # Adjust filters values\n",
    "        filters[\"this_exact_phrase\"] = f'\\\"{filters[\"this_exact_phrase\"]}\\\"' if filters[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "\n",
    "        _any_terms = []\n",
    "        _any_raw = filters[\"any_of_these_words\"].strip()\n",
    "        if _any_raw:\n",
    "            for g1, g2, g3 in re.findall(r'\"([^\"]+)\"|\\'([^\\']+)\\'|(\\S+)', _any_raw):\n",
    "                _any_terms.append(g1 or g2 or g3)\n",
    "        filters[\"any_of_these_words\"] = (\n",
    "            f'({\" OR \".join(_any_terms)})' if _any_terms else \"\"\n",
    "        )\n",
    "\n",
    "        filters[\"none_of_these_words\"] = f'{\" \".join(f\"-{i}\" for i in filters[\"none_of_these_words\"].split())}' if filters[\"none_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"these_hashtags\"] = f'({\" OR \".join(f\"{i}\" for i in filters[\"these_hashtags\"].split())})' if filters[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"from_accounts\"] = f'({\" OR \".join(f\"from:{i}\" for i in filters[\"from_accounts\"].split())})' if filters[\"from_accounts\"] != \"\" else \"\"\n",
    "        filters[\"to_accounts\"] = f'({\" OR \".join(f\"to:{i}\" for i in filters[\"to_accounts\"].split())})' if filters[\"to_accounts\"] != \"\" else \"\"\n",
    "        filters[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in filters[\"mentioning_accounts\"].split())})' if filters[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"replies\"] = \"\" if filters[\"replies\"] else \"-filter:replies\" \n",
    "        filters[\"links\"] = \"\" if filters[\"links\"] else \"-filter:links\"\n",
    "\n",
    "        filters[\"Minimum_replies\"] = f'min_replies:{filters[\"Minimum_replies\"]}' if filters[\"Minimum_replies\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_likes\"] = f'min_faves:{filters[\"Minimum_likes\"]}' if filters[\"Minimum_likes\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_retweets\"] = f'min_retweets:{filters[\"Minimum_retweets\"]}' if filters[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "        FILTERS_COMBINATION = \"\"\n",
    "        for _, value in filters.items():\n",
    "            if value != \"\":\n",
    "                FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "        self.FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "\n",
    "        # Dates handling\n",
    "        if startDate == \"\":\n",
    "            startDate = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if endDate == \"\":\n",
    "            endDate = \"2006-01-01\"  # Twitter launch date\n",
    "        self.start_date = startDate\n",
    "        self.end_date = endDate\n",
    "\n",
    "        # scraping params\n",
    "        self.WAIT_SHORT = scraping_Params[\"wait_short\"]\n",
    "        self.WAIT_LONG = scraping_Params[\"wait_long\"]\n",
    "        self.DETECTION_WAIT = scraping_Params[\"detection_wait\"]\n",
    "        self.MAX_EMPTY_PAGES = scraping_Params[\"max_empty_pages\"]\n",
    "\n",
    "        # Other params\n",
    "        self.saveFormat = saveFormat\n",
    "        self.autoSave = autoSave\n",
    "        self.autoSaveInterval = autoSaveInterval\n",
    "        self.continue_if_timeout = continue_if_timeout\n",
    "        self.processDir = processDir if processDir != \"\" else datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        if resume_from_savepoint:\n",
    "            self._load_latest_savepoint()\n",
    "        \n",
    "        self.scrape()\n",
    "        \n",
    "    def scrape(self):\n",
    "        '''\n",
    "        This function is used to scrape the posts based on the given filters and date range.\n",
    "        '''\n",
    "        reached_all_posts = False\n",
    "        counter = 0\n",
    "        seen = set()\n",
    "        if all(k in self.theDict for k in (\"post_text\", \"Date\", \"User\")) and self.theDict[\"post_text\"]:\n",
    "            seen = {\n",
    "                (self.theDict[\"post_text\"][i], self.theDict[\"Date\"][i], self.theDict[\"User\"][i])\n",
    "                for i in range(len(self.theDict[\"post_text\"]))\n",
    "            }\n",
    "        start_date = self.start_date\n",
    "        try:\n",
    "            while True:\n",
    "\n",
    "                # Get the current date upper limit\n",
    "                current_date_limit = start_date\n",
    "\n",
    "                # If all shits been scraped, will save and break\n",
    "                if reached_all_posts:\n",
    "                    print(\"All posts have been scraped!\")\n",
    "                    # Delete all temps aka Savepoints\n",
    "                    shutil.rmtree(f'Process/{self.processDir}/Savepoints/')\n",
    "                    self.save(\"final\")\n",
    "                    break\n",
    "\n",
    "                self.driver.get(self.build_search_url(current_date_limit))\n",
    "                time.sleep(self.WAIT_SHORT)\n",
    "\n",
    "                # CHECKER\n",
    "                ##  1 CHECKER FOR SCRAPING DETECTION, IF `continue_if_timeout` IS TRUE, WILL WAIT AND CONTINUE, ELSE WILL JUST STOP.\n",
    "                if self.continue_if_timeout:\n",
    "                    if self.scrape_detected():\n",
    "                        print(\"Scraping detected! Auto-saving progress...\")\n",
    "                        self.save(\"savepoint\")\n",
    "                        print(f\"Waiting for {self.DETECTION_WAIT} seconds\")\n",
    "                        # Wait for abyssmal amount of time\n",
    "                        wait(self.DETECTION_WAIT)\n",
    "                        continue\n",
    "\n",
    "                else:\n",
    "                    if self.scrape_detected():\n",
    "                        self.save(\"savepoint\")\n",
    "                        raise RuntimeError(\"Scraping detected! All progress have been saved.\")\n",
    "\n",
    "                ##  2 CHECKER FOR NO POSTS FOUND, IF SHIT HAPPENS WILL ROLE BACK FOR LIKE A DAY. IF SHIT KEEPS HAPPENING TILL `MAX_EMPTY_PAGES``, WILL STOP.\n",
    "                start_date, counter, reached_all_posts = self.wait_for_posts_or_stepback(start_date, counter)\n",
    "                if reached_all_posts:\n",
    "                    print(\"No more posts found!\")\n",
    "                    continue\n",
    "\n",
    "                last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                while True:\n",
    "                    elements = self.driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "\n",
    "                    for element in elements[:-1]:\n",
    "                        try:\n",
    "                            post_text, quoted_text, post_user, post_date = self.extract_post_data(element)\n",
    "                        except (NoSuchElementException, StaleElementReferenceException):\n",
    "                            continue\n",
    "\n",
    "                        key = (post_text, post_date, post_user)\n",
    "                        if key in seen:\n",
    "                            continue\n",
    "\n",
    "                        # If end date is reached, functional if user specified end_date at self.start()\n",
    "                        if self.theDict[\"Date\"] and getTime(self.theDict[\"Date\"][-1]) < getTime(self.end_date):\n",
    "                            reached_all_posts = True\n",
    "                            break\n",
    "                        \n",
    "                        seen.add(key)\n",
    "                        self.theDict[\"post_text\"].append(post_text)\n",
    "                        self.theDict[\"quotedPost_text\"].append(quoted_text)\n",
    "                        self.theDict[\"User\"].append(post_user)\n",
    "                        self.theDict[\"Date\"].append(post_date)\n",
    "\n",
    "                        for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                            self.theDict[\"Reply_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"Repost_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"Like_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"View_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "\n",
    "                        if self.autoSave and len(seen) % self.autoSaveInterval == 0:\n",
    "                            self.save(\"savepoint\")\n",
    "                        \n",
    "                    if reached_all_posts:\n",
    "                        break\n",
    "                        \n",
    "                    self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(self.WAIT_SHORT)\n",
    "                    new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                    if new_height == last_height:\n",
    "                        if self.theDict[\"Date\"]:\n",
    "                            last_date_only = \"-\".join(self.theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                            if getTime(last_date_only) >= getTime(current_date_limit):\n",
    "                                start_date = minOneDay(last_date_only)\n",
    "                            else:\n",
    "                                start_date = last_date_only\n",
    "                        else:\n",
    "                            start_date = minOneDay(current_date_limit)\n",
    "\n",
    "                        self.start_date = start_date\n",
    "                        break\n",
    "\n",
    "                    last_height = new_height\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Auto-saving progress before exiting...\")\n",
    "            self.save(\"savepoint\")\n",
    "            self.driver.quit()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = twitterScrapper(\"Credentials/twitter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\\'Makan Bergizi Gratis\\' \\'MBG\\'\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"grok\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",            # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"\",          # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All posts have been scraped!\n"
     ]
    }
   ],
   "source": [
    "session.start(\n",
    "    endDate=\"2025-12-31\",\n",
    "    filters=FILTERS,\n",
    "    saveFormat=\"both\",\n",
    "    autoSave=True,\n",
    "    autoSaveInterval=100,\n",
    "    continue_if_timeout=True,\n",
    "    processDir=\"MBG\",\n",
    "    resume_from_savepoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual, deprecrated in favour of the syntax above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1st fucking option to handle the change\n",
    "\n",
    "# import zendriver as zd\n",
    "\n",
    "\n",
    "# browser =  await zd.start()\n",
    "# await browser.get(\"https://www.browserscan.net/bot-detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rhey\\anaconda3\\envs\\scraper\\Lib\\re\\_compiler.py:309: RuntimeWarning: coroutine 'start' was never awaited\n",
      "  break\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# # 2nd method, which works and also compactivle with selenium function and syntax\n",
    "\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# driver = uc.Chrome()\n",
    "\n",
    "# driver.get('https://www.browserscan.net/bot-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Credentials/twitter.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "    username = credentials[\"username\"]\n",
    "    password = credentials[\"password\"]\n",
    "    email = credentials[\"email\"]\n",
    "\n",
    "# Initialize the driver and check bot detection\n",
    "driver = uc.Chrome()\n",
    "driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "# Check bot detection, \n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "time.sleep(4)\n",
    "botResult = driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "if botResult != \"Normal\":\n",
    "    warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "# Get to X login page\n",
    "driver.get(\"https://x.com/i/flow/login\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Login handling\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(username)\n",
    "driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "time.sleep(5)               # This will wait for the next login pop-up to appear\n",
    "\n",
    "# if email is neeeded. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "try:\n",
    "    if driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "        print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "        if email is None:\n",
    "            raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "        driver.find_element(By.XPATH, \"//input\").send_keys(email)\n",
    "        driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Put password\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(password)\n",
    "driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "print(\"Login sucess!\")\n",
    "wait(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://x.com/search?q=%28MBG%20OR%20Makan%20Bergizi%20Gratis%29&f=live&src=typed_query'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "FILTES = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\\'MBG\\' \\'Makan Bergizi Gratis\\'\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",           # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"\",   # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}\n",
    "\n",
    "# Adjust filters values\n",
    "FILTES[\"this_exact_phrase\"] = f'\\\"{FILTES[\"this_exact_phrase\"]}\\\"' if FILTES[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "\n",
    "_any_terms = []\n",
    "_any_raw = FILTES[\"any_of_these_words\"].strip()\n",
    "if _any_raw:\n",
    "    for g1, g2, g3 in re.findall(r'\"([^\"]+)\"|\\'([^\\']+)\\'|(\\S+)', _any_raw):\n",
    "        _any_terms.append(g1 or g2 or g3)\n",
    "FILTES[\"any_of_these_words\"] = (\n",
    "    f'({\" OR \".join(_any_terms)})' if _any_terms else \"\"\n",
    " )\n",
    "\n",
    "FILTES[\"none_of_these_words\"] = f'{\" \".join(f\"-{i}\" for i in FILTES[\"none_of_these_words\"].split())}' if FILTES[\"none_of_these_words\"] != \"\" else \"\"\n",
    "FILTES[\"these_hashtags\"] = f'({\" OR \".join(f\"{i}\" for i in FILTES[\"these_hashtags\"].split())})' if FILTES[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"from_accounts\"] = f'({\" OR \".join(f\"from:{i}\" for i in FILTES[\"from_accounts\"].split())})' if FILTES[\"from_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"to_accounts\"] = f'({\" OR \".join(f\"to:{i}\" for i in FILTES[\"to_accounts\"].split())})' if FILTES[\"to_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in FILTES[\"mentioning_accounts\"].split())})' if FILTES[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"replies\"] = \"\" if FILTES[\"replies\"] else \"-filter:replies\" \n",
    "FILTES[\"links\"] = \"\" if FILTES[\"links\"] else \"-filter:links\"\n",
    "\n",
    "FILTES[\"Minimum_replies\"] = f'min_replies:{FILTES[\"Minimum_replies\"]}' if FILTES[\"Minimum_replies\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_likes\"] = f'min_faves:{FILTES[\"Minimum_likes\"]}' if FILTES[\"Minimum_likes\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_retweets\"] = f'min_retweets:{FILTES[\"Minimum_retweets\"]}' if FILTES[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "\n",
    "\n",
    "FILTERS_COMBINATION = \"\"\n",
    "for _, value in FILTES.items():\n",
    "    if value != \"\":\n",
    "        FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "SEARCH_URL_WITH_FILTERS = f\"{SEARCH_URL}{FILTERS_COMBINATION}&f=live&src=typed_query\"\n",
    "SEARCH_URL_WITH_FILTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "            \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts found!\n",
      "All posts have been scraped!\n"
     ]
    }
   ],
   "source": [
    "# CASE 1, no startdate and enddate given.\n",
    "\n",
    "timeout = False\n",
    "reachedallPost = False \n",
    "continueifTimeout = True\n",
    "counter = 0\n",
    "startDate = datetime.now().strftime(\"%Y-%m-%d\") # Get ur current date\n",
    "\n",
    "while True:\n",
    "    currentSearchDate_limit = startDate\n",
    "\n",
    "    if  reachedallPost:\n",
    "        print(\"All posts have been scraped!\")\n",
    "        pd.DataFrame(theDict).to_csv(f'testRhey.csv', index=False)\n",
    "        break\n",
    "\n",
    "    # Adjust Link\n",
    "    searchLink = f\"{SEARCH_URL}{FILTERS_COMBINATION}{quote(f\" until:{currentSearchDate_limit}\")}&f=live&src=typed_query\"\n",
    "    driver.get(searchLink)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # CHECKERS!\n",
    "    # This will check if scrapping is detected, if it is. It'll wait for 20 mins\n",
    "    if continueifTimeout:\n",
    "        try:\n",
    "            #   This is essential to check if the scrapping is detected\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            print(\"Saving!\")\n",
    "            pd.DataFrame(theDict).to_csv(f'Savepoints/{str(datetime.now())}.csv', index=False)\n",
    "            print(\"Scrapping detected, waiting for 15 mins\")\n",
    "            wait(900)\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            timeout = False\n",
    "        except:\n",
    "            pass\n",
    "        if timeout: break\n",
    "\n",
    "    # This will wait for the page to load, if nothing exists. Minus one day and repeat\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\")))\n",
    "    except:\n",
    "        startDate = minOneDay(startDate)\n",
    "        if counter == 2:\n",
    "            print(\"No more posts found!\")\n",
    "            reachedallPost = True\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # If everything's set, will scrape posts\n",
    "    while True:\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "        \n",
    "        for element in elements[:-1]:\n",
    "\n",
    "            # Get the actual fucking post and its fuckin text an emoji\n",
    "            try:\n",
    "                postElement = element.find_element(By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]')\n",
    "                parts = postElement.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "                postText = \"\"\n",
    "                for p in parts:\n",
    "                    if p.tag_name == \"img\":\n",
    "                        postText += p.get_attribute(\"alt\")\n",
    "                    elif p.tag_name == \"a\":\n",
    "                        postText += p.postText + \" \"\n",
    "                    else:\n",
    "                        postText += p.postText\n",
    "            except:\n",
    "                # If no text, then continue. Tf ima do with just attachments\n",
    "                continue\n",
    "\n",
    "            # Get the fucking quoted post if there is one\n",
    "            try:\n",
    "                quotedPostElement = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "                quotedPostText = ''.join([i.text for i in quotedPostElement.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')])\n",
    "            except:\n",
    "                quotedPostText = \"\"\n",
    "                pass\n",
    "\n",
    "            # Check if text exists, if not then continue.\n",
    "            postDate = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "            postUser = element.find_element(By.XPATH, './/a/div/span').text\n",
    "\n",
    "            # Check for duplicates, same text, date, and user. If yes, ignore that shit\n",
    "            if postText in theDict[\"post_text\"] and postDate in theDict[\"Date\"] and postUser in theDict[\"User\"]:\n",
    "                continue\n",
    "\n",
    "            # append\n",
    "            theDict[\"post_text\"].append(postText)\n",
    "            theDict[\"quotedPost_text\"].append(quotedPostText)\n",
    "            theDict[\"User\"].append(postUser)\n",
    "            theDict[\"Date\"].append(postDate)\n",
    "            \n",
    "            # post attrs\n",
    "            for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                theDict[\"Reply_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Repost_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Like_count\"].append(int(re.search(r'\\d+',group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"View_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))[0]) if re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\")) else 0)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # will break if the scrollbar is at the bottom\n",
    "        if new_height == last_height:\n",
    "            \n",
    "            startDate = '-'.join(theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "            # This will check if the untilDate is the same as endDate, if it is. It'll minus one day.\n",
    "            if currentSearchDate_limit == startDate:\n",
    "                startDate = minOneDay(startDate)\n",
    "            \n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
