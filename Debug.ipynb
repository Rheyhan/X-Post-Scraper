{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import re\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import shutil\n",
    "\n",
    "# Scrapping and crawling modules\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "\n",
    "# New chromediver\n",
    "import undetected_chromedriver as uc\n",
    "from requests.utils import quote, unquote\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "\n",
    "lang_codes = {'Arabic': 'ar',\n",
    " 'Arabic (Feminine)': 'ar-x-fm',\n",
    " 'Bangla': 'bn',\n",
    " 'Basque': 'eu',\n",
    " 'Bulgarian': 'bg',\n",
    " 'Catalan': 'ca',\n",
    " 'Croatian': 'hr',\n",
    " 'Czech': 'cs',\n",
    " 'Danish': 'da',\n",
    " 'Dutch': 'nl',\n",
    " 'English': 'en',\n",
    " 'Finnish': 'fi',\n",
    " 'French': 'fr',\n",
    " 'German': 'de',\n",
    " 'Greek': 'el',\n",
    " 'Gujarati': 'gu',\n",
    " 'Hebrew': 'he',\n",
    " 'Hindi': 'hi',\n",
    " 'Hungarian': 'hu',\n",
    " 'Indonesian': 'id',\n",
    " 'Italian': 'it',\n",
    " 'Japanese': 'ja',\n",
    " 'Kannada': 'kn',\n",
    " 'Korean': 'ko',\n",
    " 'Marathi': 'mr',\n",
    " 'Norwegian': 'no',\n",
    " 'Persian': 'fa',\n",
    " 'Polish': 'pl',\n",
    " 'Portuguese': 'pt',\n",
    " 'Romanian': 'ro',\n",
    " 'Russian': 'ru',\n",
    " 'Serbian': 'sr',\n",
    " 'Simplified Chinese': 'zh-cn',\n",
    " 'Slovak': 'sk',\n",
    " 'Spanish': 'es',\n",
    " 'Swedish': 'sv',\n",
    " 'Tamil': 'ta',\n",
    " 'Thai': 'th',\n",
    " 'Traditional Chinese': 'zh-tw',\n",
    " 'Turkish': 'tr',\n",
    " 'Ukrainian': 'uk',\n",
    " 'Urdu': 'ur',\n",
    " 'Vietnamese': 'vi'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(str: str) -> datetime:\n",
    "    '''\n",
    "    Convert string of datetime in isoformat to datetime object and adjust the timezone\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    - str: str\n",
    "        String of datetime in isoformat\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    - date_time_obj: datetime\n",
    "        Datetime object adjusted to UTC+7\n",
    "    '''\n",
    "    date_time_obj = datetime.fromisoformat(str)\n",
    "    date_time_obj =date_time_obj + timedelta(hours=7)   # Fuckass X timezone needs to be adjusted to UTC+7. Might as well make it adjustable based on user timezone later\n",
    "\n",
    "    return date_time_obj\n",
    "\n",
    "def minOneDay(str: str) -> str:\n",
    "    '''\n",
    "    This function is used to subtract one day from the given date\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    - str: str\n",
    "        String of datetime in isoformat\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    - str: str\n",
    "        String of datetime in isoformat minus one day\n",
    "    '''\n",
    "    str = datetime.strptime(str, \"%Y-%m-%d\")\n",
    "    str = str - timedelta(days=1)\n",
    "    str = str.strftime(\"%Y-%m-%d\")\n",
    "    return str\n",
    "\n",
    "def wait(timeout: int = 10) -> None:\n",
    "    '''\n",
    "    just a glorified simple function to wait for a certain amount of time with a progress bar.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    - timeout: int\n",
    "        Time to wait in seconds\n",
    "    '''\n",
    "    for _ in tqdm(range(timeout), desc=\"Waiting\"):\n",
    "        time.sleep(1)\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "def safe_int_from_aria(aria_label: str) -> int:\n",
    "    '''\n",
    "    Safely extract an integer from an aria-label string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - aria_label : str\n",
    "        The aria-label string containing the number.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - int\n",
    "        The extracted integer from the aria-label string. Returns 0 if no integer is found.\n",
    "    '''\n",
    "    match = re.search(r\"\\d+\", aria_label or \"\")\n",
    "    return int(match.group(0)) if match else 0\n",
    "\n",
    "\n",
    "\n",
    "class twitterScrapper:\n",
    "    '''\n",
    "    This class is used to scrape posts from X (formerly known as Twitter) based on given filters and date range.\n",
    "    \n",
    "    Methods\n",
    "    ----------\n",
    "    - login()\n",
    "        - Logs into Twitter using the provided credentials\n",
    "    - start()\n",
    "        - Starts the scraping process based on the given filters and date range\n",
    "    '''\n",
    "\n",
    "    def __init__(self, credentials: str = \"Credentials/twitter.json\"):\n",
    "        '''\n",
    "        This function is used to initialize the class and will also login to twitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        credentials : str\n",
    "            - Path to the twitter credentials json file\n",
    "            - The json file should be in the following format:\n",
    "            ```\n",
    "            {   \"username\" : \"your_username\",\n",
    "                \"password\" : \"your_password\",\n",
    "                \"email\"    : \"your_email\"}\n",
    "            ```\n",
    "        '''\n",
    "        \n",
    "        # Bunch of checkers and loaders for credentials\n",
    "        if not os.path.exists(credentials):\n",
    "            raise FileNotFoundError(\"Credentials file not found!\")\n",
    "        with open(credentials, \"r\") as f:\n",
    "            credentials = json.load(f)\n",
    "            try:\n",
    "                username = credentials[\"username\"]\n",
    "                password = credentials[\"password\"]\n",
    "                email = credentials[\"email\"]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Credentials file is not in the correct format!\")\n",
    "        \n",
    "        # Check if any of those credentials is None\n",
    "        if username is None or password is None:\n",
    "            raise ValueError(\"Username or password can't be empty!\")\n",
    "        if email is None:\n",
    "            warnings.warn(\"Email is not provided, this shit might not work if suspicious login attempt is detected!\", UserWarning)\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "\n",
    "        # For storing all the data during scraping\n",
    "        self.theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "                         \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}\n",
    "        \n",
    "        self.login()\n",
    "\n",
    "    # Utilities for this class\n",
    "    def _build_search_url(self, date_limit: str) -> str:\n",
    "        '''\n",
    "        Build the search URL based on the Filters and given datelimit\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - date_limit : str\n",
    "            The date limit for the search in the format \"YYYY-MM-DD\".\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - str\n",
    "            The constructed search URL.\n",
    "        '''\n",
    "        return f\"{self.SEARCH_URL}{self.FILTERS_COMBINATION}{quote(f' until:{date_limit}')}&f=live&src=typed_query\"\n",
    "\n",
    "    def _scrape_detected(self) -> bool:\n",
    "        '''\n",
    "        Check if scraping is detected by looking for the \"Something went wrong. Try reloading.\" message. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        - bool\n",
    "            True if scraping is detected (based on the presence of the error message), False otherwise.\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            sumthingwrong = WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                 EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span'))\n",
    "            )\n",
    "            sumthingwrong.text == \"Something went wrong. Try reloading.\"\n",
    "            return True\n",
    "        \n",
    "        except TimeoutException:\n",
    "            return False\n",
    "\n",
    "    def _wait_for_posts(self, current_date: str, counter: int) -> tuple[str, int, bool]:\n",
    "        '''\n",
    "        Wait for posts to load on the page. If no posts are found within the timeout period, step back one day and increment the counter.\n",
    "\n",
    "        The counter tracks the number of consecutive days with no posts found. If the counter exceeds the maximum allowed empty pages, the function indicates that all posts have been reached.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - current_date : str\n",
    "            The current date being checked in the format \"YYYY-MM-DD\".\n",
    "        - counter : int\n",
    "            The number of consecutive days with no posts found.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - tuple[str, int, bool]\n",
    "            A tuple containing the updated current date, the updated counter, and a boolean indicating whether the maximum number of empty pages has been reached.\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            # Is there a container for post?\n",
    "            WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\"))\n",
    "            )\n",
    "            counter = 0\n",
    "            return current_date, counter, False # Yes\n",
    "        except TimeoutException:\n",
    "            current_date = minOneDay(current_date)\n",
    "            counter += 1\n",
    "            print(f\"No posts found, stepping back to {current_date}, attempt {counter + 1}/{self.MAX_EMPTY_PAGES}\")\n",
    "            return current_date, counter, counter > self.MAX_EMPTY_PAGES    # Nothingburger and minus by one day\n",
    "\n",
    "    def _parse_post(self, post_element) -> str:\n",
    "        '''\n",
    "        Parse the post element to extract the full text of the post.\n",
    "\n",
    "        NOTE: This only handles text, links, emojis, and such. Media (images, videos, gifs) are not handled ***yet***.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - post_element : WebElement\n",
    "            The WebElement representing the post.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - str\n",
    "            The full text of the post.\n",
    "        '''\n",
    "\n",
    "        parts = post_element.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "        text = \"\"\n",
    "        for p in parts:\n",
    "            if p.tag_name == \"img\":                 # Emojis\n",
    "                text += p.get_attribute(\"alt\")\n",
    "            elif p.tag_name == \"a\":                 # Links (Not shortened with t.co domain)\n",
    "                text += p.text + \" \"\n",
    "            else:                                   # Normal text                 \n",
    "                text += p.text\n",
    "        return text\n",
    "\n",
    "    def _extract_post_data(self, element) -> tuple[str, str, str, str]:\n",
    "        '''\n",
    "        Extract post data from the given post element.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - element : WebElement\n",
    "            The WebElement representing the post.\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        - tuple[str, str, str, str]\n",
    "            A tuple containing the post text, quoted post text, post user, and post date.\n",
    "        '''\n",
    "\n",
    "        # Get the entire post element\n",
    "        post_element = element.find_element(\n",
    "            By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]',\n",
    "        )\n",
    "        # Post\n",
    "        post_text = self._parse_post(post_element)\n",
    "\n",
    "        # Quoted Post\n",
    "        try:\n",
    "            quoted_element = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "            quoted_text = ''.join(\n",
    "                i.text for i in quoted_element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            quoted_text = \"\"\n",
    "\n",
    "        post_date = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        post_user = element.find_element(By.XPATH, './/a/div/span').text\n",
    "\n",
    "        return post_text, quoted_text, post_user, post_date\n",
    "    \n",
    "    def _write_json(self, filename: str) -> None:\n",
    "        '''\n",
    "        Write the scraped data to a JSON file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        - filename : str\n",
    "            The name of the file to write the JSON data to.\n",
    "        '''\n",
    "        # theDict conversion to a list of dictionaries\n",
    "        theDict_JSON = {i: {\n",
    "                j: self.theDict[j][i] for j in self.theDict.keys()\n",
    "            } for i in range(len(self.theDict[\"post_text\"]))\n",
    "            }\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(theDict_JSON, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _write_csv(self, filename: str) -> None:\n",
    "        '''\n",
    "        Write the scraped data to a CSV file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - filename : str\n",
    "            The name of the file to write the CSV data to.\n",
    "        '''\n",
    "        df = pd.DataFrame(self.theDict)\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    def _load_latest_savepoint(self) -> bool:\n",
    "        '''\n",
    "        Load the latest savepoint from the Savepoints directory.\n",
    "\n",
    "        Uses the eariest date as the `self.start_date` from the most recently modified file in the Savepoints directory and loads the data into `self.theDict`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        - bool\n",
    "            True if a savepoint was loaded, False otherwise.\n",
    "        '''\n",
    "\n",
    "        save_dir = f\"Process/{self.processDir}/Savepoints\"\n",
    "\n",
    "        # Checkers if there's any savepoint\n",
    "        if not os.path.isdir(save_dir):\n",
    "            return False\n",
    "        files = [f for f in os.listdir(save_dir) if f.endswith((\".csv\", \".json\"))]\n",
    "        if not files:\n",
    "            return False\n",
    "        \n",
    "        # Get the latest savepoint file\n",
    "        latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(save_dir, f)))\n",
    "        latest_path = os.path.join(save_dir, latest_file)\n",
    "\n",
    "        if latest_file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(latest_path)\n",
    "        else:\n",
    "            with open(latest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "        \n",
    "        self.theDict = {col: df[col].tolist() for col in df.columns}    # Convert DataFrame to dictionary\n",
    "        if \"Date\" in self.theDict and self.theDict[\"Date\"]:\n",
    "            try:\n",
    "                earliest_dt = min(\n",
    "                    datetime.strptime(d, \"%Y-%m-%d-%H:%M:%S\")\n",
    "                    for d in self.theDict[\"Date\"]\n",
    "                    if isinstance(d, str)\n",
    "                )\n",
    "                self.start_date = earliest_dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        print(f\"Resumed from savepoint: {latest_file}\")\n",
    "        return True\n",
    "\n",
    "    def save(self, type: Literal[\"final\", \"savepoint\"]) -> str:\n",
    "        '''\n",
    "        Save the current progress to a file.\n",
    "\n",
    "        file extension and format is based on `self.saveFormat`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - type : Literal[\"final\", \"savepoint\"]\n",
    "            The type of save to perform. \"final\" for final save, \"savepoint\" for intermediate savepoint.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        - str\n",
    "            The path to the saved file.\n",
    "        '''\n",
    "        if type not in {\"final\", \"savepoint\"}:\n",
    "            raise ValueError(\"Save type must be 'final' or 'savepoint'.\")\n",
    "\n",
    "        os.makedirs(f\"Process/{self.processDir}/Savepoints\", exist_ok=True)\n",
    "        \n",
    "        if type == \"savepoint\":\n",
    "            save_path = f\"Process/{self.processDir}/Savepoints/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        else:\n",
    "            save_path = f\"Process/{self.processDir}/Final\"\n",
    "\n",
    "        if self.saveFormat == \"csv\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "        elif self.saveFormat == \"json\":\n",
    "            self._write_json(f\"{save_path}.json\")\n",
    "        elif self.saveFormat == \"both\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "            if type == \"final\":                         # There's no fucking logic to save both on savepoint, only do it on final save\n",
    "                self._write_json(f\"{save_path}.json\")\n",
    "        else:\n",
    "            raise ValueError(\"saveFormat must be 'csv', 'json', or 'both'.\")\n",
    "        \n",
    "        return save_path    # This ain't used, but yeah.\n",
    "    \n",
    "    def login(self) -> None:\n",
    "        '''\n",
    "        Tries to login to the account based from the given credentials\n",
    "\n",
    "        Will use `self.username` and `self.password` only. However, if X detects your login as suspicious, `self.email` will be used.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        - ValueError\n",
    "            If email is required due to suspicious login attempt, but email is not provided on credentials.\n",
    "        '''\n",
    "        # Initialize the driver and check bot detection\n",
    "        self.driver = uc.Chrome()\n",
    "        self.driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "        # Check bot detection, \n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "        time.sleep(4)\n",
    "        botResult = self.driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "        if botResult != \"Normal\":\n",
    "            warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "        # Get to X login page\n",
    "        self.driver.get(\"https://x.com/i/flow/login\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Login handling\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "        time.sleep(3)\n",
    "        username_input = self.driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\")\n",
    "        for i in self.username:\n",
    "            username_input.send_keys(i)\n",
    "            time.sleep(time.uniform(0.05, 0.2))\n",
    "        self.driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # if email is needed. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "        try:\n",
    "            if self.driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "                print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "                if self.email is None:\n",
    "                    raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "                email_input = self.driver.find_element(By.XPATH, \"//input\")\n",
    "                for i in self.email:\n",
    "                    email_input.send_keys(i)\n",
    "                    time.sleep(time.uniform(0.05, 0.2))\n",
    "                self.driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Put password\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "        password_input = self.driver.find_element(By.XPATH, \"//input[@name='password']\")\n",
    "        for i in self.password:\n",
    "            password_input.send_keys(i)\n",
    "            time.sleep(time.uniform(0.05, 0.2))\n",
    "        self.driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "        print(\"Login sucess!\")\n",
    "        wait(10)\n",
    "\n",
    "    def start(self, filters, startDate: str = \"\", endDate: str = \"\",\n",
    "              scraping_Params  =  {\"wait_short\": 10, \"wait_long\": 30,\n",
    "                                  \"detection_wait\": 900, \"max_empty_pages\": 2},\n",
    "                                  saveFormat: Literal[\"csv\", \"json\", \"both\"] = \"csv\", \n",
    "                                  autoSave: bool = False, autoSaveInterval: int = 15, continue_if_timeout: bool = True,\n",
    "                                  processDir: str = \"\", resume_from_savepoint: bool = True) -> None:\n",
    "        '''\n",
    "        This function is used to start the scrapping process based on the given filters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        - Filters : dict\n",
    "            - A dictionary containing the filters for scrapping.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"all_these_words\": \"\",\n",
    "                \"this_exact_phrase\": \"\",\n",
    "                \"any_of_these_words\": \"\",\n",
    "                \"none_of_these_words\": \"\",\n",
    "                \"these_hashtags\": \"\", \n",
    "                \"from_accounts\": \"\",            \n",
    "                \"to_accounts\": \"\",              \n",
    "                \"mentioning_accounts\": \"\",\n",
    "                \"language\": \"\",   \n",
    "                \"Minimum_replies\": \"\",\n",
    "                \"Minimum_likes\": \"\",\n",
    "                \"Minimum_retweets\": \"\",\n",
    "                \"links\": True,\n",
    "                \"replies\": True,                \n",
    "            }\n",
    "            ```\n",
    "        - startDate : str\n",
    "            - The latest date for scrapping in the format \"YYYY-MM-DD\".\n",
    "            - If empty, will default to current date.\n",
    "        \n",
    "        - endDate : str\n",
    "            - The earliest date for scrapping in the format \"YYYY-MM-DD\".\n",
    "            - If empty, will default to \"2006-01-01\" (Twitter launch date).\n",
    "        \n",
    "        - scraping_Params : dict\n",
    "            - A dictionary containing the scrapping parameters.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"wait_short\": 10,\n",
    "                \"wait_long\": 30,\n",
    "                \"detection_wait\": 900,\n",
    "                \"max_empty_pages\": 2\n",
    "            }\n",
    "            ```\n",
    "        - wait_short : int\n",
    "            - Short wait time in seconds for page loading. Used for normal page loads. E.g, scrollig down to load more posts\n",
    "\n",
    "        - wait_long : int\n",
    "            - Long wait time in seconds for page loading. Used for cases where the page takes longer to load. Like after getting into a new page\n",
    "            \n",
    "        - detection_wait : int\n",
    "            - Wait time in seconds when scraping detection is encountered.\n",
    "\n",
    "        - max_empty_pages : int\n",
    "            - Maximum number of consecutive empty pages before stopping scrapping.\n",
    "\n",
    "        - saveFormat : Literal[\"csv\", \"json\", \"both\"]\n",
    "            - The format to save the scrapped data. Can be \"csv\", \"json\", or \"both\".\n",
    "            - Default is \"csv\".\n",
    "\n",
    "        - autoSave : bool\n",
    "            - Whether to automatically save the scrapped data at regular intervals.\n",
    "            - Default is False.\n",
    "\n",
    "        - autoSaveInterval : int\n",
    "            - The interval to automatically save the scrapped data based on how many posts have been scraped.\n",
    "            - Default is 15.\n",
    "\n",
    "        - continue_if_timeout : bool\n",
    "            - Whether to continue scrapping if scraping detection is encountered.\n",
    "            - Default is True.\n",
    "\n",
    "        - processDir : str\n",
    "            - The directory to save the scrapped data.\n",
    "            - If empty, will default to current date in \"YYYY-MM-DD\" format.\n",
    "\n",
    "        - resume_from_savepoint : bool\n",
    "            - Whether to resume scrapping from the latest savepoint if available.\n",
    "            - Default is True.\n",
    "\n",
    "        '''\n",
    "        self.SEARCH_URL = \"https://x.com/search?q=\"\n",
    "        \n",
    "        # Adjust filters values\n",
    "        filters[\"this_exact_phrase\"] = f'\\\"{filters[\"this_exact_phrase\"]}\\\"' if filters[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "        _any_terms = []\n",
    "        _any_raw = filters[\"any_of_these_words\"].strip()\n",
    "        if _any_raw:\n",
    "            for g1, g2, g3 in re.findall(r'\"([^\"]+)\"|\\'([^\\']+)\\'|(\\S+)', _any_raw):\n",
    "                _any_terms.append(g1 or g2 or g3)\n",
    "        filters[\"any_of_these_words\"] = (\n",
    "            f'({\" OR \".join(_any_terms)})' if _any_terms else \"\"\n",
    "        )\n",
    "        filters[\"none_of_these_words\"] = f'{\" \".join(f\"-{i}\" for i in filters[\"none_of_these_words\"].split())}' if filters[\"none_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"these_hashtags\"] = f'({\" OR \".join(f\"{i}\" for i in filters[\"these_hashtags\"].split())})' if filters[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"from_accounts\"] = f'({\" OR \".join(f\"from:{i}\" for i in filters[\"from_accounts\"].split())})' if filters[\"from_accounts\"] != \"\" else \"\"\n",
    "        filters[\"to_accounts\"] = f'({\" OR \".join(f\"to:{i}\" for i in filters[\"to_accounts\"].split())})' if filters[\"to_accounts\"] != \"\" else \"\"\n",
    "        filters[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in filters[\"mentioning_accounts\"].split())})' if filters[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"language\"] = f'lang:{lang_codes[filters[\"language\"]]}' if filters[\"language\"] != \"\" else \"\"\n",
    "        filters[\"replies\"] = \"\" if filters[\"replies\"] else \"-filter:replies\" \n",
    "        filters[\"links\"] = \"\" if filters[\"links\"] else \"-filter:links\"\n",
    "\n",
    "        filters[\"Minimum_replies\"] = f'min_replies:{filters[\"Minimum_replies\"]}' if filters[\"Minimum_replies\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_likes\"] = f'min_faves:{filters[\"Minimum_likes\"]}' if filters[\"Minimum_likes\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_retweets\"] = f'min_retweets:{filters[\"Minimum_retweets\"]}' if filters[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "        FILTERS_COMBINATION = \"\"\n",
    "        for _, value in filters.items():\n",
    "            if value != \"\":\n",
    "                FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "        self.FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "\n",
    "        # Dates handling\n",
    "        if startDate == \"\":\n",
    "            startDate = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if endDate == \"\":\n",
    "            endDate = \"2006-01-01\"  # Twitter launch date\n",
    "        self.start_date = startDate\n",
    "        self.end_date = endDate\n",
    "\n",
    "        # scraping params\n",
    "        self.WAIT_SHORT = scraping_Params[\"wait_short\"]\n",
    "        self.WAIT_LONG = scraping_Params[\"wait_long\"]\n",
    "        self.DETECTION_WAIT = scraping_Params[\"detection_wait\"]\n",
    "        self.MAX_EMPTY_PAGES = scraping_Params[\"max_empty_pages\"]\n",
    "\n",
    "        # Other params\n",
    "        self.saveFormat = saveFormat\n",
    "        self.autoSave = autoSave\n",
    "        self.autoSaveInterval = autoSaveInterval\n",
    "        self.continue_if_timeout = continue_if_timeout\n",
    "        self.processDir = processDir if processDir != \"\" else datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        if resume_from_savepoint:\n",
    "            self._load_latest_savepoint()\n",
    "        \n",
    "        self.scrape()   # Immidiately start scraping right here right fucking now\n",
    "        \n",
    "    def scrape(self) -> None:\n",
    "        '''\n",
    "        Starts the scraping process.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        - RuntimeError\n",
    "            If scraping is detected and `continue_if_timeout` is False.\n",
    "        - Exception\n",
    "            For any other exceptions that occur during scraping including keyboard interrupts.\n",
    "        '''\n",
    "        reached_all_posts = False\n",
    "        counter = 0\n",
    "        seen = set()    # Uniqueness so there won't be a fuckton of duplicates\n",
    "\n",
    "        # If there's already data on self.theDict, populate seen set. Used for resuming from savepoint\n",
    "        if all(k in self.theDict for k in (\"post_text\", \"Date\", \"User\")) and self.theDict[\"post_text\"]:\n",
    "            seen = {\n",
    "                (self.theDict[\"post_text\"][i], self.theDict[\"Date\"][i], self.theDict[\"User\"][i])\n",
    "                for i in range(len(self.theDict[\"post_text\"]))\n",
    "            }\n",
    "        start_date = self.start_date\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "\n",
    "                # Get the current date upper limit\n",
    "                current_date_limit = start_date\n",
    "\n",
    "                # If all shits been scraped, will save and break\n",
    "                if reached_all_posts:\n",
    "                    print(\"All posts have been scraped!\")\n",
    "                    # Delete all temps aka Savepoints\n",
    "                    shutil.rmtree(f'Process/{self.processDir}/Savepoints/')\n",
    "                    self.save(\"final\")\n",
    "                    break\n",
    "\n",
    "                self.driver.get(self._build_search_url(current_date_limit))\n",
    "                time.sleep(self.WAIT_SHORT)\n",
    "\n",
    "                # CHECKER\n",
    "                ##  1 CHECKER FOR SCRAPING DETECTION, IF `continue_if_timeout` IS TRUE, WILL WAIT AND CONTINUE, ELSE WILL JUST STOP.\n",
    "                if self.continue_if_timeout:\n",
    "                    if self._scrape_detected():\n",
    "                        print(\"Scraping detected! Auto-saving progress...\")\n",
    "                        self.save(\"savepoint\")\n",
    "                        print(f\"Waiting for {self.DETECTION_WAIT} seconds\")\n",
    "                        # Wait for abyssmal amount of time\n",
    "                        wait(self.DETECTION_WAIT)\n",
    "                        continue\n",
    "\n",
    "                else:\n",
    "                    if self._scrape_detected():\n",
    "                        self.save(\"savepoint\")\n",
    "                        raise RuntimeError(\"Scraping detected! All progress have been saved.\")\n",
    "\n",
    "                ##  2 CHECKER FOR NO POSTS FOUND, IF SHIT HAPPENS WILL ROLE BACK FOR LIKE A DAY. IF SHIT KEEPS HAPPENING TILL `MAX_EMPTY_PAGES``, WILL STOP.\n",
    "                start_date, counter, reached_all_posts = self._wait_for_posts(start_date, counter)\n",
    "                if reached_all_posts:\n",
    "                    print(\"No more posts found!\")\n",
    "                    continue\n",
    "\n",
    "                last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                while True:\n",
    "                    elements = self.driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "\n",
    "                    for element in elements[:-1]:\n",
    "                        try:\n",
    "                            post_text, quoted_text, post_user, post_date = self._extract_post_data(element)\n",
    "                        except (NoSuchElementException, StaleElementReferenceException):\n",
    "                            continue\n",
    "\n",
    "                        key = (post_text, post_date, post_user)\n",
    "                        if key in seen:\n",
    "                            continue\n",
    "\n",
    "                        # If end date is reached, functional if user specified end_date at self.start()\n",
    "                        if self.theDict[\"Date\"] and getTime(self.theDict[\"Date\"][-1]) < getTime(self.end_date):\n",
    "                            reached_all_posts = True\n",
    "                            break\n",
    "                        \n",
    "                        seen.add(key)\n",
    "                        self.theDict[\"post_text\"].append(post_text)\n",
    "                        self.theDict[\"quotedPost_text\"].append(quoted_text)\n",
    "                        self.theDict[\"User\"].append(post_user)\n",
    "                        self.theDict[\"Date\"].append(post_date)\n",
    "\n",
    "                        for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                            self.theDict[\"Reply_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"Repost_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"Like_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "                            self.theDict[\"View_count\"].append(\n",
    "                                safe_int_from_aria(group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))\n",
    "                            )\n",
    "\n",
    "                        if self.autoSave and len(seen) % self.autoSaveInterval == 0:\n",
    "                            self.save(\"savepoint\")\n",
    "                        \n",
    "                    if reached_all_posts:\n",
    "                        break\n",
    "                        \n",
    "                    self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(self.WAIT_SHORT)\n",
    "                    new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                    if new_height == last_height:\n",
    "                        if self.theDict[\"Date\"]:\n",
    "                            last_date_only = \"-\".join(self.theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                            if getTime(last_date_only) >= getTime(current_date_limit):\n",
    "                                start_date = minOneDay(last_date_only)\n",
    "                            else:\n",
    "                                start_date = last_date_only\n",
    "                        else:\n",
    "                            start_date = minOneDay(current_date_limit)\n",
    "\n",
    "                        self.start_date = start_date\n",
    "                        break\n",
    "\n",
    "                    last_height = new_height\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Auto-saving progress before exiting...\")\n",
    "            self.save(\"savepoint\")\n",
    "            self.driver.quit()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = twitterScrapper(\"Credentials/myCreds.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"sawit\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"grok\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",            # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"\",          # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    \"language\": \"\",               # Example: lang:en · Tweets in English. If empty, will not filter by language.\n",
    "    \n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"10\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=144.0.7559.60)\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0xfc2993\n",
      "\t0xfc29d4\n",
      "\t0xdbb260\n",
      "\t0xdaa20e\n",
      "\t0xdc8cc3\n",
      "\t0xe2edfc\n",
      "\t0xe44be9\n",
      "\t0xe27d46\n",
      "\t0xdf94a9\n",
      "\t0xdfa264\n",
      "\t0x121a674\n",
      "\t0x12158ed\n",
      "\t0x123328d\n",
      "\t0xfdc198\n",
      "\t0xfe3f0d\n",
      "\t0xfcb158\n",
      "\t0xfcb322\n",
      "\t0xfb4e3a\n",
      "\t0x77215d49\n",
      "\t0x775ed5db\n",
      "\t0x775ed561\n",
      "\n",
      "Auto-saving progress before exiting...\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=144.0.7559.60)\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0xfc2993\n\t0xfc29d4\n\t0xdbb260\n\t0xdaa20e\n\t0xdc8cc3\n\t0xe2edfc\n\t0xe44be9\n\t0xe27d46\n\t0xdf94a9\n\t0xdfa264\n\t0x121a674\n\t0x12158ed\n\t0x123328d\n\t0xfdc198\n\t0xfe3f0d\n\t0xfcb158\n\t0xfcb322\n\t0xfb4e3a\n\t0x77215d49\n\t0x775ed5db\n\t0x775ed561\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidSessionIdException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFILTERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaveFormat\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoSave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoSaveInterval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_if_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessDir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest_run_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from_savepoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 576\u001b[39m, in \u001b[36mtwitterScrapper.start\u001b[39m\u001b[34m(self, filters, startDate, endDate, scraping_Params, saveFormat, autoSave, autoSaveInterval, continue_if_timeout, processDir, resume_from_savepoint)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resume_from_savepoint:\n\u001b[32m    574\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_latest_savepoint()\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 710\u001b[39m, in \u001b[36mtwitterScrapper.scrape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28mself\u001b[39m.save(\u001b[33m\"\u001b[39m\u001b[33msavepoint\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m.driver.quit()\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 635\u001b[39m, in \u001b[36mtwitterScrapper.scrape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    632\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScraping detected! All progress have been saved.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    634\u001b[39m \u001b[38;5;66;03m##  2 CHECKER FOR NO POSTS FOUND, IF SHIT HAPPENS WILL ROLE BACK FOR LIKE A DAY. IF SHIT KEEPS HAPPENING TILL `MAX_EMPTY_PAGES``, WILL STOP.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m start_date, counter, reached_all_posts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reached_all_posts:\n\u001b[32m    637\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo more posts found!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mtwitterScrapper._wait_for_posts\u001b[39m\u001b[34m(self, current_date, counter)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03mWait for posts to load on the page. If no posts are found within the timeout period, step back one day and increment the counter.\u001b[39;00m\n\u001b[32m    168\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m \u001b[33;03m    A tuple containing the updated current date, the updated counter, and a boolean indicating whether the maximum number of empty pages has been reached.\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# Is there a container for post?\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mWAIT_LONG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m//div[@data-testid=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcellInnerDiv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     counter = \u001b[32m0\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m current_date, counter, \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Yes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\selenium\\webdriver\\support\\wait.py:129\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         value = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:104\u001b[39m, in \u001b[36mpresence_of_element_located.<locals>._predicate\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:914\u001b[39m, in \u001b[36mWebDriver.find_element\u001b[39m\u001b[34m(self, by, value)\u001b[39m\n\u001b[32m    911\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby.root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:447\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    445\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mInvalidSessionIdException\u001b[39m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=144.0.7559.60)\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0xfc2993\n\t0xfc29d4\n\t0xdbb260\n\t0xdaa20e\n\t0xdc8cc3\n\t0xe2edfc\n\t0xe44be9\n\t0xe27d46\n\t0xdf94a9\n\t0xdfa264\n\t0x121a674\n\t0x12158ed\n\t0x123328d\n\t0xfdc198\n\t0xfe3f0d\n\t0xfcb158\n\t0xfcb322\n\t0xfb4e3a\n\t0x77215d49\n\t0x775ed5db\n\t0x775ed561\n"
     ]
    }
   ],
   "source": [
    "session.start(\n",
    "    filters=FILTERS,\n",
    "    saveFormat=\"both\",\n",
    "    autoSave=True,\n",
    "    autoSaveInterval=100,\n",
    "    continue_if_timeout=True,\n",
    "    processDir=\"test_run_1\",\n",
    "    resume_from_savepoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual, deprecrated in favour of the syntax above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1st fucking option to handle the change\n",
    "\n",
    "# import zendriver as zd\n",
    "\n",
    "\n",
    "# browser =  await zd.start()\n",
    "# await browser.get(\"https://www.browserscan.net/bot-detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rhey\\anaconda3\\envs\\scraper\\Lib\\re\\_compiler.py:309: RuntimeWarning: coroutine 'start' was never awaited\n",
      "  break\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# # 2nd method, which works and also compactivle with selenium function and syntax\n",
    "\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# driver = uc.Chrome()\n",
    "\n",
    "# driver.get('https://www.browserscan.net/bot-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Credentials/twitter.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "    username = credentials[\"username\"]\n",
    "    password = credentials[\"password\"]\n",
    "    email = credentials[\"email\"]\n",
    "\n",
    "# Initialize the driver and check bot detection\n",
    "driver = uc.Chrome()\n",
    "driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "# Check bot detection, \n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "time.sleep(4)\n",
    "botResult = driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "if botResult != \"Normal\":\n",
    "    warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "# Get to X login page\n",
    "driver.get(\"https://x.com/i/flow/login\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Login handling\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(username)\n",
    "driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "time.sleep(5)               # This will wait for the next login pop-up to appear\n",
    "\n",
    "# if email is neeeded. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "try:\n",
    "    if driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "        print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "        if email is None:\n",
    "            raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "        driver.find_element(By.XPATH, \"//input\").send_keys(email)\n",
    "        driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Put password\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(password)\n",
    "driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "print(\"Login sucess!\")\n",
    "wait(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://x.com/search?q=%22sawit%22%20-grok%20min_faves%3A10&f=live&src=typed_query'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "FILTES = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"sawit\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"grok\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",            # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"\",          # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    \"language\": \"\",               # Example: lang:en · Tweets in English. If empty, will not filter by language.\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"10\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}\n",
    "\n",
    "# Adjust filters values\n",
    "FILTES[\"this_exact_phrase\"] = f'\\\"{FILTES[\"this_exact_phrase\"]}\\\"' if FILTES[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "_any_terms = []\n",
    "_any_raw = FILTES[\"any_of_these_words\"].strip()\n",
    "if _any_raw:\n",
    "    for g1, g2, g3 in re.findall(r'\"([^\"]+)\"|\\'([^\\']+)\\'|(\\S+)', _any_raw):\n",
    "        _any_terms.append(g1 or g2 or g3)\n",
    "FILTES[\"any_of_these_words\"] = (\n",
    "    f'({\" OR \".join(_any_terms)})' if _any_terms else \"\"\n",
    ")\n",
    "FILTES[\"none_of_these_words\"] = f'{\" \".join(f\"-{i}\" for i in FILTES[\"none_of_these_words\"].split())}' if FILTES[\"none_of_these_words\"] != \"\" else \"\"\n",
    "FILTES[\"these_hashtags\"] = f'({\" OR \".join(f\"{i}\" for i in FILTES[\"these_hashtags\"].split())})' if FILTES[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"from_accounts\"] = f'({\" OR \".join(f\"from:{i}\" for i in FILTES[\"from_accounts\"].split())})' if FILTES[\"from_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"to_accounts\"] = f'({\" OR \".join(f\"to:{i}\" for i in FILTES[\"to_accounts\"].split())})' if FILTES[\"to_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in FILTES[\"mentioning_accounts\"].split())})' if FILTES[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"language\"] = f'lang:{lang_codes[FILTES[\"language\"]]}' if FILTES[\"language\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"replies\"] = \"\" if FILTES[\"replies\"] else \"-filter:replies\" \n",
    "FILTES[\"links\"] = \"\" if FILTES[\"links\"] else \"-filter:links\"\n",
    "\n",
    "FILTES[\"Minimum_replies\"] = f'min_replies:{FILTES[\"Minimum_replies\"]}' if FILTES[\"Minimum_replies\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_likes\"] = f'min_faves:{FILTES[\"Minimum_likes\"]}' if FILTES[\"Minimum_likes\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_retweets\"] = f'min_retweets:{FILTES[\"Minimum_retweets\"]}' if FILTES[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "\n",
    "\n",
    "FILTERS_COMBINATION = \"\"\n",
    "for _, value in FILTES.items():\n",
    "    if value != \"\":\n",
    "        FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "SEARCH_URL_WITH_FILTERS = f\"{SEARCH_URL}{FILTERS_COMBINATION}&f=live&src=typed_query\"\n",
    "SEARCH_URL_WITH_FILTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "            \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts found!\n",
      "All posts have been scraped!\n"
     ]
    }
   ],
   "source": [
    "# CASE 1, no startdate and enddate given.\n",
    "\n",
    "timeout = False\n",
    "reachedallPost = False \n",
    "continueifTimeout = True\n",
    "counter = 0\n",
    "startDate = datetime.now().strftime(\"%Y-%m-%d\") # Get ur current date\n",
    "\n",
    "while True:\n",
    "    currentSearchDate_limit = startDate\n",
    "\n",
    "    if  reachedallPost:\n",
    "        print(\"All posts have been scraped!\")\n",
    "        pd.DataFrame(theDict).to_csv(f'testRhey.csv', index=False)\n",
    "        break\n",
    "\n",
    "    # Adjust Link\n",
    "    searchLink = f\"{SEARCH_URL}{FILTERS_COMBINATION}{quote(f\" until:{currentSearchDate_limit}\")}&f=live&src=typed_query\"\n",
    "    driver.get(searchLink)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # CHECKERS!\n",
    "    # This will check if scrapping is detected, if it is. It'll wait for 20 mins\n",
    "    if continueifTimeout:\n",
    "        try:\n",
    "            #   This is essential to check if the scrapping is detected\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            print(\"Saving!\")\n",
    "            pd.DataFrame(theDict).to_csv(f'Savepoints/{str(datetime.now())}.csv', index=False)\n",
    "            print(\"Scrapping detected, waiting for 15 mins\")\n",
    "            wait(900)\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            timeout = False\n",
    "        except:\n",
    "            pass\n",
    "        if timeout: break\n",
    "\n",
    "    # This will wait for the page to load, if nothing exists. Minus one day and repeat\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\")))\n",
    "    except:\n",
    "        startDate = minOneDay(startDate)\n",
    "        if counter == 2:\n",
    "            print(\"No more posts found!\")\n",
    "            reachedallPost = True\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # If everything's set, will scrape posts\n",
    "    while True:\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "        \n",
    "        for element in elements[:-1]:\n",
    "\n",
    "            # Get the actual fucking post and its fuckin text an emoji\n",
    "            try:\n",
    "                postElement = element.find_element(By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]')\n",
    "                parts = postElement.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "                postText = \"\"\n",
    "                for p in parts:\n",
    "                    if p.tag_name == \"img\":\n",
    "                        postText += p.get_attribute(\"alt\")\n",
    "                    elif p.tag_name == \"a\":\n",
    "                        postText += p.postText + \" \"\n",
    "                    else:\n",
    "                        postText += p.postText\n",
    "            except:\n",
    "                # If no text, then continue. Tf ima do with just attachments\n",
    "                continue\n",
    "\n",
    "            # Get the fucking quoted post if there is one\n",
    "            try:\n",
    "                quotedPostElement = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "                quotedPostText = ''.join([i.text for i in quotedPostElement.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')])\n",
    "            except:\n",
    "                quotedPostText = \"\"\n",
    "                pass\n",
    "\n",
    "            # Check if text exists, if not then continue.\n",
    "            postDate = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "            postUser = element.find_element(By.XPATH, './/a/div/span').text\n",
    "\n",
    "            # Check for duplicates, same text, date, and user. If yes, ignore that shit\n",
    "            if postText in theDict[\"post_text\"] and postDate in theDict[\"Date\"] and postUser in theDict[\"User\"]:\n",
    "                continue\n",
    "\n",
    "            # append\n",
    "            theDict[\"post_text\"].append(postText)\n",
    "            theDict[\"quotedPost_text\"].append(quotedPostText)\n",
    "            theDict[\"User\"].append(postUser)\n",
    "            theDict[\"Date\"].append(postDate)\n",
    "            \n",
    "            # post attrs\n",
    "            for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                theDict[\"Reply_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Repost_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Like_count\"].append(int(re.search(r'\\d+',group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"View_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))[0]) if re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\")) else 0)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # will break if the scrollbar is at the bottom\n",
    "        if new_height == last_height:\n",
    "            \n",
    "            startDate = '-'.join(theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "            # This will check if the untilDate is the same as endDate, if it is. It'll minus one day.\n",
    "            if currentSearchDate_limit == startDate:\n",
    "                startDate = minOneDay(startDate)\n",
    "            \n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
