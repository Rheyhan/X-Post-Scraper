{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Scrapping and crawling modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from user_agent import generate_user_agent\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "\n",
    "# New chromediver\n",
    "import undetected_chromedriver as uc\n",
    "from requests.utils import quote, unquote\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(str):\n",
    "    '''\n",
    "    This function is used to convert string of datetime in isoformat to datetime object\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        date_time_obj:\n",
    "            - Datetime object\n",
    "    '''\n",
    "    date_time_obj = datetime.fromisoformat(str)\n",
    "    date_time_obj =date_time_obj + timedelta(hours=7)\n",
    "    return date_time_obj\n",
    "\n",
    "def minOneDay(str):\n",
    "    '''\n",
    "    This function is used to subtract one day from the given date\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        str:\n",
    "            - String of datetime in isoformat after subtracting one day\n",
    "    '''\n",
    "    str = datetime.strptime(str, \"%Y-%m-%d\")\n",
    "    str = str - timedelta(days=1)\n",
    "    str = str.strftime(\"%Y-%m-%d\")\n",
    "    return str\n",
    "\n",
    "def wait(timeout: int = 10):\n",
    "    '''\n",
    "    just a glorified simple function to wait for a certain amount of time\n",
    "    '''\n",
    "    for i in tqdm(range(timeout), desc=\"Waiting\"):\n",
    "        time.sleep(1)\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "def safe_int_from_aria(aria_label: str) -> int:\n",
    "    match = re.search(r\"\\d+\", aria_label or \"\")\n",
    "    return int(match.group(0)) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite and add profile post scraper function thus completing this entire fucking project \n",
    "\n",
    "class twitterScrapper:\n",
    "    def __init__(self, credentials: str = \"Credentials/twitter.json\",\n",
    "                 ):\n",
    "        '''\n",
    "        This function is used to initialize the class and will also login to twitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        credentials : str\n",
    "            - Path to the twitter credentials json file\n",
    "            - The json file should be in the following format:\n",
    "            ```\n",
    "            {   \"username\" : \"your_username\",\n",
    "                \"password\" : \"your_password\",\n",
    "                \"email\"    : \"your_email\"}\n",
    "            ```\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(credentials):\n",
    "            raise FileNotFoundError(\"Credentials file not found!\")\n",
    "\n",
    "        with open(credentials, \"r\") as f:\n",
    "            credentials = json.load(f)\n",
    "            try:\n",
    "                username = credentials[\"username\"]\n",
    "                password = credentials[\"password\"]\n",
    "                email = credentials[\"email\"]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Credentials file is not in the correct format!\")\n",
    "        \n",
    "        # Check if any of those credentials is None\n",
    "        if username is None or password is None:\n",
    "            raise ValueError(\"Username or password can't be empty!\")\n",
    "        if email is None:\n",
    "            warnings.warn(\"Email is not provided, this shit might not work if suspicious login attempt is detected!\", UserWarning)\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "\n",
    "        # For storing all the data\n",
    "        self.theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "                         \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}\n",
    "        \n",
    "        self.login()\n",
    "    \n",
    "    def login(self):\n",
    "        '''\n",
    "        Tries to login to the account based from the given credentials\n",
    "\n",
    "        Will use `self.username` and `self.password` only. However, if X detects your login as suspicious, `self.email` will be used.\n",
    "        '''\n",
    "        # Initialize the driver and check bot detection\n",
    "        self.driver = uc.Chrome()\n",
    "        self.driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "        # Check bot detection, \n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "        time.sleep(4)\n",
    "        botResult = self.driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "        if botResult != \"Normal\":\n",
    "            warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "        # Get to X login page\n",
    "        self.driver.get(\"https://x.com/i/flow/login\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Login handling\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(self.username)\n",
    "        self.driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # if email is needed. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "        try:\n",
    "            if self.driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "                print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "                if self.email is None:\n",
    "                    raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "                self.driver.find_element(By.XPATH, \"//input\").send_keys(self.email)\n",
    "                self.driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Put password\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(self.password)\n",
    "        self.driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "        print(\"Login sucess!\")\n",
    "        wait(10)\n",
    "\n",
    "    def start(self, filters, startDate: str = \"\", endDate: str = \"\",\n",
    "              scraping_Params  =  {\"wait_short\": 5, \"wait_long\": 20,\n",
    "                                  \"detection_wait\": 900, \"max_empty_pages\": 2},\n",
    "                                  saveFormat: Literal[\"csv\", \"json\", \"both\"] = \"csv\", \n",
    "                                  autoSave: bool = False, autoSaveInterval: int = 15, continue_if_timeout: bool = True):\n",
    "        \n",
    "        '''\n",
    "        This function is used to start the scrapping process based on the given filters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filters : dict\n",
    "            - A dictionary containing the filters for scrapping.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"keywords\": \"your keywords\",\n",
    "                \"from\": \"username\",\n",
    "                \"to\": \"username\",\n",
    "                \"mentions\": \"username\",\n",
    "                \"since\": \"YYYY-MM-DD\",\n",
    "                \"until\": \"YYYY-MM-DD\",\n",
    "                \"min_replies\": int,\n",
    "                \"min_likes\": int,\n",
    "                \"min_reposts\": int,\n",
    "                \"min_views\": int,\n",
    "                \"has_links\": bool,\n",
    "                \"has_media\": bool,\n",
    "                \"has_photos\": bool,\n",
    "                \"has_videos\": bool,\n",
    "                \"has_gifs\": bool\n",
    "            }\n",
    "            ```\n",
    "        '''\n",
    "        self.SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "        # Adjust filters values\n",
    "        filters[\"this_exact_phrase\"] = f'\\\"{filters[\"this_exact_phrase\"]}\\\"' if filters[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "        filters[\"any_of_these_words\"] = f'({filters[\"any_of_these_words\"]})' if filters[\"any_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"none_of_these_words\"] = f'-{filters[\"none_of_these_words\"]}' if filters[\"none_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"these_hashtags\"] = f'({filters[\"these_hashtags\"]})' if filters[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"from_accounts\"] = f'(from:{filters[\"from_accounts\"]})' if filters[\"from_accounts\"] != \"\" else \"\"\n",
    "        filters[\"to_accounts\"] = f'(to:{filters[\"to_accounts\"]})' if filters[\"to_accounts\"] != \"\" else \"\"\n",
    "        filters[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in filters[\"mentioning_accounts\"].split())})' if filters[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"replies\"] = \"\" if filters[\"replies\"] else \"-filter:replies\" \n",
    "        filters[\"links\"] = \"\" if filters[\"links\"] else \"-filter:links\"\n",
    "\n",
    "        filters[\"Minimum_replies\"] = f'min_replies:{filters[\"Minimum_replies\"]}' if filters[\"Minimum_replies\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_likes\"] = f'min_faves:{filters[\"Minimum_likes\"]}' if filters[\"Minimum_likes\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_retweets\"] = f'min_retweets:{filters[\"Minimum_retweets\"]}' if filters[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "        FILTERS_COMBINATION = \"\"\n",
    "        for _, value in filters.items():\n",
    "            if value != \"\":\n",
    "                FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "        self.FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "\n",
    "        # Dates handling\n",
    "        if startDate == \"\":\n",
    "            startDate = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if endDate == \"\":\n",
    "            endDate = \"2006-01-01\"  # Twitter launch date\n",
    "        self.start_date = startDate\n",
    "        self.end_date = endDate\n",
    "\n",
    "        # scraping params\n",
    "        self.WAIT_SHORT = scraping_Params[\"wait_short\"]\n",
    "        self.WAIT_LONG = scraping_Params[\"wait_long\"]\n",
    "        self.DETECTION_WAIT = scraping_Params[\"detection_wait\"]\n",
    "        self.MAX_EMPTY_PAGES = scraping_Params[\"max_empty_pages\"]\n",
    "\n",
    "        # Other params\n",
    "        self.saveFormat = saveFormat\n",
    "        self.autoSave = autoSave\n",
    "        self.autoSaveInterval = autoSaveInterval\n",
    "        self.continue_if_timeout = continue_if_timeout\n",
    "        \n",
    "        self.scrape()\n",
    "\n",
    "    def build_search_url(self, date_limit: str) -> str:\n",
    "        '''\n",
    "        This function is used to build the search URL based on the given date limit.\n",
    "        '''\n",
    "        return f\"{self.SEARCH_URL}{self.FILTERS_COMBINATION}{quote(f' until:{date_limit}')}&f=live&src=typed_query\"\n",
    "\n",
    "\n",
    "    def scrape_detected(self) -> bool:\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.WAIT_SHORT).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')\n",
    "                )\n",
    "            )\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            return False\n",
    "\n",
    "    def wait_for_posts_or_stepback(self, current_date: str, counter: int) -> tuple[str, int, bool]:\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\"))\n",
    "            )\n",
    "            return current_date, counter, False\n",
    "        except TimeoutException:\n",
    "            current_date = minOneDay(current_date)\n",
    "            counter += 1\n",
    "            return current_date, counter, counter > self.MAX_EMPTY_PAGES\n",
    "\n",
    "    def parse_post(self, post_element) -> str:\n",
    "        parts = post_element.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "        text = \"\"\n",
    "        for p in parts:\n",
    "            if p.tag_name == \"img\":\n",
    "                text += p.get_attribute(\"alt\")\n",
    "            elif p.tag_name == \"a\":\n",
    "                text += p.text + \" \"\n",
    "            else:\n",
    "                text += p.text\n",
    "        return text\n",
    "\n",
    "    def extract_post_data(self, element) -> tuple[str, str, str, str]:\n",
    "        post_element = element.find_element(\n",
    "            By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]',\n",
    "        )\n",
    "        post_text = self.parse_post(post_element)\n",
    "        try:\n",
    "            quoted_element = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "            quoted_text = ''.join(\n",
    "                i.text for i in quoted_element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            quoted_text = \"\"\n",
    "\n",
    "        post_date = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        post_user = element.find_element(By.XPATH, './/a/div/span').text\n",
    "        return post_text, quoted_text, post_user, post_date\n",
    "\n",
    "\n",
    "    def save(self, type):\n",
    "        if type == \"savepoint\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def scrape(self):\n",
    "        '''\n",
    "        This function is used to scrape the posts based on the given filters and date range.\n",
    "        '''\n",
    "        reached_all_posts = False\n",
    "        counter = 0\n",
    "\n",
    "        seen = set()\n",
    "\n",
    "        while True:\n",
    "            current_date_limit = self.start_date\n",
    "\n",
    "            if reached_all_posts:\n",
    "                print(\"All posts have been scraped!\")\n",
    "                pd.DataFrame(self.theDict).to_csv(\"testRhey.csv\", index=False)\n",
    "                break\n",
    "\n",
    "            self.driver.get(self.build_search_url(current_date_limit))\n",
    "            time.sleep(3)\n",
    "\n",
    "            if continue_if_timeout:\n",
    "                if self.scrape_detected():\n",
    "                    print(\"Saving!\")\n",
    "                    pd.DataFrame(self.theDict).to_csv(f\"Savepoints/{str(datetime.now())}.csv\", index=False)\n",
    "                    print(f\"Scraping detected, waiting for {self.DETECTION_WAIT} seconds\")\n",
    "                    wait(self.DETECTION_WAIT)\n",
    "                    continue\n",
    "            else:\n",
    "                if self.scrape_detected():\n",
    "                    break\n",
    "\n",
    "            start_date, counter, reached_all_posts = self.wait_for_posts_or_stepback(start_date, counter)\n",
    "            if reached_all_posts:\n",
    "                print(\"No more posts found!\")\n",
    "                continue\n",
    "\n",
    "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                elements = self.driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "\n",
    "                for element in elements[:-1]:\n",
    "                    try:\n",
    "                        post_text, quoted_text, post_user, post_date = self.extract_post_data(element)\n",
    "                    except (NoSuchElementException, StaleElementReferenceException):\n",
    "                        continue\n",
    "\n",
    "                    key = (post_text, post_date, post_user)\n",
    "                    if key in seen:\n",
    "                        continue\n",
    "\n",
    "                    seen.add(key)\n",
    "                    self.theDict[\"post_text\"].append(post_text)\n",
    "                    self.theDict[\"quotedPost_text\"].append(quoted_text)\n",
    "                    self.theDict[\"User\"].append(post_user)\n",
    "                    self.theDict[\"Date\"].append(post_date)\n",
    "\n",
    "                    for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                        self.theDict[\"Reply_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "                        self.theDict[\"Repost_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "                        self.theDict[\"Like_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "                        self.theDict[\"View_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(5)\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                if self.theDict[\"Date\"] and getTime(self.theDict[\"Date\"][-1]) < getTime(self.end_date):\n",
    "                    reached_all_posts = True\n",
    "                    break\n",
    "\n",
    "                if new_height == last_height:\n",
    "                    if self.theDict[\"Date\"]:\n",
    "                        start_date = \"-\".join(self.theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                        if current_date_limit == start_date:\n",
    "                            start_date = minOneDay(start_date)\n",
    "                    else:\n",
    "                        start_date = minOneDay(start_date)\n",
    "                    break\n",
    "\n",
    "                last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "FILTES = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",           # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"rei1_f\",   # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = twitterScrapper(\"Credentials/twitter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.searchAndscrap('Pemakzulanjokowi', \"2010-10-01\", \"2024-09-29\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual, deprecrated in favour of the syntax above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1st fucking option to handle the change\n",
    "\n",
    "# import zendriver as zd\n",
    "\n",
    "\n",
    "# browser =  await zd.start()\n",
    "# await browser.get(\"https://www.browserscan.net/bot-detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rhey\\anaconda3\\envs\\scraper\\Lib\\re\\_compiler.py:309: RuntimeWarning: coroutine 'start' was never awaited\n",
      "  break\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# # 2nd method, which works and also compactivle with selenium function and syntax\n",
    "\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# driver = uc.Chrome()\n",
    "\n",
    "# driver.get('https://www.browserscan.net/bot-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Credentials/twitter.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "    username = credentials[\"username\"]\n",
    "    password = credentials[\"password\"]\n",
    "    email = credentials[\"email\"]\n",
    "\n",
    "# Initialize the driver and check bot detection\n",
    "driver = uc.Chrome()\n",
    "driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "# Check bot detection, \n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "time.sleep(4)\n",
    "botResult = driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "if botResult != \"Normal\":\n",
    "    warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "# Get to X login page\n",
    "driver.get(\"https://x.com/i/flow/login\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Login handling\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(username)\n",
    "driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "time.sleep(5)               # This will wait for the next login pop-up to appear\n",
    "\n",
    "# if email is neeeded. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "try:\n",
    "    if driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "        print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "        if email is None:\n",
    "            raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "        driver.find_element(By.XPATH, \"//input\").send_keys(email)\n",
    "        driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Put password\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(password)\n",
    "driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "print(\"Login sucess!\")\n",
    "wait(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://x.com/search?q=%28from%3Arei1_f%29&f=live&src=typed_query'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "FILTES = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",           # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"rei1_f\",   # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}\n",
    "\n",
    "# Adjust filters values\n",
    "FILTES[\"this_exact_phrase\"] = f'\\\"{FILTES[\"this_exact_phrase\"]}\\\"' if FILTES[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "FILTES[\"any_of_these_words\"] = f'({FILTES[\"any_of_these_words\"]})' if FILTES[\"any_of_these_words\"] != \"\" else \"\"\n",
    "FILTES[\"none_of_these_words\"] = f'-{FILTES[\"none_of_these_words\"]}' if FILTES[\"none_of_these_words\"] != \"\" else \"\"\n",
    "FILTES[\"these_hashtags\"] = f'({FILTES[\"these_hashtags\"]})' if FILTES[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"from_accounts\"] = f'(from:{FILTES[\"from_accounts\"]})' if FILTES[\"from_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"to_accounts\"] = f'(to:{FILTES[\"to_accounts\"]})' if FILTES[\"to_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in FILTES[\"mentioning_accounts\"].split())})' if FILTES[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"replies\"] = \"\" if FILTES[\"replies\"] else \"-filter:replies\" \n",
    "FILTES[\"links\"] = \"\" if FILTES[\"links\"] else \"-filter:links\"\n",
    "\n",
    "FILTES[\"Minimum_replies\"] = f'min_replies:{FILTES[\"Minimum_replies\"]}' if FILTES[\"Minimum_replies\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_likes\"] = f'min_faves:{FILTES[\"Minimum_likes\"]}' if FILTES[\"Minimum_likes\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_retweets\"] = f'min_retweets:{FILTES[\"Minimum_retweets\"]}' if FILTES[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "\n",
    "\n",
    "FILTERS_COMBINATION = \"\"\n",
    "for _, value in FILTES.items():\n",
    "    if value != \"\":\n",
    "        FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "SEARCH_URL_WITH_FILTERS = f\"{SEARCH_URL}{FILTERS_COMBINATION}&f=live&src=typed_query\"\n",
    "SEARCH_URL_WITH_FILTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "            \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts found!\n",
      "All posts have been scraped!\n"
     ]
    }
   ],
   "source": [
    "# CASE 1, no startdate and enddate given.\n",
    "\n",
    "timeout = False\n",
    "reachedallPost = False \n",
    "continueifTimeout = True\n",
    "counter = 0\n",
    "startDate = datetime.now().strftime(\"%Y-%m-%d\") # Get ur current date\n",
    "\n",
    "while True:\n",
    "    currentSearchDate_limit = startDate\n",
    "\n",
    "    if  reachedallPost:\n",
    "        print(\"All posts have been scraped!\")\n",
    "        pd.DataFrame(theDict).to_csv(f'testRhey.csv', index=False)\n",
    "        break\n",
    "\n",
    "    # Adjust Link\n",
    "    searchLink = f\"{SEARCH_URL}{FILTERS_COMBINATION}{quote(f\" until:{currentSearchDate_limit}\")}&f=live&src=typed_query\"\n",
    "    driver.get(searchLink)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # CHECKERS!\n",
    "    # This will check if scrapping is detected, if it is. It'll wait for 20 mins\n",
    "    if continueifTimeout:\n",
    "        try:\n",
    "            #   This is essential to check if the scrapping is detected\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            print(\"Saving!\")\n",
    "            pd.DataFrame(theDict).to_csv(f'Savepoints/{str(datetime.now())}.csv', index=False)\n",
    "            print(\"Scrapping detected, waiting for 15 mins\")\n",
    "            wait(900)\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            timeout = False\n",
    "        except:\n",
    "            pass\n",
    "        if timeout: break\n",
    "\n",
    "    # This will wait for the page to load, if nothing exists. Minus one day and repeat\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\")))\n",
    "    except:\n",
    "        startDate = minOneDay(startDate)\n",
    "        if counter == 2:\n",
    "            print(\"No more posts found!\")\n",
    "            reachedallPost = True\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # If everything's set, will scrape posts\n",
    "    while True:\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "        \n",
    "        for element in elements[:-1]:\n",
    "\n",
    "            # Get the actual fucking post and its fuckin text an emoji\n",
    "            try:\n",
    "                postElement = element.find_element(By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]')\n",
    "                parts = postElement.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "                postText = \"\"\n",
    "                for p in parts:\n",
    "                    if p.tag_name == \"img\":\n",
    "                        postText += p.get_attribute(\"alt\")\n",
    "                    elif p.tag_name == \"a\":\n",
    "                        postText += p.postText + \" \"\n",
    "                    else:\n",
    "                        postText += p.postText\n",
    "            except:\n",
    "                # If no text, then continue. Tf ima do with just attachments\n",
    "                continue\n",
    "\n",
    "            # Get the fucking quoted post if there is one\n",
    "            try:\n",
    "                quotedPostElement = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "                quotedPostText = ''.join([i.text for i in quotedPostElement.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')])\n",
    "            except:\n",
    "                quotedPostText = \"\"\n",
    "                pass\n",
    "\n",
    "            # Check if text exists, if not then continue.\n",
    "            postDate = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "            postUser = element.find_element(By.XPATH, './/a/div/span').text\n",
    "\n",
    "            # Check for duplicates, same text, date, and user. If yes, ignore that shit\n",
    "            if postText in theDict[\"post_text\"] and postDate in theDict[\"Date\"] and postUser in theDict[\"User\"]:\n",
    "                continue\n",
    "\n",
    "            # append\n",
    "            theDict[\"post_text\"].append(postText)\n",
    "            theDict[\"quotedPost_text\"].append(quotedPostText)\n",
    "            theDict[\"User\"].append(postUser)\n",
    "            theDict[\"Date\"].append(postDate)\n",
    "            \n",
    "            # post attrs\n",
    "            for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                theDict[\"Reply_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Repost_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Like_count\"].append(int(re.search(r'\\d+',group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"View_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))[0]) if re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\")) else 0)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # will break if the scrollbar is at the bottom\n",
    "        if new_height == last_height:\n",
    "            \n",
    "            startDate = '-'.join(theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "            # This will check if the untilDate is the same as endDate, if it is. It'll minus one day.\n",
    "            if currentSearchDate_limit == startDate:\n",
    "                startDate = minOneDay(startDate)\n",
    "            \n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts found!\n",
      "All posts have been scraped!\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "WAIT_SHORT = 5\n",
    "WAIT_LONG = 20\n",
    "DETECTION_WAIT = 900\n",
    "MAX_EMPTY_PAGES = 2\n",
    "\n",
    "def build_search_url(date_limit: str) -> str:\n",
    "    return f\"{SEARCH_URL}{FILTERS_COMBINATION}{quote(f' until:{date_limit}')}&f=live&src=typed_query\"\n",
    "\n",
    "def scrape_detected(driver) -> bool:\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SHORT).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')\n",
    "            )\n",
    "        )\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "def wait_for_posts_or_stepback(driver, current_date: str, counter: int) -> tuple[str, int, bool]:\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_LONG).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\"))\n",
    "        )\n",
    "        return current_date, counter, False\n",
    "    except TimeoutException:\n",
    "        current_date = minOneDay(current_date)\n",
    "        counter += 1\n",
    "        return current_date, counter, counter >= MAX_EMPTY_PAGES\n",
    "\n",
    "def parse_text_with_emojis(post_element) -> str:\n",
    "    parts = post_element.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "    text = \"\"\n",
    "    for p in parts:\n",
    "        if p.tag_name == \"img\":\n",
    "            text += p.get_attribute(\"alt\")\n",
    "        elif p.tag_name == \"a\":\n",
    "            text += p.text + \" \"\n",
    "        else:\n",
    "            text += p.text\n",
    "    return text\n",
    "\n",
    "def safe_int_from_aria(aria_label: str) -> int:\n",
    "    match = re.search(r\"\\d+\", aria_label or \"\")\n",
    "    return int(match.group(0)) if match else 0\n",
    "\n",
    "def extract_post_data(element) -> tuple[str, str, str, str]:\n",
    "    post_element = element.find_element(\n",
    "        By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]',\n",
    "    )\n",
    "    post_text = parse_text_with_emojis(post_element)\n",
    "\n",
    "    try:\n",
    "        quoted_element = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "        quoted_text = ''.join(\n",
    "            i.text for i in quoted_element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')\n",
    "        )\n",
    "    except NoSuchElementException:\n",
    "        quoted_text = \"\"\n",
    "\n",
    "    post_date = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "    post_user = element.find_element(By.XPATH, './/a/div/span').text\n",
    "    return post_text, quoted_text, post_user, post_date\n",
    "\n",
    "timeout = False\n",
    "reached_all_posts = False\n",
    "continue_if_timeout = True\n",
    "counter = 0\n",
    "start_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "end_date = \"2010-01-01\"\n",
    "\n",
    "seen = set()\n",
    "\n",
    "while True:\n",
    "    current_date_limit = start_date\n",
    "\n",
    "    if reached_all_posts:\n",
    "        print(\"All posts have been scraped!\")\n",
    "        pd.DataFrame(theDict).to_csv(\"testRhey.csv\", index=False)\n",
    "        break\n",
    "\n",
    "    driver.get(build_search_url(current_date_limit))\n",
    "    time.sleep(3)\n",
    "\n",
    "    if continue_if_timeout:\n",
    "        if scrape_detected(driver):\n",
    "            print(\"Saving!\")\n",
    "            pd.DataFrame(theDict).to_csv(f\"Savepoints/{str(datetime.now())}.csv\", index=False)\n",
    "            print(\"Scraping detected, waiting for 15 mins\")\n",
    "            wait(DETECTION_WAIT)\n",
    "            continue\n",
    "    else:\n",
    "        if scrape_detected(driver):\n",
    "            timeout = True\n",
    "        if timeout:\n",
    "            break\n",
    "\n",
    "    start_date, counter, reached_all_posts = wait_for_posts_or_stepback(driver, start_date, counter)\n",
    "    if reached_all_posts:\n",
    "        print(\"No more posts found!\")\n",
    "        continue\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        elements = driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "\n",
    "        for element in elements[:-1]:\n",
    "            try:\n",
    "                post_text, quoted_text, post_user, post_date = extract_post_data(element)\n",
    "            except (NoSuchElementException, StaleElementReferenceException):\n",
    "                continue\n",
    "\n",
    "            key = (post_text, post_date, post_user)\n",
    "            if key in seen:\n",
    "                continue\n",
    "\n",
    "            seen.add(key)\n",
    "            theDict[\"post_text\"].append(post_text)\n",
    "            theDict[\"quotedPost_text\"].append(quoted_text)\n",
    "            theDict[\"User\"].append(post_user)\n",
    "            theDict[\"Date\"].append(post_date)\n",
    "\n",
    "            for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                theDict[\"Reply_count\"].append(\n",
    "                    safe_int_from_aria(group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))\n",
    "                )\n",
    "                theDict[\"Repost_count\"].append(\n",
    "                    safe_int_from_aria(group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))\n",
    "                )\n",
    "                theDict[\"Like_count\"].append(\n",
    "                    safe_int_from_aria(group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))\n",
    "                )\n",
    "                theDict[\"View_count\"].append(\n",
    "                    safe_int_from_aria(group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))\n",
    "                )\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if theDict[\"Date\"] and getTime(theDict[\"Date\"][-1]) < getTime(end_date):\n",
    "            reached_all_posts = True\n",
    "            break\n",
    "\n",
    "        if new_height == last_height:\n",
    "            if theDict[\"Date\"]:\n",
    "                start_date = \"-\".join(theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                if current_date_limit == start_date:\n",
    "                    start_date = minOneDay(start_date)\n",
    "            else:\n",
    "                start_date = minOneDay(start_date)\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User': ['@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f',\n",
       "  '@rei1_f'],\n",
       " 'Date': ['2026-01-15-17:50:27',\n",
       "  '2025-12-21-18:34:40',\n",
       "  '2025-11-19-17:04:02',\n",
       "  '2025-11-16-20:53:40',\n",
       "  '2025-11-09-09:00:13',\n",
       "  '2025-11-05-12:51:38',\n",
       "  '2025-11-02-18:58:46',\n",
       "  '2025-11-02-15:06:15',\n",
       "  '2025-11-01-18:50:26',\n",
       "  '2025-10-31-20:03:54',\n",
       "  '2025-10-30-18:37:04',\n",
       "  '2025-10-23-16:25:04',\n",
       "  '2025-10-15-18:02:53',\n",
       "  '2025-09-18-15:28:57',\n",
       "  '2025-08-30-16:47:21',\n",
       "  '2025-08-28-23:10:49',\n",
       "  '2025-08-28-22:57:51',\n",
       "  '2025-08-25-06:39:41',\n",
       "  '2025-08-22-20:42:17',\n",
       "  '2025-08-22-20:40:56',\n",
       "  '2025-08-22-11:39:43',\n",
       "  '2025-08-17-16:29:42',\n",
       "  '2025-08-17-04:17:09',\n",
       "  '2025-08-15-21:52:33',\n",
       "  '2025-08-15-03:08:22',\n",
       "  '2025-08-14-21:00:49',\n",
       "  '2025-08-14-04:18:48',\n",
       "  '2025-08-14-03:58:01',\n",
       "  '2025-08-13-17:30:46',\n",
       "  '2025-08-13-05:39:38',\n",
       "  '2025-08-12-23:23:51',\n",
       "  '2025-08-12-23:21:20',\n",
       "  '2025-08-12-21:03:05',\n",
       "  '2025-08-12-20:36:50',\n",
       "  '2025-08-12-17:16:02',\n",
       "  '2025-08-12-04:37:38',\n",
       "  '2025-08-11-16:01:31',\n",
       "  '2025-08-11-16:00:02',\n",
       "  '2025-08-11-14:39:48',\n",
       "  '2025-08-11-14:19:51',\n",
       "  '2025-08-11-14:16:14',\n",
       "  '2025-08-11-06:45:46',\n",
       "  '2025-08-11-06:27:51',\n",
       "  '2025-08-11-06:26:30',\n",
       "  '2025-08-11-06:13:02',\n",
       "  '2025-08-11-06:09:24',\n",
       "  '2025-08-11-06:06:28',\n",
       "  '2025-08-11-06:04:01',\n",
       "  '2025-08-11-06:02:15',\n",
       "  '2025-08-11-06:00:43',\n",
       "  '2025-08-11-05:58:30',\n",
       "  '2025-08-11-02:18:12',\n",
       "  '2025-08-11-02:17:48',\n",
       "  '2025-08-07-18:22:09',\n",
       "  '2025-08-01-15:57:21',\n",
       "  '2025-07-27-21:45:49',\n",
       "  '2025-07-27-18:36:01',\n",
       "  '2025-07-27-16:38:52',\n",
       "  '2025-07-27-16:37:29',\n",
       "  '2025-07-26-16:53:15',\n",
       "  '2025-07-25-20:56:37',\n",
       "  '2025-07-25-20:55:30',\n",
       "  '2025-07-24-18:13:33',\n",
       "  '2025-07-24-13:33:43',\n",
       "  '2025-07-24-12:17:34',\n",
       "  '2025-07-24-12:16:41',\n",
       "  '2025-07-23-19:56:26',\n",
       "  '2025-07-23-07:06:08',\n",
       "  '2025-07-23-03:03:20',\n",
       "  '2025-07-16-22:29:21',\n",
       "  '2025-07-16-02:15:09',\n",
       "  '2025-07-15-16:10:33',\n",
       "  '2025-07-15-01:18:15',\n",
       "  '2025-07-14-21:59:15',\n",
       "  '2025-07-10-15:39:07',\n",
       "  '2025-07-02-17:11:20',\n",
       "  '2025-06-28-19:03:15',\n",
       "  '2025-06-24-01:26:48',\n",
       "  '2025-06-24-01:07:27',\n",
       "  '2025-06-23-04:20:52',\n",
       "  '2025-06-23-01:16:06',\n",
       "  '2025-06-22-23:21:20',\n",
       "  '2025-06-22-19:40:07',\n",
       "  '2025-06-21-22:18:33',\n",
       "  '2025-06-21-16:27:41',\n",
       "  '2025-06-21-03:01:24',\n",
       "  '2025-06-20-17:16:39',\n",
       "  '2025-06-19-14:01:34',\n",
       "  '2025-06-15-16:34:41',\n",
       "  '2025-06-12-03:33:01',\n",
       "  '2025-06-10-17:40:55',\n",
       "  '2025-06-09-21:38:51',\n",
       "  '2025-06-09-16:07:33',\n",
       "  '2025-06-09-15:58:09',\n",
       "  '2025-06-09-14:58:12',\n",
       "  '2025-06-09-01:30:18',\n",
       "  '2025-06-08-18:37:07',\n",
       "  '2025-06-08-17:59:40',\n",
       "  '2025-06-06-12:40:53',\n",
       "  '2025-06-06-01:03:33',\n",
       "  '2025-06-02-21:08:28',\n",
       "  '2025-05-29-20:42:45',\n",
       "  '2025-05-28-00:30:39',\n",
       "  '2025-05-16-15:17:37',\n",
       "  '2025-05-16-11:10:17',\n",
       "  '2025-05-15-21:53:08',\n",
       "  '2025-05-14-22:39:01',\n",
       "  '2025-05-14-16:39:52',\n",
       "  '2025-05-12-16:19:40',\n",
       "  '2025-05-12-15:40:39',\n",
       "  '2025-05-12-11:38:16',\n",
       "  '2025-05-11-13:09:23',\n",
       "  '2025-05-10-23:53:19',\n",
       "  '2025-05-10-22:14:48',\n",
       "  '2025-05-10-22:11:54',\n",
       "  '2025-05-09-19:17:47',\n",
       "  '2025-05-09-12:08:48',\n",
       "  '2025-05-07-20:58:58',\n",
       "  '2025-05-06-22:51:34',\n",
       "  '2025-05-05-22:51:57',\n",
       "  '2025-05-05-15:09:53',\n",
       "  '2025-05-05-15:07:46',\n",
       "  '2025-05-01-23:42:56',\n",
       "  '2025-05-01-16:49:19',\n",
       "  '2025-04-30-07:14:43',\n",
       "  '2025-04-29-17:05:25',\n",
       "  '2025-04-26-19:42:43',\n",
       "  '2025-04-26-19:40:03',\n",
       "  '2025-04-26-18:44:47',\n",
       "  '2025-04-25-11:42:31',\n",
       "  '2025-04-23-00:34:35',\n",
       "  '2025-04-23-00:26:45',\n",
       "  '2025-04-22-20:24:17',\n",
       "  '2025-04-20-18:05:41',\n",
       "  '2025-04-18-20:24:51',\n",
       "  '2025-04-18-11:30:09',\n",
       "  '2025-04-17-22:31:59',\n",
       "  '2025-04-11-22:41:42',\n",
       "  '2025-04-11-07:33:10',\n",
       "  '2025-04-09-22:51:34',\n",
       "  '2025-04-09-17:01:15',\n",
       "  '2025-04-09-15:16:06',\n",
       "  '2025-04-09-11:29:46',\n",
       "  '2025-04-07-23:56:33',\n",
       "  '2025-04-07-17:43:15',\n",
       "  '2025-04-06-11:23:34',\n",
       "  '2025-04-06-06:31:41',\n",
       "  '2025-04-06-06:02:44',\n",
       "  '2025-04-06-04:34:51',\n",
       "  '2025-04-05-19:38:31',\n",
       "  '2025-04-05-15:48:34',\n",
       "  '2025-04-05-15:47:24',\n",
       "  '2025-04-03-13:30:26',\n",
       "  '2025-04-02-17:42:56',\n",
       "  '2025-03-30-20:26:43',\n",
       "  '2025-03-29-17:24:50',\n",
       "  '2025-03-29-09:33:27',\n",
       "  '2025-03-28-08:00:07',\n",
       "  '2025-03-28-07:58:23',\n",
       "  '2025-03-28-07:56:48',\n",
       "  '2025-03-28-07:55:26',\n",
       "  '2025-03-28-07:52:15',\n",
       "  '2025-03-28-07:50:50',\n",
       "  '2025-03-28-07:46:44',\n",
       "  '2025-03-27-13:08:43',\n",
       "  '2025-03-25-19:01:55',\n",
       "  '2025-03-23-23:01:35',\n",
       "  '2025-03-21-11:40:08',\n",
       "  '2025-03-21-10:54:58',\n",
       "  '2025-03-21-07:28:16',\n",
       "  '2025-03-21-07:27:05',\n",
       "  '2025-03-21-07:18:42',\n",
       "  '2025-03-17-13:05:55',\n",
       "  '2025-03-14-17:17:20',\n",
       "  '2025-03-14-14:59:24',\n",
       "  '2025-03-09-20:15:36',\n",
       "  '2025-03-09-20:15:09',\n",
       "  '2025-03-09-15:22:00',\n",
       "  '2025-03-09-15:21:32',\n",
       "  '2025-03-08-22:33:28',\n",
       "  '2025-03-08-22:29:21',\n",
       "  '2025-03-08-03:53:13',\n",
       "  '2025-03-08-03:41:20',\n",
       "  '2025-02-04-23:24:36',\n",
       "  '2025-01-18-17:28:16',\n",
       "  '2025-01-17-13:16:50',\n",
       "  '2025-01-16-13:44:43',\n",
       "  '2025-01-11-22:11:58',\n",
       "  '2025-01-05-14:08:07',\n",
       "  '2025-01-05-02:38:40',\n",
       "  '2025-01-04-15:22:50',\n",
       "  '2025-01-04-13:45:30',\n",
       "  '2025-01-03-14:31:25',\n",
       "  '2024-12-29-18:37:22',\n",
       "  '2024-12-28-21:36:40',\n",
       "  '2024-12-23-17:19:51',\n",
       "  '2024-12-22-15:11:16',\n",
       "  '2024-12-22-11:16:47',\n",
       "  '2024-12-16-18:46:53',\n",
       "  '2024-12-08-17:12:31',\n",
       "  '2024-12-06-08:52:14',\n",
       "  '2024-11-30-18:52:26',\n",
       "  '2024-11-29-16:38:54',\n",
       "  '2024-11-20-06:42:27',\n",
       "  '2024-11-17-19:43:06',\n",
       "  '2024-10-21-14:12:39',\n",
       "  '2024-10-20-19:13:57',\n",
       "  '2024-05-09-01:08:35',\n",
       "  '2023-07-20-21:00:10',\n",
       "  '2023-07-12-23:40:47'],\n",
       " 'post_text': ['@RafliRadithya@RafliRadithya , found a retard',\n",
       "  'Tetap semangat 🔥',\n",
       "  'Islamic golden age',\n",
       "  'Huh?',\n",
       "  \"Haii, what's his power?\",\n",
       "  'IDENTIFIED',\n",
       "  'Battle of Yarmouk',\n",
       "  'Cheating on roblox',\n",
       "  'Close enough, welcome back qarmatians.',\n",
       "  't.co/InNR6qqP9e ',\n",
       "  'SAS',\n",
       "  \"This u gng? \\n\\nHave you not realized, the course of actions made by our government were compliant to 'recommendations' you stated a while ago?\\nTldr, many of Israeli athletes are ex IDF soldiers. Israel has also been provocative to disrupt peace amongst IOC countries in the region.\",\n",
       "  '@ezdubs_bot@ezdubs_bot  indonesian english',\n",
       "  'why create a scraper when you can unscrape smth?',\n",
       "  'Bjir',\n",
       "  'Ini ae gw baru nyadar sampe separah ini cok',\n",
       "  'Kacung pemerintah.',\n",
       "  'Gray',\n",
       "  'Karena life is roblox',\n",
       "  'Ah, kalau flexing gitu dengan statusnya udah secara resmi menikah sih lazim bang terutama kalau lu ngomongnya ke orang yang udah nikah atau tua 😭Sorry gagal paham tadi',\n",
       "  'Lingkup circle di masa SMA gw heterogen banget bang, sekalinya ngumpul pasti ada aja kejadian anak ayam baru keluar kandang. Sharing kenakalannya sebagai bentuk flexing dan gw cuma bisa denger sembari kecewa ke orang² itu',\n",
       "  'Bcz, art imitates life.',\n",
       "  'Bisa aja nder kalau misal tahun depan udah gaji 20 juta per bulan',\n",
       "  'Primebowo Sentinelianto',\n",
       "  'Billions must larp',\n",
       "  'BLACK SOULS defo.\\n\\nGeForce RTX PC Week',\n",
       "  'My Femboy Roommate: Special Weekend\\nGeForce PC Week',\n",
       "  'GeForce PC Week',\n",
       "  'Siapa tuh manusianya nder? \\nCongrats yaa btw 🎉',\n",
       "  'Makin nolep lu ntar fli ama skil sosial lu berkurang.\\n\\nPenggunaan jangka pendek memang bisa hilangi rasa kesendirian lu, tapi jangka panjang bisa hancuri persepsi lu dalam menangani atau menjalani relasi sama orang lain',\n",
       "  'Tapi memang enak njir curhat ke chatbot yang udah lu lakuin fine tuning dan kasih instruksi secara personal.\\n\\nSayangnya, perilaku gitu gak baik untuk kesehatan dalam jangka panjang (gw jarang dapet afeksi selain dari keluarga).',\n",
       "  'lebih karena kelatih ama jurnal dsbnya sih bang, LLM kan secara praktiknya untuk ngeprediksi kata selanjutnya. Kalau input lu tentang masalah psikologi tentu jawaban responnya bakal dateng dari buku psikologi. Sebenernya memang reliable, cuma gak akan menitikberatkan kesalahan lu',\n",
       "  'Mungkin dari perspektif dia, pakai chatbot untuk berbicara tentang masalah hidup memang enak bro, karena konformasi dan feedbacknya itu pasti didapat ke sender (gk serupa kalau lu ngomong ke orng lain).\\n\\nDari situ mungkin beliau jadi percaya² aja ama hasil perkalian matrix 😓',\n",
       "  'Satu pria berbahagia atas cinta (tp red flag), 19 juta pria lainnya bersedih menunggu lapangan pekerjaan',\n",
       "  'Serem kali tipu daya Shizuka ini🥀',\n",
       "  'Uhhhh?...',\n",
       "  'Kalau kartap sih enak bro, bisa jadi leverage untuk promosi terus dan dianggap orang penting',\n",
       "  'v1.samehadaku.how kusonime.com nimegami.id otakudesu.best otakupoi.org ',\n",
       "  'Web bajak laut gratis dan lengkap nder 😇',\n",
       "  'Sebagai manusia yang pernah merasakan dipoisisi sang mantan (dia skrng kabarnya gmn ya?). Gw tentu langsung kurang suka dan hilang respek ke beliau ini 😓',\n",
       "  'Kisah beliau yang merelakan kehormatan demi jadian ama orang, walaupun hasilnya ditolak pas sudah selesai. Lucunya beliau cerita ke orang² (kita termasuk) dan ke mantannya setelah itu.',\n",
       "  '🇮🇩 BREAKING NEWS: A former Indonesian president’s diploma has been discovered by an Indonesian patriot on the WPlace platform.',\n",
       "  'Setelah ngelihat beberapa kasus juga bisa kita maklumatkan bila yang bersalah itu beliau, bukan temen-temen kita.\\n\\nKasus yang masih gw inget teh tentang konfes beliau di Desember lalu, sampai saat ini gw udah hilang respect ama beliau.',\n",
       "  'Cui, realistis aja. Sudah berapa temen kita yang beliau ini sakiti? Beberapa temen beliau itu kasusnya temen kita juga.\\nDengan mengamati aja, gw jadi skip mau berteman ama beliau ini',\n",
       "  'Ghosting weh, atau ikuti cara si Abang pemain game kuda',\n",
       "  'terkait dengan approach beliau yang friendly gt, gw jadi agak kasihan sama orang yang terkena false affection.\\n\"tipu daya wanita sgt lah dahsyat\"\\nBeliau ini kan suka banget trauma dump ke orang random (terutama cowo) yang baru dia kenal + pasti bakal ngechat trus di minggu2 awal',\n",
       "  'Beliau itu hanya butuh pendukung yang ngefeed ego dan narsistiknya saja. Bukan sebuah teman yang berani mau ngoreksi dan kasih tau kecacatan beliau.\\n\\nKan lucu kalau lu lihat bagaimana yang approach dan interaksinya berubah itu temen lamanya beliau, bukan beliaunya sendiri?',\n",
       "  'Giliran lu, gw, dll (yang udah leave grup) kasih sugesti dan saran. Apa yang dia lakukan? ya, betul. hanya percuma saja. Belum ditambah dengan kekecewaan gw terhadap bagaimana dia ngeresolve isu-isu lain.',\n",
       "  'Lu pikir aja, mana ada orang percaya takdir itu digenggam oleh generative chatbot terkait dengan kartu tarrot. Trus overthink selama 2 minggu di grup terkait hal tsb, ngomong gitu-itu terus dan butuh perhatian sambil ngeklaim liat hal supernatural sehari-hari.',\n",
       "  'Titik letak gw jenuh dan gak mau lagi karena beliau ini susah sekali untuk disadarkan dan punya pikiran semacam anak SMA. Umur sudah kepala 2 tapi dikit-dikit ngeluh, nyalahin diri sendiri, nyalahin keadaan, suka pamer, denger sugesti hanya masuk telinga kiri keluar kanan',\n",
       "  'Yah bro, masalahnya beliau ini jarang ngerespect opini atau ngehargai boundary orang. Gw gak permasalahkan mau lawan jenis atau tidak, curhat dan sebagaimananya atau konversasi random. Lebih baik dengeri permasalahan orang daripada denger dia bundir atau gmn gt kan.',\n",
       "  'Untung gw udah cut connection ama beliau 🤣',\n",
       "  'Biasanya sih kalau gini mau pdkt ama lu fli',\n",
       "  'Can i just have my men dressed in VKBO EMR 💔?',\n",
       "  'Do mine ^^',\n",
       "  '\"%d/%m/%Y\" is better than \"%m/%d/%Y\". x.com/SchmoozeX/stat\\n… …',\n",
       "  \"Dw unc, it'll lead to even worse\",\n",
       "  'Tak kembali pulang karena disibukan dengan agenda menjadi dewan komisaris BUMN',\n",
       "  'DM ke si jojo bro, kalau gak salah dulu dia distributornya  🤣',\n",
       "  'Ini aja nder, biar diajak ngopi² sama rekan kantor setelah jampul. Siapa tau direkomendasiin jadi komisaris.',\n",
       "  'Cari bahasan yang lu dan dia suka dong bang, ente kan jago 😭',\n",
       "  'Gas sampe dikelola palantir 💔',\n",
       "  'Dibeli nder, tapi minusnya ntar langsung masuk neraka',\n",
       "  'Such a bs, my co workers usually start their day with these combinations.',\n",
       "  'Suicide',\n",
       "  'They call me 00007\\n0 alcohol\\n0 cigarette\\n0 weed\\n0 drugs\\n7 drowning in thoughts',\n",
       "  'Assembled a squad of individuals from various factions to fight off the Monolith. I was the sole survivor upon reaching the Chernobyl NPP.',\n",
       "  \">femcel\\n>looks inside\\n>She's e-dating 2-9 ppl\\n\\nReal talk. Sounds good on paper, but in reality if you end up with these people. it's somewhat taxing. Please normalize intellectual relationships where you can actually have good conversation from time to time.\",\n",
       "  'Deketin dan tanyain wes,tapi jaga jarak. Kalau lu mundur di awal dan taunya itu hanya foto bersama saudara kan hilang tuh kesempatan lu',\n",
       "  '🤠👍',\n",
       "  '\"I support zionism\"',\n",
       "  'Ini bro orangnya',\n",
       "  'Thank you John Roblox, very cool !',\n",
       "  'Ask not what your country can do for you; ask what you can do for Barovia. \\n\\nI stand with #Barovia#Barovia  #freethehostages#freethehostages  #BringThemHomeNow#BringThemHomeNow 🎗️',\n",
       "  'Wishing i was a protected class.',\n",
       "  'Kebetulan, saya juga tsel.',\n",
       "  '@HaloBCA@HaloBCA ',\n",
       "  \"[1] Iranian missiles launch toward US military bases in Iraq and Qatar\\n[2] Interception over Qatar's airspace\\n[3] Panic at a mall in Doha, Qatar. \\n\\nThings might've gonna go even more bleak in the upcoming days.\",\n",
       "  'Ini Jono!?',\n",
       "  'Although appear `democratic`, The US and Indonesia governments seem to love swaying away public narratives with botted accounts.',\n",
       "  'At your service, my queen.',\n",
       "  \"It's 6 million likes actually.\",\n",
       "  'P sharelock.',\n",
       "  'Yuk bisa yuk',\n",
       "  \"My stupid ass slacked way too much and forgot the beauty of the real world. There's no way I'm taking another semester.\",\n",
       "  'Who hurt you bro? 😞',\n",
       "  \"Ain't an american myself, we might be the opposite spectrum of the same table. But, it seems we truly don't have anything in common, you lack the capability that makes humans human, the sense of decency.\",\n",
       "  'An hour ago',\n",
       "  'x.com/i/grok/share/m\\n… …\"Grok was trained with neutral datasets\"',\n",
       "  '>',\n",
       "  'This u?',\n",
       "  'This u?',\n",
       "  '>',\n",
       "  '\"Hostages\"',\n",
       "  '>',\n",
       "  'Lebih seru side questnya kak',\n",
       "  '0%',\n",
       "  'This u bro?',\n",
       "  \"This was my hood, you wouldn't survive a day without being blasted by a sabot round.\",\n",
       "  'Estimasi selesai jam berapa ya? \\nSaya ada keperluan pakai komputer.',\n",
       "  '@grok@grok  put an Israel flag on the background',\n",
       "  '@grok@grok  put an israel flag behind the person and add Lebron James',\n",
       "  'Bismillah, proyek menghasilkan cuan.',\n",
       "  'Batak israel',\n",
       "  'Kalau bentuknya orang, mending disogok bang. Sapa tau dia mau sepakat ngerjain diri dia sendiri 😇🙏',\n",
       "  'Ngerjain skripsi tidak seseru mencari duit 😔☝',\n",
       "  'People could believe and acknowledge what you said if you and your entire government PR had not  repeatedly lied prior. You got short memory loss or smth?',\n",
       "  'Kalau gw HRny, mingdep langsung ttd kontrak. Gk usah ada user intv',\n",
       "  '\"Zero evidence Indian rafales were shot down\"',\n",
       "  'Pajeets and their obsessions with rape should be studied.',\n",
       "  'Gak usah dijawab bro',\n",
       "  \"Oy vey goys! \\n\\nIf we see through your lies, we'll call you liars\\n\\n*but*\\n\\nIf you see through our lies, we'll call you anti semitics.\",\n",
       "  'What about in indonesia. if it had happened, what  would it have meant for the average Indonesians?',\n",
       "  '@grok@grok  has there been a recorded case where a man got arrested by posting an altered image showing the current president of the US kissing the former president?',\n",
       "  \"Nigga, there's no way in hell you could you get arrested by posting a product of generative image model. I.e, demonstrating how badly it can be misused by showing a man kissing another man. \\n\\nTs tuff.\",\n",
       "  'Temen aing unpad stat udah wisuda euy 🥲',\n",
       "  \"Me when my super duper silly pookie bear reaches me to discuss nepotism and rampant corruption problems in the country (we ain't solving allat) :3\",\n",
       "  'Jakarta daerah mana itu? 😞',\n",
       "  'Comically large GPU (317 mm)',\n",
       "  'Bisa',\n",
       "  'Rill cok, begadang malem tuh pasti ada banyak momen mikirin hidup, uang, kesuksesan, asmara, dsb',\n",
       "  'Day 1 mengganggur di semester skripsi (rakit pc)',\n",
       "  'Emados >>> almaz',\n",
       "  \"Nigga, roblox ain't linkedin 😭\",\n",
       "  'Chunk chunk chunk sahur',\n",
       "  'Gajian ✅Direstui kasubdiv ambil proyek kantor sebagai topik skripsi ✅Alhamdulillah, berkah 😇',\n",
       "  'Antara gak menghargai data sensitif (kebobolan) atau gak meghargai user setelah mendaftar (telemarketer ngejar target  dengan cara spam call \"calon\" nasabah)',\n",
       "  'Ya, walaupun gw sekarang optimis sih yang nelpon gw itu telemarketer dan notif wa itu cuma untuk promosi setelah telpon hotline customer service. \\n\\nGak ada kah yang namanya SOP bagi seorang telemarketer? Manggil calon customer sampai berkali² dengan notelp yang berbeda²',\n",
       "  'Bjir, ternyata cuma iklan yang di WA itu 🥲',\n",
       "  '@gork@gork   is this true?',\n",
       "  'Doxxing jadi hal yang umum\\nWitch huntpun ada, terutama bila bukan pendukung dari golongan gang sama\\nMob justice juga sudah banyak terjadi meskipun verdict di akhir menampilkan casus belli yang tidak jelas dan tidak faktual. \\nWow lah.',\n",
       "  'Twitter itu hebat,\\nPlatform paling cepat up informasi tapi banyak misinfofmasi\\nPlatform yang menyuarakan free speech tapi ownernya kontroversial\\nPlatform paling open minded tapi 11/12 seperti app kandang monyet.\\nPlatform kritis tapi banyak isu yang diposisikan black and white',\n",
       "  'Lanjut perpanjang pkwt apa lanjut skripsi? \\n\\nPengen cepet² dapet SKL 😔',\n",
       "  '@grok@grok  is this true?',\n",
       "  '@gork@gork  is this true?',\n",
       "  'Now do it with the T72B3 :3',\n",
       "  'x.com/shitpost_2077/\\n… …',\n",
       "  'Laptop kantor my beloved (akhirnya gak berbasis collab) 🥰',\n",
       "  'Pop mi',\n",
       "  \"Been playing since 2013. I used to play it with various friends I'd met online, but mostly alone. Now, i have a nephew who has just started elementary that occasionally asks me to play together. \\n\\nTime sure flies fast\",\n",
       "  '17:00 Masih di kereta: ❌17:00 Dah sampe rumah: ✅Kantor deket rumah itu enak tapi gak ada alasan untuk masuk telat bjir',\n",
       "  'Dwifungsi IT',\n",
       "  'info loker bang',\n",
       "  'Libur telah usai 😞',\n",
       "  'Semua orang pernah ngerasa gitu kok',\n",
       "  'BI ini cok, moal aya lamun urang jawa mah',\n",
       "  'Ada, mau yang BI pun ada',\n",
       "  'Ts peak',\n",
       "  'Beda agama Fik☹️',\n",
       "  'Tmn yang di us, bisa dapet part time sbg store clerk digaji 12$/hour.\\nTmn di israel dapet full time sbg sales bank digaji 18$/hour.\\nTemen di Russia masuk ke militer dpt base gaji 1800$~ per bulan. \\n\\nNjir, orang² keknya tajir bener dah. Moga kekayaaanya bisa nyebar ke gw',\n",
       "  'Rill cui, jawab aja \"doaiin aja wisuda Agustus om/tante 🙂\"',\n",
       "  'Gaji jakarta tapi tinggal di jateng sepertinya enak....',\n",
       "  'Kangen!?',\n",
       "  'Dude, i thought you were a man. Unfollowed 😔',\n",
       "  'Kalau jungler mid lane bisa gak fik?',\n",
       "  'Belum nemu jodohnya bang 🥲🙏',\n",
       "  'Biasanya model  yang gini nikah lebih duluan daripada temen-temennya Fik 😂',\n",
       "  'Ini gw kerja apa dikerjain dah, sial.',\n",
       "  'Timeline gak diundur, tapi keinginan banyak dan berubah-rubah.\\nKalau gw tau dari hari awal kalau cuma mau dipake 17 kelas mah, udah gw beresin dan tingkatin dari kapan tahu anj.\\nGoblok.',\n",
       "  'Minggu kedua di hari jumat baru ada rapat lagi kalau akhirnya yang dipake cm 17 kelas (18 tapi gw nego). Cuma perubahan model dari image clas ke object detection.\\nHari jumat langsung gas subset data dan upload untuk anotasi.',\n",
       "  'Waktu 3 minggu, minta A-Z.\\nminggu pertama minta 58 kelas, alhamdulillah terlaksana dan sudah beres hingga tahapan deploy dengan akurasi yang baik.\\nMinggu kedua diminta penambahan 100 kelas, oke lah kita bantu pengambilan data yang sesuai dengan kelasnya.',\n",
       "  'Mana ada penambahan data baru hari Rabu yang belum dianotasi sampe saat ini dan dipaksa untuk dipakai sebelum tanggal 8.\\nKemana hak gw untuk berlibur di hari raya?',\n",
       "  'Minimal ngotak dikit lah bangsat, \\nMulai anotasi Senin, selesai anotasi hari Selasa. Masuk tahap pemodelan hari Rabu dan diekpetasi untuk deploy hari Jumat ini?\\nGblk',\n",
       "  'terima gaji telat, deadline gak ngotak, hari libur pun terpaksa lembur.\\n\\nMemang kontol',\n",
       "  'Me n who fr',\n",
       "  'Thrnya dong tante 😇🙏',\n",
       "  'Susah susah buat framework 3rd stage image classification dengan akurasi yang tinggi dengan gamabar terbatas. Malah diminta ke object detection satu stage saja dengan gimik waktunya lama (padahal di deploy streamlit gw cepet).\\nMemang anjing',\n",
       "  'Mau jam malan siang bg?',\n",
       "  'Pricy euy tapi 🥲',\n",
       "  'Bola kalah 5-1 dan ruu TNI disahkan 😇🙏',\n",
       "  'Ya Allah, kenapa aku WNI',\n",
       "  'None',\n",
       "  'ANOTHER CLSSSIC Z VICTORY. ZZZZZZZZ',\n",
       "  'Welcoms to rock bottom',\n",
       "  'Aku pilih kamu',\n",
       "  \"LET'S FUCKING GOOOOO HAHAHAHAHAHAAHA\",\n",
       "  'Cari ani-ani aja fik',\n",
       "  'Database saja pake google drive anjir',\n",
       "  'Di database hasil scrappingan ada total 60~ ribu gambar dengan 49 kelas. Kepaksa ngambil 100 gambar dari masing² kelas karena tahapan verifikasi tuh hilang ntah ke mana karena birokrasi + bukan jobdesk kita.Seginipun seharusnya pengambilan data tuh dikasih bukan kita cari sendiri',\n",
       "  'Mana minta deploy sebelum Kamis pake API tapi cloud servernya gak ada. Wow',\n",
       "  '\"Ini akan sulit pak karena kekurangan data, lusa lalu sudah minta data ke Bapak X, Y, Z tapi beberapa ada yang belum ngirim dan adapun yang ngirim, per kelasnya cuma satu\"\\n\\n\"Kan kamu bisa ngegunaiin satu gambar untuk ngelatih model pake augmentasi\"\\n\\nBjir',\n",
       "  'Live service sih live service, tapi gak gini juga lah anjir. Di kira gw Roro Jonggrang bisa buat candi semalem',\n",
       "  'Gw jadi paham kenapa banyak banget game triple A  / proyekan pemerintah yang dipaksa deploy walaupun belum jadi seutuhnya.\\n\\nTimeline 8 minggu dijadikan 5 hari kerja + sudah harus dideploy dan dipresentasikan di hari tsb dengan gimik sistem akan terus diperbarui (Live service).',\n",
       "  'Air of the chinese century',\n",
       "  '>',\n",
       "  '>',\n",
       "  'Since when did it get reverted? Should have been increased to at least 15% minimum. I love paying taxes.',\n",
       "  'What GHPC version is this?',\n",
       "  'Well we could say the jews had a swimming pool in one of the camps. \"Concentration camps\" Where the oppressed had private pools.',\n",
       "  'People in real life: hey man, how is it going',\n",
       "  'Me',\n",
       "  'Usual suspect',\n",
       "  'Kan sekantor fik wkwkwk',\n",
       "  '> ack!',\n",
       "  'Just use aws or gcp duh, they offer free credits for a newly created acc in which you can spend it on a vps with great specs.',\n",
       "  'x.com/AdrianneCurry/\\n… …',\n",
       "  'Do you ship to Indonesia? 🥰',\n",
       "  \"How's the weather in Sukabumi my fellow sundanese?\",\n",
       "  'Kill feral hogs. Behead feral hogs. Roundhouse kick a feral hog into the concrete. Slam dunk a feral hog piglet into the trashcan. Crucify filthy hogs. Defecate in a feral hog’s feed trough. Launch feral hogs into orbit. Stir fry feral hogs in a wok.',\n",
       "  'Agus buntung',\n",
       "  'Gak jadi extend bang?',\n",
       "  'Rill',\n",
       "  'Bawa boneka aja bang, mayan ada yang nemenin',\n",
       "  'Ajak baku hantam aja OP',\n",
       "  'Blok aja mas',\n",
       "  \"Same energy, they sure became the thing they hate the most. Don't they?\",\n",
       "  'Bjir, resign bang',\n",
       "  '@ID_MyRepublic@ID_MyRepublic  Tiap hari packet loss terutama pada malam hari, hal ini sudah berlangsung sejak tahun 2023. Kapan dibenahi?',\n",
       "  '@MyRepublicID@MyRepublicID  Internet mu hebat, mengajarkanku rasa untuk bersabar 😇',\n",
       "  '@MyRepublicID@MyRepublicID  Halo, ini kenapa setiap malam internet suka tidak stabil ya? Niat rekreasi dapetnya emosi'],\n",
       " 'quotedPost_text': ['',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'A leaked WikiLeaks cable from 2007 reveals some of UAE Mohammed bin Zayed’s views on free elections, Islam, Quran schools…. Pakistan…  Thread 1,2,3,4,5,6,7,8,9',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Glodok',\n",
       "  '',\n",
       "  ' URGENT:\\nIn Jakarta, Indonesia (28/8/2025) police brutality has led to a civilian killed after being run over by a Brimob vehicle\\n\\nCCTV in the area has been shut off. Please help amplify this globally because the Indonesia government and police want to make this stay hidden x.com/suprayit_now/s…',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'how would a criminal billionaire become president?? i know it’s comics but this is a little too unrealistic x.com/AxelTalksFilm/…',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Southeast Asia',\n",
       "  '',\n",
       "  '',\n",
       "  'Share a piece of S.T.A.L.K.E.R lore about yourself',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'tweet like you live in metropolis: x.com/jayjjalen/stat…',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ' Joshua (IG: joshuamsolomon_) / OXFORD / Computer Science\\n\\n- Honorable Mention - International Mathematical Olympiad 2023 in Japan\\n- Bronze Medal - International Zhautykov Olympiad (IZhO) in Mathematics 2023\\n- Third Prize - European Mathematical Cup (EMC) 2023',\n",
       "  'Talking points have been issued.',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Oh shut up, @GretaThunberg - you attention-seeking narcissist. What an insult to the actual hostages in Gaza who really WERE kidnapped. This stupid stunt is all about your ego, and will make zero difference to the plight of innocent Palestinians caught up in this dreadful war. x.com/Osint613/statu…',\n",
       "  'kenapa sih yang suka sama gua selalu cowo jelek aneh dan mesummmm????? \\n\\nkenapa gaada cowo ganteng soft spoken tinggi 180 cm lebih rajin ibadah dateng depan mata guaaa??',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Having 14,000 people commenting on this post telling me how we fucking deserved these missiles, that not enough Ukrainian kids died, that \"You can\\'t bomb a fleet of Russian aircraft and expect nothing in return.\" Has destroyed all hope I had left in humanity.\\n\\nFuck this platform. x.com/frontlinekit/s…',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'It’s a war crime for a terrorist warlord to hide in a bunker under a hospital. It is not a war crime to target that terrorist warlord. You’re welcome. x.com/MosabAbuToha/s…',\n",
       "  '',\n",
       "  'There’s ZERO evidence that Pakistan shot down an Indian Air Force Rafale jet.',\n",
       "  'जी',\n",
       "  '',\n",
       "  'Well said',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  \"Mahasiswi ITB Ditangkap Usai Buat Meme Prabowo-Jokowi 'Ciuman'\",\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'degens… with an air of aristocracy',\n",
       "  '',\n",
       "  'Mending ngerokok dan narkobaan dah sumpah. x.com/SBKCF/status/1…',\n",
       "  '',\n",
       "  '',\n",
       "  '“Open-air prison” where prisoners have private pools.',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Kenapa bang? Ada niat ngebom? @DivHumas_Polri @CCICPolri',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'They hate us.\\n\\nThey despise our values.\\n\\nThey kill us.\\n\\nAnd our children.\\n\\nWe have to stand up.\\n\\nWith perseverance and vigor,\\n\\nNo more. \\n\\nNever again.\\n\\nThis is our land, our freedom, our life.\\n\\nAnd we’ll defend it and never surrender. \\n\\nJoin me and we’ll win. \\n\\nWe will win!',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " 'Reply_count': [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  49,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " 'Repost_count': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  197,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'Like_count': [0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  54,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2262,\n",
       "  0,\n",
       "  82,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  10,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  54,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'View_count': [284,\n",
       "  45,\n",
       "  38,\n",
       "  2277,\n",
       "  39,\n",
       "  54,\n",
       "  35,\n",
       "  25,\n",
       "  323,\n",
       "  115,\n",
       "  30,\n",
       "  51,\n",
       "  306,\n",
       "  45,\n",
       "  28,\n",
       "  41,\n",
       "  34,\n",
       "  15,\n",
       "  48,\n",
       "  41,\n",
       "  47,\n",
       "  68,\n",
       "  44,\n",
       "  73,\n",
       "  64,\n",
       "  29,\n",
       "  124,\n",
       "  20,\n",
       "  45,\n",
       "  55,\n",
       "  64,\n",
       "  58,\n",
       "  79,\n",
       "  72,\n",
       "  63,\n",
       "  63,\n",
       "  44,\n",
       "  129,\n",
       "  71,\n",
       "  49,\n",
       "  59,\n",
       "  301,\n",
       "  76,\n",
       "  65,\n",
       "  68,\n",
       "  71,\n",
       "  63,\n",
       "  57,\n",
       "  90,\n",
       "  81,\n",
       "  88,\n",
       "  93,\n",
       "  82,\n",
       "  178,\n",
       "  52,\n",
       "  82,\n",
       "  182,\n",
       "  77,\n",
       "  73,\n",
       "  123,\n",
       "  37,\n",
       "  49,\n",
       "  60,\n",
       "  127,\n",
       "  33,\n",
       "  50,\n",
       "  244,\n",
       "  181,\n",
       "  92,\n",
       "  41,\n",
       "  33,\n",
       "  63,\n",
       "  178,\n",
       "  247,\n",
       "  68,\n",
       "  73,\n",
       "  63,\n",
       "  218,\n",
       "  96,\n",
       "  237,\n",
       "  705,\n",
       "  1569,\n",
       "  19,\n",
       "  39,\n",
       "  43,\n",
       "  168,\n",
       "  26,\n",
       "  215031,\n",
       "  71,\n",
       "  4216,\n",
       "  71,\n",
       "  61,\n",
       "  129,\n",
       "  222,\n",
       "  62,\n",
       "  67,\n",
       "  294,\n",
       "  137,\n",
       "  3969,\n",
       "  78,\n",
       "  128,\n",
       "  4817,\n",
       "  61,\n",
       "  362,\n",
       "  86,\n",
       "  43,\n",
       "  60,\n",
       "  63,\n",
       "  85,\n",
       "  94,\n",
       "  41,\n",
       "  56,\n",
       "  29,\n",
       "  59,\n",
       "  92,\n",
       "  259,\n",
       "  119,\n",
       "  31,\n",
       "  48,\n",
       "  31,\n",
       "  36,\n",
       "  64,\n",
       "  78,\n",
       "  22,\n",
       "  50,\n",
       "  72,\n",
       "  48,\n",
       "  42,\n",
       "  114,\n",
       "  26,\n",
       "  78,\n",
       "  88,\n",
       "  65,\n",
       "  5744,\n",
       "  203,\n",
       "  103,\n",
       "  186,\n",
       "  70,\n",
       "  51,\n",
       "  150,\n",
       "  79,\n",
       "  100,\n",
       "  73,\n",
       "  60,\n",
       "  28,\n",
       "  77,\n",
       "  57,\n",
       "  90,\n",
       "  76,\n",
       "  194,\n",
       "  30,\n",
       "  62,\n",
       "  27,\n",
       "  47,\n",
       "  27,\n",
       "  31,\n",
       "  41,\n",
       "  68,\n",
       "  81,\n",
       "  84,\n",
       "  86,\n",
       "  56,\n",
       "  59,\n",
       "  54,\n",
       "  87,\n",
       "  49,\n",
       "  53,\n",
       "  50,\n",
       "  54,\n",
       "  52,\n",
       "  57,\n",
       "  33,\n",
       "  277,\n",
       "  40,\n",
       "  39,\n",
       "  63,\n",
       "  55,\n",
       "  52,\n",
       "  47,\n",
       "  74,\n",
       "  123,\n",
       "  79,\n",
       "  69,\n",
       "  255,\n",
       "  42,\n",
       "  161,\n",
       "  88,\n",
       "  206,\n",
       "  172,\n",
       "  308,\n",
       "  117,\n",
       "  172,\n",
       "  30,\n",
       "  961,\n",
       "  53,\n",
       "  118,\n",
       "  50,\n",
       "  111,\n",
       "  118,\n",
       "  120,\n",
       "  51,\n",
       "  81,\n",
       "  39,\n",
       "  89,\n",
       "  44,\n",
       "  147,\n",
       "  96,\n",
       "  105,\n",
       "  73,\n",
       "  87]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(str):\n",
    "    '''\n",
    "    This function is used to convert string of datetime in isoformat to datetime object\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        date_time_obj:\n",
    "            - Datetime object\n",
    "    '''\n",
    "    date_time_obj = datetime.fromisoformat(str)\n",
    "    date_time_obj =date_time_obj + timedelta(hours=7)\n",
    "    return date_time_obj\n",
    "\n",
    "def minOneDay(str):\n",
    "    '''\n",
    "    This function is used to subtract one day from the given date\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        str:\n",
    "            - String of datetime in isoformat after subtracting one day\n",
    "    '''\n",
    "    str = datetime.strptime(str, \"%Y-%m-%d\")\n",
    "    str = str - timedelta(days=1)\n",
    "    str = str.strftime(\"%Y-%m-%d\")\n",
    "    return str\n",
    "\n",
    "def wait(timeout: int = 10):\n",
    "    '''\n",
    "    just a glorified simple function to wait for a certain amount of time\n",
    "    '''\n",
    "    for i in tqdm(range(timeout), desc=\"Waiting\"):\n",
    "        time.sleep(1)\n",
    "    clear_output()  \n",
    "\n",
    "# TODO: Rewrite and add profile post scraper function thus completing this entire fucking project \n",
    "\n",
    "class twitterScrapper:\n",
    "    def __init__(self, credentials: str = \"Credentials/twitter.json\",\n",
    "                 ):\n",
    "        '''\n",
    "        This function is used to initialize the class and will also login to twitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        credentials : str\n",
    "            - Path to the twitter credentials json file\n",
    "            - The json file should be in the following format:\n",
    "            ```\n",
    "            {   \"username\" : \"your_username\",\n",
    "                \"password\" : \"your_password\",\n",
    "                \"email\"    : \"your_email\"}\n",
    "            ```\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(credentials):\n",
    "            raise FileNotFoundError(\"Credentials file not found!\")\n",
    "\n",
    "        with open(credentials, \"r\") as f:\n",
    "            credentials = json.load(f)\n",
    "            try:\n",
    "                username = credentials[\"username\"]\n",
    "                password = credentials[\"password\"]\n",
    "                email = credentials[\"email\"]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Credentials file is not in the correct format!\")\n",
    "        \n",
    "        # Check if any of those credentials is None\n",
    "        if username is None or password is None:\n",
    "            raise ValueError(\"Username or password can't be empty!\")\n",
    "        if email is None:\n",
    "            warnings.warn(\"Email is not provided, this shit might not work if suspicious login attempt is detected!\", UserWarning)\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "\n",
    "        # For storing all the data\n",
    "        self.theDict = { \"User\" : [], \"Date\" : [], \"Text\" : [],\n",
    "                         \"Reply\": [], \"Repost\": [], \"Like\": [], \"View\": []}\n",
    "        \n",
    "        self.login()\n",
    "    \n",
    "    def login(self):\n",
    "        '''\n",
    "        Tries to login to the account based from the given credentials\n",
    "\n",
    "        Will use `self.username` and `self.password` only. However, if X detects your login as suspicious, `self.email` will be used.\n",
    "        '''\n",
    "        # TODO: FUCK THEY CHANGED THE METHOD\n",
    "\n",
    "        # Twitter login URL, Which will redirect to home after login\n",
    "        loginURL = 'https://x.com/i/flow/login?redirect_after_login=%2Fsearch%3Fq%3Disrael%26src%3Dtyped_query%26f%3Dlive%26mx%3D2'\n",
    "        \n",
    "        # Initialize the driver\n",
    "        usergAgent = generate_user_agent(device_type=\"desktop\", os=\"win\", navigator=\"chrome\", platform=\"win\")\n",
    "        options = Options()\n",
    "        options.add_argument(f'user-agent={usergAgent}')\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(loginURL)\n",
    "\n",
    "        # Login handling\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(self.username)\n",
    "        self.driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "        time.sleep(5)               # This will wait for the next login pop-up to appear\n",
    "\n",
    "        # if email is neeeded. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "        try:\n",
    "            if self.driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "                print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "                if self.email is None:\n",
    "                    raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "                self.driver.find_element(By.XPATH, \"//input\").send_keys(self.email)\n",
    "                self.driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Put password\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(self.password)\n",
    "        self.driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "        print(\"Login sucess!\")\n",
    "        wait(10)\n",
    "    \n",
    "    # TODO: REWRITE THIS ENTIRE FUCKING THINGS THUS ALL THE FILTERINGS WILL EVENTUALLY WORK!\n",
    "    def searchAndscrap(self, keyword, startDate, endDate, continueifTimeout = True):\n",
    "        '''\n",
    "        This function is used to load the post and scrap the data\n",
    "        Params:\n",
    "            keyword:\n",
    "                - Keyword to search need to be encoded beforehand\n",
    "            startDate:\n",
    "                - Start date of the search\n",
    "            endDate:\n",
    "                - End date of the search\n",
    "        '''\n",
    "        notTimeout = True\n",
    "        notDateReached = True \n",
    "        \n",
    "        while True:\n",
    "            if not notDateReached:\n",
    "                print(\"startDate has been reached!\")\n",
    "                pd.DataFrame(self.theDict).to_csv(f'Completed.csv', index=False)\n",
    "                break\n",
    "            \n",
    "            untilDate = endDate\n",
    "            searchLink = f\"https://x.com/search?q={keyword}%20until%3A{untilDate}%20since%3A{startDate}&src=typed_query&f=live\"\n",
    "            self.driver.get(searchLink)\n",
    "            \n",
    "            # This will check if scrapping is detected, if it is. It'll wait for 20 mins\n",
    "            if continueifTimeout:\n",
    "                try:\n",
    "                    #   This is essential to check if the scrapping is detected\n",
    "                    WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "                    print(\"Saving!\")\n",
    "                    pd.DataFrame(self.theDict).to_csv(f'Savepoints/{untilDate}.csv', index=False)\n",
    "                    print(\"Scrapping detected, waiting for 15 mins\")\n",
    "                    for i in tqdm(range(900), desc=\"Waiting\"):\n",
    "                        time.sleep(1)\n",
    "                    clear_output()\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "            if not continueifTimeout:\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "                    notTimeout = False\n",
    "                except:\n",
    "                    pass\n",
    "                if not notTimeout: break\n",
    "            \n",
    "            # This will wait for the page to load, if nothing exists. Minus one day and repeat\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\")))   # Initialize\n",
    "            except:\n",
    "                endDate = minOneDay(endDate)\n",
    "                continue\n",
    "            \n",
    "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "\n",
    "                elements = self.driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "                \n",
    "                for element in elements[:-1]:\n",
    "                    \n",
    "                    # Check if text exists, if not then continue.  \n",
    "                    try:\n",
    "                        text = ''.join([i.text for i in element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')])\n",
    "                    except:\n",
    "                        continue\n",
    "                    if text in self.theDict[\"Text\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    \n",
    "                    # TODO: Get date and check if theDate is lower than startDate. Break entire loop\n",
    "                    theDate = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\"))\n",
    "                    \n",
    "                    # append text\n",
    "                    self.theDict[\"Text\"].append(text)\n",
    "                    \n",
    "                    # append user\n",
    "                    self.theDict[\"User\"].append(element.find_element(By.XPATH, './/a/div/span').text)\n",
    "                    \n",
    "                    # append Date\n",
    "                    self.theDict[\"Date\"].append(theDate.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "                    \n",
    "                    # post attrs\n",
    "                    for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                        self.theDict[\"Reply\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                        self.theDict[\"Repost\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                        self.theDict[\"Like\"].append(int(re.search(r'\\d+',group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                        self.theDict[\"View\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))[0]) if re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\")) else 0)\n",
    "                \n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(5)\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                # This check if startdate has been reached, thus breaking the entire function.\n",
    "                if getTime(self.theDict[\"Date\"][-1]) < getTime(startDate):\n",
    "                    notDateReached = False\n",
    "                    break\n",
    "                \n",
    "                # will break if the scrollbar is at the bottom\n",
    "                if new_height == last_height:\n",
    "                    endDate = '-'.join(self.theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                    # This will check if the untilDate is the same as endDate, if it is. It'll minus one day.\n",
    "                    if untilDate == endDate:\n",
    "                        endDate = minOneDay(endDate)\n",
    "                    break\n",
    "                \n",
    "                last_height = new_height"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
