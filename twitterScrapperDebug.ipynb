{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Scrapping and crawling modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from user_agent import generate_user_agent\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "\n",
    "# New chromediver\n",
    "import undetected_chromedriver as uc\n",
    "from requests.utils import quote, unquote\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(str):\n",
    "    '''\n",
    "    This function is used to convert string of datetime in isoformat to datetime object\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        date_time_obj:\n",
    "            - Datetime object\n",
    "    '''\n",
    "    date_time_obj = datetime.fromisoformat(str)\n",
    "    date_time_obj =date_time_obj + timedelta(hours=7)\n",
    "    return date_time_obj\n",
    "\n",
    "def minOneDay(str):\n",
    "    '''\n",
    "    This function is used to subtract one day from the given date\n",
    "    Params:\n",
    "        str:\n",
    "            - String of datetime in isoformat\n",
    "    return:\n",
    "        str:\n",
    "            - String of datetime in isoformat after subtracting one day\n",
    "    '''\n",
    "    str = datetime.strptime(str, \"%Y-%m-%d\")\n",
    "    str = str - timedelta(days=1)\n",
    "    str = str.strftime(\"%Y-%m-%d\")\n",
    "    return str\n",
    "\n",
    "def wait(timeout: int = 10):\n",
    "    '''\n",
    "    just a glorified simple function to wait for a certain amount of time\n",
    "    '''\n",
    "    for i in tqdm(range(timeout), desc=\"Waiting\"):\n",
    "        time.sleep(1)\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "def safe_int_from_aria(aria_label: str) -> int:\n",
    "    match = re.search(r\"\\d+\", aria_label or \"\")\n",
    "    return int(match.group(0)) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twitterScrapper:\n",
    "    def __init__(self, credentials: str = \"Credentials/twitter.json\",\n",
    "                 ):\n",
    "        '''\n",
    "        This function is used to initialize the class and will also login to twitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        credentials : str\n",
    "            - Path to the twitter credentials json file\n",
    "            - The json file should be in the following format:\n",
    "            ```\n",
    "            {   \"username\" : \"your_username\",\n",
    "                \"password\" : \"your_password\",\n",
    "                \"email\"    : \"your_email\"}\n",
    "            ```\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(credentials):\n",
    "            raise FileNotFoundError(\"Credentials file not found!\")\n",
    "\n",
    "        with open(credentials, \"r\") as f:\n",
    "            credentials = json.load(f)\n",
    "            try:\n",
    "                username = credentials[\"username\"]\n",
    "                password = credentials[\"password\"]\n",
    "                email = credentials[\"email\"]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Credentials file is not in the correct format!\")\n",
    "        \n",
    "        # Check if any of those credentials is None\n",
    "        if username is None or password is None:\n",
    "            raise ValueError(\"Username or password can't be empty!\")\n",
    "        if email is None:\n",
    "            warnings.warn(\"Email is not provided, this shit might not work if suspicious login attempt is detected!\", UserWarning)\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "\n",
    "        # For storing all the data\n",
    "        self.theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "                         \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}\n",
    "        \n",
    "        self.login()\n",
    "\n",
    "    # Utils\n",
    "    def build_search_url(self, date_limit: str) -> str:\n",
    "        '''\n",
    "        This function is used to build the search URL based on the given date limit.\n",
    "        '''\n",
    "        return f\"{self.SEARCH_URL}{self.FILTERS_COMBINATION}{quote(f' until:{date_limit}')}&f=live&src=typed_query\"\n",
    "\n",
    "    def scrape_detected(self) -> bool:\n",
    "        try:\n",
    "            sumthingwrong = WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                 EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span'))\n",
    "            )\n",
    "            sumthingwrong.text == \"Something went wrong. Try reloading.\"\n",
    "            return True\n",
    "        \n",
    "        except TimeoutException:\n",
    "            return False\n",
    "\n",
    "    def wait_for_posts_or_stepback(self, current_date: str, counter: int) -> tuple[str, int, bool]:\n",
    "        try:\n",
    "            # Is there a container for post?\n",
    "            WebDriverWait(self.driver, self.WAIT_LONG).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\"))\n",
    "            )\n",
    "            counter = 0\n",
    "            return current_date, counter, False # Yes\n",
    "        except TimeoutException:\n",
    "            current_date = minOneDay(current_date)\n",
    "            counter += 1\n",
    "            return current_date, counter, counter > self.MAX_EMPTY_PAGES    # Nothing ever happens and minus by one day\n",
    "\n",
    "    def parse_post(self, post_element) -> str:\n",
    "        parts = post_element.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "        text = \"\"\n",
    "        for p in parts:\n",
    "            if p.tag_name == \"img\":\n",
    "                text += p.get_attribute(\"alt\")\n",
    "            elif p.tag_name == \"a\":\n",
    "                text += p.text + \" \"\n",
    "            else:\n",
    "                text += p.text\n",
    "        return text\n",
    "\n",
    "    def extract_post_data(self, element) -> tuple[str, str, str, str]:\n",
    "        post_element = element.find_element(\n",
    "            By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]',\n",
    "        )\n",
    "        post_text = self.parse_post(post_element)\n",
    "        try:\n",
    "            quoted_element = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "            quoted_text = ''.join(\n",
    "                i.text for i in quoted_element.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')\n",
    "            )\n",
    "        except NoSuchElementException:\n",
    "            quoted_text = \"\"\n",
    "\n",
    "        post_date = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        post_user = element.find_element(By.XPATH, './/a/div/span').text\n",
    "        return post_text, quoted_text, post_user, post_date\n",
    "    \n",
    "    def _write_json(self, filename: str) -> None:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.theDict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _write_csv(self, filename: str) -> None:\n",
    "        df = pd.DataFrame(self.theDict)\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    def save(self, type: Literal[\"final\", \"savepoint\"]) -> str:\n",
    "        if type not in {\"final\", \"savepoint\"}:\n",
    "            raise ValueError(\"Save type must be 'final' or 'savepoint'.\")\n",
    "\n",
    "        os.makedirs(f\"Process/{self.processDir}/Savepoints\", exist_ok=True)\n",
    "        \n",
    "        if type == \"savepoint\":\n",
    "            save_path = f\"Process/{self.processDir}/Savepoints/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        else:\n",
    "            save_path = f\"Process/{self.processDir}/Final\"\n",
    "\n",
    "        if self.saveFormat == \"csv\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "        elif self.saveFormat == \"json\":\n",
    "            self._write_json(f\"{save_path}.json\")\n",
    "        elif self.saveFormat == \"both\":\n",
    "            self._write_csv(f\"{save_path}.csv\")\n",
    "            self._write_json(f\"{save_path}.json\")\n",
    "        else:\n",
    "            raise ValueError(\"saveFormat must be 'csv', 'json', or 'both'.\")\n",
    "        \n",
    "        return save_path\n",
    "    \n",
    "    def login(self):\n",
    "        '''\n",
    "        Tries to login to the account based from the given credentials\n",
    "\n",
    "        Will use `self.username` and `self.password` only. However, if X detects your login as suspicious, `self.email` will be used.\n",
    "        '''\n",
    "        # Initialize the driver and check bot detection\n",
    "        self.driver = uc.Chrome()\n",
    "        self.driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "        # Check bot detection, \n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "        time.sleep(4)\n",
    "        botResult = self.driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "        if botResult != \"Normal\":\n",
    "            warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "        # Get to X login page\n",
    "        self.driver.get(\"https://x.com/i/flow/login\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Login handling\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(self.username)\n",
    "        self.driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # if email is needed. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "        try:\n",
    "            if self.driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "                print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "                if self.email is None:\n",
    "                    raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "                self.driver.find_element(By.XPATH, \"//input\").send_keys(self.email)\n",
    "                self.driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Put password\n",
    "        WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "        self.driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(self.password)\n",
    "        self.driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "        print(\"Login sucess!\")\n",
    "        wait(10)\n",
    "\n",
    "    def start(self, filters, startDate: str = \"\", endDate: str = \"\",\n",
    "              scraping_Params  =  {\"wait_short\": 10, \"wait_long\": 30,\n",
    "                                  \"detection_wait\": 900, \"max_empty_pages\": 2},\n",
    "                                  saveFormat: Literal[\"csv\", \"json\", \"both\"] = \"csv\", \n",
    "                                  autoSave: bool = False, autoSaveInterval: int = 15, continue_if_timeout: bool = True,\n",
    "                                  processDir: str = \"\"):\n",
    "        \n",
    "        '''\n",
    "        This function is used to start the scrapping process based on the given filters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filters : dict\n",
    "            - A dictionary containing the filters for scrapping.\n",
    "            - The dictionary should be in the following format:\n",
    "            ```\n",
    "            {\n",
    "                \"keywords\": \"your keywords\",\n",
    "                \"from\": \"username\",\n",
    "                \"to\": \"username\",\n",
    "                \"mentions\": \"username\",\n",
    "                \"since\": \"YYYY-MM-DD\",\n",
    "                \"until\": \"YYYY-MM-DD\",\n",
    "                \"min_replies\": int,\n",
    "                \"min_likes\": int,\n",
    "                \"min_reposts\": int,\n",
    "                \"min_views\": int,\n",
    "                \"has_links\": bool,\n",
    "                \"has_media\": bool,\n",
    "                \"has_photos\": bool,\n",
    "                \"has_videos\": bool,\n",
    "                \"has_gifs\": bool\n",
    "            }\n",
    "            ```\n",
    "        '''\n",
    "        self.SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "        # Adjust filters values\n",
    "        filters[\"this_exact_phrase\"] = f'\\\"{filters[\"this_exact_phrase\"]}\\\"' if filters[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "        filters[\"any_of_these_words\"] = f'({filters[\"any_of_these_words\"]})' if filters[\"any_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"none_of_these_words\"] = f'-{filters[\"none_of_these_words\"]}' if filters[\"none_of_these_words\"] != \"\" else \"\"\n",
    "        filters[\"these_hashtags\"] = f'({filters[\"these_hashtags\"]})' if filters[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"from_accounts\"] = f'(from:{filters[\"from_accounts\"]})' if filters[\"from_accounts\"] != \"\" else \"\"\n",
    "        filters[\"to_accounts\"] = f'(to:{filters[\"to_accounts\"]})' if filters[\"to_accounts\"] != \"\" else \"\"\n",
    "        filters[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in filters[\"mentioning_accounts\"].split())})' if filters[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "        filters[\"replies\"] = \"\" if filters[\"replies\"] else \"-filter:replies\" \n",
    "        filters[\"links\"] = \"\" if filters[\"links\"] else \"-filter:links\"\n",
    "\n",
    "        filters[\"Minimum_replies\"] = f'min_replies:{filters[\"Minimum_replies\"]}' if filters[\"Minimum_replies\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_likes\"] = f'min_faves:{filters[\"Minimum_likes\"]}' if filters[\"Minimum_likes\"] != \"\" else \"\"\n",
    "        filters[\"Minimum_retweets\"] = f'min_retweets:{filters[\"Minimum_retweets\"]}' if filters[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "        FILTERS_COMBINATION = \"\"\n",
    "        for _, value in filters.items():\n",
    "            if value != \"\":\n",
    "                FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "        self.FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "\n",
    "        # Dates handling\n",
    "        if startDate == \"\":\n",
    "            startDate = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if endDate == \"\":\n",
    "            endDate = \"2006-01-01\"  # Twitter launch date\n",
    "        self.start_date = startDate\n",
    "        self.end_date = endDate\n",
    "\n",
    "        # scraping params\n",
    "        self.WAIT_SHORT = scraping_Params[\"wait_short\"]\n",
    "        self.WAIT_LONG = scraping_Params[\"wait_long\"]\n",
    "        self.DETECTION_WAIT = scraping_Params[\"detection_wait\"]\n",
    "        self.MAX_EMPTY_PAGES = scraping_Params[\"max_empty_pages\"]\n",
    "\n",
    "        # Other params\n",
    "        self.saveFormat = saveFormat\n",
    "        self.autoSave = autoSave\n",
    "        self.autoSaveInterval = autoSaveInterval\n",
    "        self.continue_if_timeout = continue_if_timeout\n",
    "        self.processDir = processDir if processDir != \"\" else datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        self.scrape()\n",
    "        \n",
    "    def scrape(self):\n",
    "        '''\n",
    "        This function is used to scrape the posts based on the given filters and date range.\n",
    "        '''\n",
    "        reached_all_posts = False\n",
    "        counter = 0\n",
    "        seen = set()\n",
    "        start_date = self.start_date\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Get the current date upper limit\n",
    "            current_date_limit = start_date\n",
    "\n",
    "            # If all shits been scraped, will save and break\n",
    "            if reached_all_posts:\n",
    "                print(\"All posts have been scraped!\")\n",
    "                # Delete all temps aka Savepoints\n",
    "                import shutil\n",
    "                shutil.rmtree(f'Process/{self.processDir}/Savepoints/')\n",
    "                self.save(\"final\")\n",
    "                break\n",
    "\n",
    "            self.driver.get(self.build_search_url(current_date_limit))\n",
    "            time.sleep(self.WAIT_SHORT)\n",
    "\n",
    "            # CHECKER\n",
    "            ##  1 CHECKER FOR SCRAPING DETECTION, IF `continue_if_timeout` IS TRUE, WILL WAIT AND CONTINUE, ELSE WILL JUST STOP.\n",
    "            if self.continue_if_timeout:\n",
    "                if self.scrape_detected():\n",
    "                    print(\"Scraping detected! Auto-saving progress...\")\n",
    "                    self.save(\"savepoint\")\n",
    "                    print(f\"Waiting for {self.DETECTION_WAIT} seconds\")\n",
    "                    # Wait for abyssmal amount of time\n",
    "                    wait(self.DETECTION_WAIT)\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                if self.scrape_detected():\n",
    "                    self.save(\"savepoint\")\n",
    "                    raise RuntimeError(\"Scraping detected! All progress have been saved.\")\n",
    "\n",
    "            ##  2 CHECKER FOR NO POSTS FOUND, IF SHIT HAPPENS WILL ROLE BACK FOR LIKE A DAY. IF SHIT KEEPS HAPPENING TILL `MAX_EMPTY_PAGES``, WILL STOP.\n",
    "            start_date, counter, reached_all_posts = self.wait_for_posts_or_stepback(start_date, counter)\n",
    "            if reached_all_posts:\n",
    "                print(\"No more posts found!\")\n",
    "                continue\n",
    "\n",
    "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                elements = self.driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "\n",
    "                for element in elements[:-1]:\n",
    "                    try:\n",
    "                        post_text, quoted_text, post_user, post_date = self.extract_post_data(element)\n",
    "                    except (NoSuchElementException, StaleElementReferenceException):\n",
    "                        continue\n",
    "\n",
    "                    key = (post_text, post_date, post_user)\n",
    "                    if key in seen:\n",
    "                        continue\n",
    "\n",
    "                    seen.add(key)\n",
    "                    self.theDict[\"post_text\"].append(post_text)\n",
    "                    self.theDict[\"quotedPost_text\"].append(quoted_text)\n",
    "                    self.theDict[\"User\"].append(post_user)\n",
    "                    self.theDict[\"Date\"].append(post_date)\n",
    "\n",
    "                    for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                        self.theDict[\"Reply_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "                        self.theDict[\"Repost_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "                        self.theDict[\"Like_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "                        self.theDict[\"View_count\"].append(\n",
    "                            safe_int_from_aria(group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))\n",
    "                        )\n",
    "\n",
    "                if self.autoSave and len(seen) % self.autoSaveInterval == 0:\n",
    "                    self.save(\"savepoint\")\n",
    "\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(self.WAIT_SHORT)\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                # If end date is reached, functional if user specified end_date at self.start()\n",
    "                if self.theDict[\"Date\"] and getTime(self.theDict[\"Date\"][-1]) < getTime(self.end_date):\n",
    "                    reached_all_posts = True\n",
    "                    break\n",
    "\n",
    "                if new_height == last_height:\n",
    "                    if self.theDict[\"Date\"]:\n",
    "                        last_date_only = \"-\".join(self.theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "                        if last_date_only >= current_date_limit:\n",
    "                            start_date = minOneDay(current_date_limit)\n",
    "                        else:\n",
    "                            start_date = minOneDay(last_date_only)\n",
    "                    else:\n",
    "                        start_date = minOneDay(current_date_limit)\n",
    "\n",
    "                    self.start_date = start_date\n",
    "                    break\n",
    "\n",
    "                last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTime(\"2015-06-21\") >= getTime(\"2015-06-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = twitterScrapper(\"Credentials/twitter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTES = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",           # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"jokowi\",   # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFILTES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaveFormat\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoSave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoSaveInterval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_if_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessDir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtestingAwal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 267\u001b[39m, in \u001b[36mtwitterScrapper.start\u001b[39m\u001b[34m(self, filters, startDate, endDate, scraping_Params, saveFormat, autoSave, autoSaveInterval, continue_if_timeout, processDir)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m.continue_if_timeout = continue_if_timeout\n\u001b[32m    265\u001b[39m \u001b[38;5;28mself\u001b[39m.processDir = processDir \u001b[38;5;28;01mif\u001b[39;00m processDir != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m datetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 309\u001b[39m, in \u001b[36mtwitterScrapper.scrape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScraping detected! All progress have been saved.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m##  2 CHECKER FOR NO POSTS FOUND, IF SHIT HAPPENS WILL ROLE BACK FOR LIKE A DAY. IF SHIT KEEPS HAPPENING TILL `MAX_EMPTY_PAGES``, WILL STOP.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m start_date, counter, reached_all_posts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait_for_posts_or_stepback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reached_all_posts:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo more posts found!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtwitterScrapper.wait_for_posts_or_stepback\u001b[39m\u001b[34m(self, current_date, counter)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_for_posts_or_stepback\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_date: \u001b[38;5;28mstr\u001b[39m, counter: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mWAIT_LONG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mEC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m//div[@data-testid=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcellInnerDiv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m         counter = \u001b[32m0\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m current_date, counter, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\selenium\\webdriver\\support\\wait.py:137\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.monotonic() > end_time:\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "session.start(\n",
    "    filters=FILTES,\n",
    "    saveFormat=\"both\",\n",
    "    autoSave=True,\n",
    "    autoSaveInterval=100,\n",
    "    continue_if_timeout=True,\n",
    "    processDir=\"testingAwal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Process/testingAwal/Final'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.save(\"final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual, deprecrated in favour of the syntax above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1st fucking option to handle the change\n",
    "\n",
    "# import zendriver as zd\n",
    "\n",
    "\n",
    "# browser =  await zd.start()\n",
    "# await browser.get(\"https://www.browserscan.net/bot-detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rhey\\anaconda3\\envs\\scraper\\Lib\\re\\_compiler.py:309: RuntimeWarning: coroutine 'start' was never awaited\n",
      "  break\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# # 2nd method, which works and also compactivle with selenium function and syntax\n",
    "\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# driver = uc.Chrome()\n",
    "\n",
    "# driver.get('https://www.browserscan.net/bot-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Credentials/twitter.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "    username = credentials[\"username\"]\n",
    "    password = credentials[\"password\"]\n",
    "    email = credentials[\"email\"]\n",
    "\n",
    "# Initialize the driver and check bot detection\n",
    "driver = uc.Chrome()\n",
    "driver.get('https://www.browserscan.net/bot-detection')\n",
    "\n",
    "# Check bot detection, \n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_oxrqr1\"]')))\n",
    "time.sleep(4)\n",
    "botResult = driver.find_element(By.XPATH, '//strong[@class=\"_1ikblmd\"]').text\n",
    "if botResult != \"Normal\":\n",
    "    warnings.warn(\"Bot detection failed! X login might be detected as bot\", UserWarning)\n",
    "\n",
    "# Get to X login page\n",
    "driver.get(\"https://x.com/i/flow/login\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Login handling\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete = 'username']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@autocomplete = 'username']\").send_keys(username)\n",
    "driver.find_element(By.XPATH, \"//div/button[2]\").click()\n",
    "time.sleep(5)               # This will wait for the next login pop-up to appear\n",
    "\n",
    "# if email is neeeded. This means you logged a lot to the account and X raises a suspicious login attempt.\n",
    "try:\n",
    "    if driver.find_element(By.XPATH, \"//div[1]/div/h1/span/span\").text == \"Enter your phone number or email address\":\n",
    "        print(\"Suspicious login attempt detected, attempting to enter email on login prochedures.\")\n",
    "        if email is None:\n",
    "            raise ValueError(\"Email is required due to suspicious login attempt, but email is not provided on credentials!\")\n",
    "        driver.find_element(By.XPATH, \"//input\").send_keys(email)\n",
    "        driver.find_element(By.XPATH, \"//div[2]/div/div/div/button\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Put password\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(password)\n",
    "driver.find_element(By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "warnings.warn(\"Please zoom out te browser to 25%, thus there'll be more posts loaded per scroll\", UserWarning)\n",
    "print(\"Login sucess!\")\n",
    "wait(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://x.com/search?q=%28from%3Arei1_f%29&f=live&src=typed_query'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_URL = \"https://x.com/search?q=\"\n",
    "\n",
    "FILTES = {\n",
    "    # Basic filters\n",
    "    \"all_these_words\": \"\",           # Example: what’s happening · contains both “what’s” and “happening”\n",
    "    \"this_exact_phrase\": \"\",         # Example: what’s happening · contains the exact phrase “what’s happening”\n",
    "    \"any_of_these_words\": \"\",        # Example: what’s happening · contains either “what’s” or “happening”\n",
    "    \"none_of_these_words\": \"\",       # Example: what’s happening · does not contain the words “what’s” or “happening”\n",
    "    \"these_hashtags\": \"\",           # Example: #whatshappening · contains the hashtag #whatshappening\n",
    "    \n",
    "    # Account filters\n",
    "    \"from_accounts\": \"rei1_f\",   # Example: from:Twitter · Tweets sent from the account Twitter\n",
    "    \"to_accounts\": \"\",            # Example: to:Twitter · Tweets sent in reply to the account Twitter\n",
    "    \"mentioning_accounts\": \"\",    # Example: @Twitter · Tweets that mention the account Twitter\n",
    "\n",
    "    # Additional filters\n",
    "    \"Minimum_replies\": \"\",         # Example: min_replies:100 · Tweets with at least 100 replies\n",
    "    \"Minimum_likes\": \"\",           # Example: min_faves:100 · Tweets with at least 100 likes\n",
    "    \"Minimum_retweets\": \"\",        # Example: min_retweets:100 · Tweets with at least 100 retweets\n",
    "    \"links\": True,                 # Example: filter:links · Include posts with links | If disabled, only posts without links\n",
    "    \"replies\": True,               # Example: filter:replies · Include replies and original posts | If disabled, only original posts\n",
    "}\n",
    "\n",
    "# Adjust filters values\n",
    "FILTES[\"this_exact_phrase\"] = f'\\\"{FILTES[\"this_exact_phrase\"]}\\\"' if FILTES[\"this_exact_phrase\"] != \"\" else \"\"\n",
    "FILTES[\"any_of_these_words\"] = f'({FILTES[\"any_of_these_words\"]})' if FILTES[\"any_of_these_words\"] != \"\" else \"\"\n",
    "FILTES[\"none_of_these_words\"] = f'-{FILTES[\"none_of_these_words\"]}' if FILTES[\"none_of_these_words\"] != \"\" else \"\"\n",
    "FILTES[\"these_hashtags\"] = f'({FILTES[\"these_hashtags\"]})' if FILTES[\"these_hashtags\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"from_accounts\"] = f'(from:{FILTES[\"from_accounts\"]})' if FILTES[\"from_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"to_accounts\"] = f'(to:{FILTES[\"to_accounts\"]})' if FILTES[\"to_accounts\"] != \"\" else \"\"\n",
    "FILTES[\"mentioning_accounts\"] = f'({\" OR \".join(f\"@{i}\" for i in FILTES[\"mentioning_accounts\"].split())})' if FILTES[\"mentioning_accounts\"] != \"\" else \"\"\n",
    "\n",
    "FILTES[\"replies\"] = \"\" if FILTES[\"replies\"] else \"-filter:replies\" \n",
    "FILTES[\"links\"] = \"\" if FILTES[\"links\"] else \"-filter:links\"\n",
    "\n",
    "FILTES[\"Minimum_replies\"] = f'min_replies:{FILTES[\"Minimum_replies\"]}' if FILTES[\"Minimum_replies\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_likes\"] = f'min_faves:{FILTES[\"Minimum_likes\"]}' if FILTES[\"Minimum_likes\"] != \"\" else \"\"\n",
    "FILTES[\"Minimum_retweets\"] = f'min_retweets:{FILTES[\"Minimum_retweets\"]}' if FILTES[\"Minimum_retweets\"] != \"\" else \"\"\n",
    "\n",
    "\n",
    "\n",
    "FILTERS_COMBINATION = \"\"\n",
    "for _, value in FILTES.items():\n",
    "    if value != \"\":\n",
    "        FILTERS_COMBINATION += f'{value} '\n",
    "\n",
    "FILTERS_COMBINATION = quote(FILTERS_COMBINATION.strip())\n",
    "SEARCH_URL_WITH_FILTERS = f\"{SEARCH_URL}{FILTERS_COMBINATION}&f=live&src=typed_query\"\n",
    "SEARCH_URL_WITH_FILTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "theDict = { \"User\" : [], \"Date\" : [], \"post_text\" : [], \"quotedPost_text\" : [],\n",
    "            \"Reply_count\": [], \"Repost_count\": [], \"Like_count\": [], \"View_count\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts found!\n",
      "All posts have been scraped!\n"
     ]
    }
   ],
   "source": [
    "# CASE 1, no startdate and enddate given.\n",
    "\n",
    "timeout = False\n",
    "reachedallPost = False \n",
    "continueifTimeout = True\n",
    "counter = 0\n",
    "startDate = datetime.now().strftime(\"%Y-%m-%d\") # Get ur current date\n",
    "\n",
    "while True:\n",
    "    currentSearchDate_limit = startDate\n",
    "\n",
    "    if  reachedallPost:\n",
    "        print(\"All posts have been scraped!\")\n",
    "        pd.DataFrame(theDict).to_csv(f'testRhey.csv', index=False)\n",
    "        break\n",
    "\n",
    "    # Adjust Link\n",
    "    searchLink = f\"{SEARCH_URL}{FILTERS_COMBINATION}{quote(f\" until:{currentSearchDate_limit}\")}&f=live&src=typed_query\"\n",
    "    driver.get(searchLink)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # CHECKERS!\n",
    "    # This will check if scrapping is detected, if it is. It'll wait for 20 mins\n",
    "    if continueifTimeout:\n",
    "        try:\n",
    "            #   This is essential to check if the scrapping is detected\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            print(\"Saving!\")\n",
    "            pd.DataFrame(theDict).to_csv(f'Savepoints/{str(datetime.now())}.csv', index=False)\n",
    "            print(\"Scrapping detected, waiting for 15 mins\")\n",
    "            wait(900)\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//div[@aria-label=\"Home timeline\"]/div/div/div/span[@style=\"text-overflow: unset;\"]')))\n",
    "            timeout = False\n",
    "        except:\n",
    "            pass\n",
    "        if timeout: break\n",
    "\n",
    "    # This will wait for the page to load, if nothing exists. Minus one day and repeat\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='cellInnerDiv']\")))\n",
    "    except:\n",
    "        startDate = minOneDay(startDate)\n",
    "        if counter == 2:\n",
    "            print(\"No more posts found!\")\n",
    "            reachedallPost = True\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # If everything's set, will scrape posts\n",
    "    while True:\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, '//div[@aria-label=\"Timeline: Search timeline\"]/div/div')\n",
    "        \n",
    "        for element in elements[:-1]:\n",
    "\n",
    "            # Get the actual fucking post and its fuckin text an emoji\n",
    "            try:\n",
    "                postElement = element.find_element(By.XPATH, './/div[not(@role=\"link\")]/div/div/div/div/div[@data-testid=\"tweetText\"]')\n",
    "                parts = postElement.find_elements(By.XPATH, \".//span | .//img | .//a[@dir='ltr']\")\n",
    "                postText = \"\"\n",
    "                for p in parts:\n",
    "                    if p.tag_name == \"img\":\n",
    "                        postText += p.get_attribute(\"alt\")\n",
    "                    elif p.tag_name == \"a\":\n",
    "                        postText += p.postText + \" \"\n",
    "                    else:\n",
    "                        postText += p.postText\n",
    "            except:\n",
    "                # If no text, then continue. Tf ima do with just attachments\n",
    "                continue\n",
    "\n",
    "            # Get the fucking quoted post if there is one\n",
    "            try:\n",
    "                quotedPostElement = element.find_element(By.XPATH, './/div[@role=\"link\"]')\n",
    "                quotedPostText = ''.join([i.text for i in quotedPostElement.find_elements(By.XPATH, './/div[@data-testid=\"tweetText\"]/span')])\n",
    "            except:\n",
    "                quotedPostText = \"\"\n",
    "                pass\n",
    "\n",
    "            # Check if text exists, if not then continue.\n",
    "            postDate = getTime(element.find_element(By.XPATH, './/time').get_attribute(\"datetime\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "            postUser = element.find_element(By.XPATH, './/a/div/span').text\n",
    "\n",
    "            # Check for duplicates, same text, date, and user. If yes, ignore that shit\n",
    "            if postText in theDict[\"post_text\"] and postDate in theDict[\"Date\"] and postUser in theDict[\"User\"]:\n",
    "                continue\n",
    "\n",
    "            # append\n",
    "            theDict[\"post_text\"].append(postText)\n",
    "            theDict[\"quotedPost_text\"].append(quotedPostText)\n",
    "            theDict[\"User\"].append(postUser)\n",
    "            theDict[\"Date\"].append(postDate)\n",
    "            \n",
    "            # post attrs\n",
    "            for group in element.find_elements(By.XPATH, './/div[@role=\"group\"]'):\n",
    "                theDict[\"Reply_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[1]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Repost_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[2]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"Like_count\"].append(int(re.search(r'\\d+',group.find_element(By.XPATH, './/div[3]/button').get_attribute(\"aria-label\"))[0]))\n",
    "                theDict[\"View_count\"].append(int(re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\"))[0]) if re.search(r'\\d+', group.find_element(By.XPATH, './/div[4]/a').get_attribute(\"aria-label\")) else 0)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # will break if the scrollbar is at the bottom\n",
    "        if new_height == last_height:\n",
    "            \n",
    "            startDate = '-'.join(theDict[\"Date\"][-1].split(\"-\")[:3])\n",
    "            # This will check if the untilDate is the same as endDate, if it is. It'll minus one day.\n",
    "            if currentSearchDate_limit == startDate:\n",
    "                startDate = minOneDay(startDate)\n",
    "            \n",
    "            break\n",
    "\n",
    "        last_height = new_height"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
